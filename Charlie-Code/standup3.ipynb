{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8250272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "#import torch.cuda\n",
    "import random\n",
    "from itertools import chain as chain\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import math\n",
    "\n",
    "#conda activate base\n",
    "cudaOn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14af85d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715057ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tepLoc = \"C:/Users/Charlie/Desktop/TEP_Data/\"\n",
    "\n",
    "#tepTrain = tepLoc + \"TEP_Faulty_Training.csv\"\n",
    "tepTrain = tepLoc + \"TEP_FaultFree_Training.csv\"\n",
    "\n",
    "#tepTest = tepLoc + \"TEP_FaultFree_Testing.csv\"\n",
    "tepTest = tepLoc + \"TEP_Faulty_Testing.csv\"\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#data = pd.read_csv('c172_file_1.csv')\n",
    "data = pd.read_csv(tepTrain)\n",
    "dataTest = pd.read_csv(tepTest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65eae22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookie = dataTest[dataTest['simulationRun']==112]\n",
    "#print(lookie[lookie['faultNumber']==5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03075b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>simulationRun</th>\n",
       "      <th>sample</th>\n",
       "      <th>xmeas_1</th>\n",
       "      <th>xmeas_2</th>\n",
       "      <th>xmeas_3</th>\n",
       "      <th>xmeas_4</th>\n",
       "      <th>xmeas_5</th>\n",
       "      <th>xmeas_6</th>\n",
       "      <th>xmeas_7</th>\n",
       "      <th>xmeas_8</th>\n",
       "      <th>...</th>\n",
       "      <th>xmv_2</th>\n",
       "      <th>xmv_3</th>\n",
       "      <th>xmv_4</th>\n",
       "      <th>xmv_5</th>\n",
       "      <th>xmv_6</th>\n",
       "      <th>xmv_7</th>\n",
       "      <th>xmv_8</th>\n",
       "      <th>xmv_9</th>\n",
       "      <th>xmv_10</th>\n",
       "      <th>xmv_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25038</td>\n",
       "      <td>3674.0</td>\n",
       "      <td>4529.0</td>\n",
       "      <td>9.2320</td>\n",
       "      <td>26.889</td>\n",
       "      <td>42.402</td>\n",
       "      <td>2704.3</td>\n",
       "      <td>74.863</td>\n",
       "      <td>...</td>\n",
       "      <td>53.744</td>\n",
       "      <td>24.657</td>\n",
       "      <td>62.544</td>\n",
       "      <td>22.137</td>\n",
       "      <td>39.935</td>\n",
       "      <td>42.323</td>\n",
       "      <td>47.757</td>\n",
       "      <td>47.510</td>\n",
       "      <td>41.258</td>\n",
       "      <td>18.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25109</td>\n",
       "      <td>3659.4</td>\n",
       "      <td>4556.6</td>\n",
       "      <td>9.4264</td>\n",
       "      <td>26.721</td>\n",
       "      <td>42.576</td>\n",
       "      <td>2705.0</td>\n",
       "      <td>75.000</td>\n",
       "      <td>...</td>\n",
       "      <td>53.414</td>\n",
       "      <td>24.588</td>\n",
       "      <td>59.259</td>\n",
       "      <td>22.084</td>\n",
       "      <td>40.176</td>\n",
       "      <td>38.554</td>\n",
       "      <td>43.692</td>\n",
       "      <td>47.427</td>\n",
       "      <td>41.359</td>\n",
       "      <td>17.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25038</td>\n",
       "      <td>3660.3</td>\n",
       "      <td>4477.8</td>\n",
       "      <td>9.4426</td>\n",
       "      <td>26.875</td>\n",
       "      <td>42.070</td>\n",
       "      <td>2706.2</td>\n",
       "      <td>74.771</td>\n",
       "      <td>...</td>\n",
       "      <td>54.357</td>\n",
       "      <td>24.666</td>\n",
       "      <td>61.275</td>\n",
       "      <td>22.380</td>\n",
       "      <td>40.244</td>\n",
       "      <td>38.990</td>\n",
       "      <td>46.699</td>\n",
       "      <td>47.468</td>\n",
       "      <td>41.199</td>\n",
       "      <td>20.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.24977</td>\n",
       "      <td>3661.3</td>\n",
       "      <td>4512.1</td>\n",
       "      <td>9.4776</td>\n",
       "      <td>26.758</td>\n",
       "      <td>42.063</td>\n",
       "      <td>2707.2</td>\n",
       "      <td>75.224</td>\n",
       "      <td>...</td>\n",
       "      <td>53.946</td>\n",
       "      <td>24.725</td>\n",
       "      <td>59.856</td>\n",
       "      <td>22.277</td>\n",
       "      <td>40.257</td>\n",
       "      <td>38.072</td>\n",
       "      <td>47.541</td>\n",
       "      <td>47.658</td>\n",
       "      <td>41.643</td>\n",
       "      <td>18.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.29405</td>\n",
       "      <td>3679.0</td>\n",
       "      <td>4497.0</td>\n",
       "      <td>9.3381</td>\n",
       "      <td>26.889</td>\n",
       "      <td>42.650</td>\n",
       "      <td>2705.1</td>\n",
       "      <td>75.388</td>\n",
       "      <td>...</td>\n",
       "      <td>53.658</td>\n",
       "      <td>28.797</td>\n",
       "      <td>60.717</td>\n",
       "      <td>21.947</td>\n",
       "      <td>39.144</td>\n",
       "      <td>41.955</td>\n",
       "      <td>47.645</td>\n",
       "      <td>47.346</td>\n",
       "      <td>41.507</td>\n",
       "      <td>18.461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249995</th>\n",
       "      <td>500</td>\n",
       "      <td>496</td>\n",
       "      <td>0.29325</td>\n",
       "      <td>3640.1</td>\n",
       "      <td>4473.0</td>\n",
       "      <td>9.1949</td>\n",
       "      <td>26.867</td>\n",
       "      <td>42.379</td>\n",
       "      <td>2700.2</td>\n",
       "      <td>75.533</td>\n",
       "      <td>...</td>\n",
       "      <td>53.429</td>\n",
       "      <td>29.249</td>\n",
       "      <td>60.773</td>\n",
       "      <td>21.532</td>\n",
       "      <td>40.451</td>\n",
       "      <td>34.064</td>\n",
       "      <td>48.953</td>\n",
       "      <td>48.291</td>\n",
       "      <td>40.812</td>\n",
       "      <td>18.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249996</th>\n",
       "      <td>500</td>\n",
       "      <td>497</td>\n",
       "      <td>0.29134</td>\n",
       "      <td>3625.7</td>\n",
       "      <td>4506.2</td>\n",
       "      <td>9.2109</td>\n",
       "      <td>26.889</td>\n",
       "      <td>42.291</td>\n",
       "      <td>2700.6</td>\n",
       "      <td>75.935</td>\n",
       "      <td>...</td>\n",
       "      <td>53.830</td>\n",
       "      <td>28.975</td>\n",
       "      <td>61.517</td>\n",
       "      <td>21.750</td>\n",
       "      <td>42.762</td>\n",
       "      <td>42.645</td>\n",
       "      <td>51.055</td>\n",
       "      <td>48.589</td>\n",
       "      <td>40.933</td>\n",
       "      <td>19.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249997</th>\n",
       "      <td>500</td>\n",
       "      <td>498</td>\n",
       "      <td>0.29438</td>\n",
       "      <td>3600.2</td>\n",
       "      <td>4478.3</td>\n",
       "      <td>9.1957</td>\n",
       "      <td>26.820</td>\n",
       "      <td>42.448</td>\n",
       "      <td>2700.3</td>\n",
       "      <td>74.706</td>\n",
       "      <td>...</td>\n",
       "      <td>54.163</td>\n",
       "      <td>28.676</td>\n",
       "      <td>61.656</td>\n",
       "      <td>21.487</td>\n",
       "      <td>42.109</td>\n",
       "      <td>39.770</td>\n",
       "      <td>46.770</td>\n",
       "      <td>48.648</td>\n",
       "      <td>41.465</td>\n",
       "      <td>19.344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249998</th>\n",
       "      <td>500</td>\n",
       "      <td>499</td>\n",
       "      <td>0.25269</td>\n",
       "      <td>3683.5</td>\n",
       "      <td>4486.4</td>\n",
       "      <td>9.2832</td>\n",
       "      <td>27.188</td>\n",
       "      <td>42.757</td>\n",
       "      <td>2697.4</td>\n",
       "      <td>75.101</td>\n",
       "      <td>...</td>\n",
       "      <td>53.453</td>\n",
       "      <td>24.889</td>\n",
       "      <td>61.564</td>\n",
       "      <td>21.392</td>\n",
       "      <td>39.334</td>\n",
       "      <td>42.274</td>\n",
       "      <td>43.623</td>\n",
       "      <td>48.797</td>\n",
       "      <td>39.835</td>\n",
       "      <td>18.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249999</th>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>0.25214</td>\n",
       "      <td>3648.2</td>\n",
       "      <td>4467.8</td>\n",
       "      <td>9.1344</td>\n",
       "      <td>26.886</td>\n",
       "      <td>42.534</td>\n",
       "      <td>2695.1</td>\n",
       "      <td>74.787</td>\n",
       "      <td>...</td>\n",
       "      <td>53.676</td>\n",
       "      <td>24.943</td>\n",
       "      <td>61.254</td>\n",
       "      <td>21.208</td>\n",
       "      <td>38.991</td>\n",
       "      <td>42.873</td>\n",
       "      <td>44.400</td>\n",
       "      <td>48.876</td>\n",
       "      <td>41.076</td>\n",
       "      <td>16.158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        simulationRun  sample  xmeas_1  xmeas_2  xmeas_3  xmeas_4  xmeas_5  \\\n",
       "0                   1       1  0.25038   3674.0   4529.0   9.2320   26.889   \n",
       "1                   1       2  0.25109   3659.4   4556.6   9.4264   26.721   \n",
       "2                   1       3  0.25038   3660.3   4477.8   9.4426   26.875   \n",
       "3                   1       4  0.24977   3661.3   4512.1   9.4776   26.758   \n",
       "4                   1       5  0.29405   3679.0   4497.0   9.3381   26.889   \n",
       "...               ...     ...      ...      ...      ...      ...      ...   \n",
       "249995            500     496  0.29325   3640.1   4473.0   9.1949   26.867   \n",
       "249996            500     497  0.29134   3625.7   4506.2   9.2109   26.889   \n",
       "249997            500     498  0.29438   3600.2   4478.3   9.1957   26.820   \n",
       "249998            500     499  0.25269   3683.5   4486.4   9.2832   27.188   \n",
       "249999            500     500  0.25214   3648.2   4467.8   9.1344   26.886   \n",
       "\n",
       "        xmeas_6  xmeas_7  xmeas_8  ...   xmv_2   xmv_3   xmv_4   xmv_5  \\\n",
       "0        42.402   2704.3   74.863  ...  53.744  24.657  62.544  22.137   \n",
       "1        42.576   2705.0   75.000  ...  53.414  24.588  59.259  22.084   \n",
       "2        42.070   2706.2   74.771  ...  54.357  24.666  61.275  22.380   \n",
       "3        42.063   2707.2   75.224  ...  53.946  24.725  59.856  22.277   \n",
       "4        42.650   2705.1   75.388  ...  53.658  28.797  60.717  21.947   \n",
       "...         ...      ...      ...  ...     ...     ...     ...     ...   \n",
       "249995   42.379   2700.2   75.533  ...  53.429  29.249  60.773  21.532   \n",
       "249996   42.291   2700.6   75.935  ...  53.830  28.975  61.517  21.750   \n",
       "249997   42.448   2700.3   74.706  ...  54.163  28.676  61.656  21.487   \n",
       "249998   42.757   2697.4   75.101  ...  53.453  24.889  61.564  21.392   \n",
       "249999   42.534   2695.1   74.787  ...  53.676  24.943  61.254  21.208   \n",
       "\n",
       "         xmv_6   xmv_7   xmv_8   xmv_9  xmv_10  xmv_11  \n",
       "0       39.935  42.323  47.757  47.510  41.258  18.447  \n",
       "1       40.176  38.554  43.692  47.427  41.359  17.194  \n",
       "2       40.244  38.990  46.699  47.468  41.199  20.530  \n",
       "3       40.257  38.072  47.541  47.658  41.643  18.089  \n",
       "4       39.144  41.955  47.645  47.346  41.507  18.461  \n",
       "...        ...     ...     ...     ...     ...     ...  \n",
       "249995  40.451  34.064  48.953  48.291  40.812  18.756  \n",
       "249996  42.762  42.645  51.055  48.589  40.933  19.360  \n",
       "249997  42.109  39.770  46.770  48.648  41.465  19.344  \n",
       "249998  39.334  42.274  43.623  48.797  39.835  18.512  \n",
       "249999  38.991  42.873  44.400  48.876  41.076  16.158  \n",
       "\n",
       "[250000 rows x 54 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop('Unnamed: 0',axis=1)\n",
    "data = data.drop('faultNumber',axis=1)\n",
    "#data = data.drop('simulationRun',axis=1)\n",
    "#data = data.drop('sample',axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c817fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTest = dataTest[dataTest['simulationRun']==60]\n",
    "faultNumbersT = dataTest.get('faultNumber')\n",
    "#\n",
    "dataTest = dataTest.drop('Unnamed: 0',axis=1)\n",
    "dataTest = dataTest.drop('faultNumber',axis=1)\n",
    "#dataTest = dataTest.drop('simulationRun',axis=1)\n",
    "#dataTest = dataTest.drop('sample',axis=1)\n",
    "#dataTest = data\n",
    "#dataTest = dataTest.iloc(0)[0:19500] #test A and B\n",
    "#dataTest = dataTest.iloc(0)[19500:38500] #test C and D\n",
    "#dataTest = dataTest.iloc(0)[39000:58000]\n",
    "\n",
    "faultNumbersTest = []\n",
    "for i in faultNumbersT:\n",
    "    faultNumbersTest.append(i)\n",
    "\n",
    "data = data.astype('float64')\n",
    "dataTest = dataTest.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36b49699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1\n",
      "960: 2\n",
      "1920: 3\n",
      "2880: 4\n",
      "3840: 5\n",
      "4800: 6\n",
      "5760: 7\n",
      "6720: 8\n",
      "7680: 9\n",
      "8640: 10\n",
      "9600: 11\n",
      "10560: 12\n",
      "11520: 13\n",
      "12480: 14\n",
      "13440: 15\n",
      "14400: 16\n",
      "15360: 17\n",
      "16320: 18\n",
      "17280: 19\n",
      "18240: 20\n"
     ]
    }
   ],
   "source": [
    "run_length = 960\n",
    "#num = 960\n",
    "for i in range(int(len(faultNumbersTest)/run_length)):\n",
    "    print(str(i*run_length) + \": \" + str(faultNumbersTest[i*run_length]))\n",
    "#print(faultNumbersTest[10000:10100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "708cace7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         simulationRun  sample  xmeas_1  xmeas_2  xmeas_3  xmeas_4  xmeas_5  \\\n",
      "1132800           60.0     1.0  0.25212   3665.6   4538.2   9.3005   26.596   \n",
      "1132801           60.0     2.0  0.24987   3671.5   4504.2   9.2820   27.075   \n",
      "1132802           60.0     3.0  0.25062   3664.7   4495.1   9.3984   26.734   \n",
      "1132803           60.0     4.0  0.24948   3678.0   4520.5   9.3693   27.112   \n",
      "1132804           60.0     5.0  0.28200   3708.0   4502.0   9.2449   27.011   \n",
      "...                ...     ...      ...      ...      ...      ...      ...   \n",
      "1151995           60.0   956.0  0.24885   3699.1   4522.3   9.3982   26.716   \n",
      "1151996           60.0   957.0  0.18616   3624.4   4556.8   9.4190   26.863   \n",
      "1151997           60.0   958.0  0.18621   3632.4   4499.8   9.3068   26.513   \n",
      "1151998           60.0   959.0  0.24906   3687.0   4552.0   9.3735   26.948   \n",
      "1151999           60.0   960.0  0.24974   3668.8   4501.2   9.5062   26.726   \n",
      "\n",
      "         xmeas_6  xmeas_7  xmeas_8  ...   xmv_2   xmv_3   xmv_4   xmv_5  \\\n",
      "1132800   42.126   2703.8   74.348  ...  53.635  24.487  61.958  22.068   \n",
      "1132801   42.725   2704.5   74.662  ...  54.041  24.708  61.635  22.373   \n",
      "1132802   42.102   2707.4   75.172  ...  54.149  24.642  61.968  22.353   \n",
      "1132803   42.355   2708.1   75.166  ...  53.846  24.754  58.702  22.327   \n",
      "1132804   42.729   2705.1   74.240  ...  55.136  27.556  63.323  21.989   \n",
      "...          ...      ...      ...  ...     ...     ...     ...     ...   \n",
      "1151995   42.089   2722.2   74.743  ...  54.347  24.392  60.992  18.532   \n",
      "1151996   42.472   2718.1   75.442  ...  53.870  18.400  58.889  18.016   \n",
      "1151997   42.196   2716.3   75.095  ...  54.553  18.395  62.095  17.282   \n",
      "1151998   42.403   2715.2   74.961  ...  54.021  24.565  60.179  16.958   \n",
      "1151999   42.813   2716.6   74.313  ...  54.629  24.498  58.583  16.144   \n",
      "\n",
      "          xmv_6   xmv_7   xmv_8   xmv_9  xmv_10  xmv_11  \n",
      "1132800  40.255  34.746  50.128  47.484  41.005  16.690  \n",
      "1132801  40.198  39.593  50.334  47.545  40.957  21.052  \n",
      "1132802  40.250  34.074  46.708  47.471  41.504  16.316  \n",
      "1132803  40.288  35.122  45.346  47.323  41.332  21.656  \n",
      "1132804  41.258  44.955  47.178  47.521  41.279  22.200  \n",
      "...         ...     ...     ...     ...     ...     ...  \n",
      "1151995  39.011  34.893  40.781  47.169  41.460  19.255  \n",
      "1151996  40.470  38.984  46.098  47.190  40.823  20.399  \n",
      "1151997  40.325  30.596  46.415  46.986  40.015  21.795  \n",
      "1151998  39.193  37.167  44.758  46.690  41.243  15.105  \n",
      "1151999  39.004  38.550  43.079  46.478  41.011  17.715  \n",
      "\n",
      "[19200 rows x 54 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataTest)\n",
    "numVariables = 54#52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23747611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b45cd691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split and reshape the data set by step_size , use min-max or stanrdardlize method to rescale the data\n",
    "def Splitting_dataset(data, step_size, scale=True, scaler_type=MinMaxScaler):\n",
    "        l = len(data) \n",
    "        data = scaler_type().fit_transform(data)\n",
    "        Xs = []\n",
    "        Ys = []\n",
    "        for i in range(0, (len(data) - step_size)):\n",
    "            Xs.append(data[i:i+step_size])\n",
    "            Ys.append(data[i:i+step_size])\n",
    "        train_x, test_x, train_y, test_y = [np.array(x) for x in train_test_split(Xs, Ys)]\n",
    "        assert train_x.shape[2] == test_x.shape[2] == (data.shape[1] if (type(data) == np.ndarray) else len(data))\n",
    "        return  (train_x.shape[2], train_x, train_y, test_x, test_y)\n",
    "    \n",
    "def get_batch(x, batch_size):\n",
    "    \"\"\"Made with taking test_x or XX as input\"\"\"\n",
    "    #make stochastic\n",
    "    t = 0\n",
    "    while t >= 0:\n",
    "        x_mod = len(x) % batch_size\n",
    "        start = random.random() * (len(x)-x_mod)\n",
    "        start = int(start)\n",
    "        if start + batch_size < len(x):\n",
    "            t = t-1\n",
    "    batch = torch.tensor(x[start:(start+batch_size)]) #!! added tensor line\n",
    "    #print(batch.shape)\n",
    "    return batch\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "        #print(recon_x.shape)\n",
    "        #print(x.shape)\n",
    "        BCE = F.binary_cross_entropy(recon_x, x)#, size_average=False)\n",
    "    \n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu**2 -  logvar.exp())\n",
    "        return BCE + KLD\n",
    "    \n",
    "def tep_testing_stepped(dat,step_size):\n",
    "    res = []\n",
    "    ind = 0\n",
    "    scale = MinMaxScaler().fit(dat)\n",
    "    dat = pd.DataFrame(scale.transform(dat))\n",
    "    #print(int((len(data)/step_size)))\n",
    "    for i in range(int((len(dat)/step_size))):\n",
    "        if ind + step_size < len(dat):\n",
    "            step = []\n",
    "            for j in range(step_size):\n",
    "              #print(data.iloc(0)[ind])#[ind])\n",
    "              step.append(dat.iloc(0)[ind])\n",
    "              ind = ind + 1\n",
    "            res.append(step)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "450792c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4efa905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=27, z_dim=31, n_flow_steps=1):\n",
    "        super(VAE, self).__init__()\n",
    "        #self.rnn = nn.GRU(image_size,image_size,batch_first=True)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(image_size, h_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(h_dim, z_dim*2) #is it saying its getting a mu and a var for each z dim out?\n",
    "            \n",
    "            #how can I represent the encoder as a distribution acting as the prior?\n",
    "        )\n",
    "        print(z_dim*2)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, image_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        print(image_size)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_() \n",
    "        esp = to_var(torch.randn(*mu.size()))\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #q = self.recurrent(x)\n",
    "        #print(\"--\" + str(x.shape))\n",
    "        h = self.encoder(x)\n",
    "        #print(\"--\" + str(h.shape))\n",
    "        mu, logvar = torch.chunk(h, 2, dim=1)\n",
    "        #print(mu.shape)\n",
    "        #print(logvar.shape)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        #print(str(z.shape) + str(\" XXX\"))\n",
    "        #print(z)\n",
    "        #z = z.float()\n",
    "        z = model(z)\n",
    "        #print(z)\n",
    "        tensorZ = z[0]#torch.tensor(z[0])\n",
    "        #print(str(tensorZ.shape) + str(\" YYY\"))\n",
    "        #print(z[0])\n",
    "        #print(\"--\" + str(self.decoder(tensorZ).shape))\n",
    "\n",
    "        return self.decoder(tensorZ), mu, logvar\n",
    "        #return self.decoder(z), mu, logvar\n",
    "    \n",
    "\n",
    "class stacked_NVP(nn.Module):\n",
    "    def __init__(self, d, k, hidden, n):\n",
    "        super().__init__()\n",
    "        self.bijectors = nn.ModuleList([\n",
    "            R_NVP(d, k, hidden=hidden) for _ in range(n)\n",
    "        ])\n",
    "        self.flips = [True if i%2 else False for i in range(n)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        log_jacobs = []\n",
    "\n",
    "        for bijector, f in zip(self.bijectors, self.flips):\n",
    "            x, log_pz, lj = bijector(x, flip=f)\n",
    "            log_jacobs.append(lj)\n",
    "        \n",
    "        return x, log_pz, sum(log_jacobs)\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        for bijector, f in zip(reversed(self.bijectors), reversed(self.flips)):\n",
    "            z = bijector.inverse(z, flip=f)\n",
    "        return z\n",
    "    \n",
    "class R_NVP(nn.Module):\n",
    "    def __init__(self, d, k, hidden):\n",
    "        super().__init__()\n",
    "        self.d, self.k = d, k\n",
    "        self.sig_net = nn.Sequential(\n",
    "                    nn.Linear(k, hidden),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(hidden, d - k))\n",
    "\n",
    "        self.mu_net = nn.Sequential(\n",
    "                    nn.Linear(k, hidden),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(hidden, d - k))\n",
    "\n",
    "    def forward(self, x, flip=False):\n",
    "        x1, x2 = x[:, :self.k], x[:, self.k:] \n",
    "        #print(\"t\")\n",
    "        if flip:\n",
    "            x2, x1 = x1, x2\n",
    "        \n",
    "        # forward\n",
    "        sig = self.sig_net(x1)\n",
    "        print(\"x1: \" + str(x1.shape))\n",
    "        #print(\"x2: \" + str(x2.shape))\n",
    "        z1, z2 = x1, x2 * torch.exp(sig) + self.mu_net(x1)\n",
    "        \n",
    "        if flip:\n",
    "            z2, z1 = z1, z2\n",
    "        \n",
    "        z_hat = torch.cat([z1, z2], dim=-1)\n",
    "        #print(z_hat.shape)\n",
    "        #print(\"ooooooo\")\n",
    "        #print(z_hat)\n",
    "        log_pz = base_dist.log_prob(z_hat)\n",
    "        log_jacob = sig.sum(-1)\n",
    "        \n",
    "        return z_hat, log_pz, log_jacob\n",
    "    \n",
    "    def inverse(self, Z, flip=False):\n",
    "        z1, z2 = Z[:, :self.k], Z[:, self.k:] \n",
    "        \n",
    "        if flip:\n",
    "            z2, z1 = z1, z2\n",
    "        \n",
    "        x1 = z1\n",
    "        x2 = (z2 - self.mu_net(z1)) * torch.exp(-self.sig_net(z1))\n",
    "        \n",
    "        if flip:\n",
    "            x2, x1 = x1, x2\n",
    "        return torch.cat([x1, x2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5ed6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8932e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.], dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "108\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "step_size = 3\n",
    "batch = 512\n",
    "index_step_length = numVariables\n",
    "epochs = 20\n",
    "\n",
    "num = 10\n",
    "\n",
    "d = 54\n",
    "k = 27\n",
    "\n",
    "base_mu, base_cov = torch.zeros(54), torch.eye(54)\n",
    "\n",
    "base_mu = torch.nn.parameter.Parameter(to_var(base_mu.double()))\n",
    "base_cov = torch.nn.parameter.Parameter(to_var(base_cov.double()))\n",
    "#base_mu = torch.nn.parameter.Parameter(base_mu,requires_grad=True)\n",
    "#base_cov = torch.nn.parameter.Parameter(base_cov,requires_grad=True)\n",
    "print(base_mu)\n",
    "print(base_cov)\n",
    "base_dist = MultivariateNormal(base_mu, base_cov)\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "labels, X, Y, XX, YY = Splitting_dataset(data, step_size)\n",
    "#XX.cuda()\n",
    "demo = VAE(index_step_length,h_dim=13,z_dim=54)\n",
    "model = stacked_NVP(d, k, hidden=512,n=num)#hidden -> 512\n",
    "demo.double()\n",
    "model.double()\n",
    "    \n",
    "#next set of tests should be with n=3, last set was with n=1\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "optimizer2 = torch.optim.RMSprop(demo.parameters(), lr=1e-3)\n",
    "optimizer3 = torch.optim.RMSprop([base_mu,base_cov], lr=1e-3)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.999)\n",
    "#scheduler2 = torch.optim.lr_scheduler.ExponentialLR(optimizer2, 0.999)\n",
    "\n",
    "if torch.cuda.is_available() & cudaOn:\n",
    "    demo.cuda()\n",
    "    print(\"demo done\")\n",
    "    model.cuda()\n",
    "    print(\"model done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fda6b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "191093b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4770, -2.3366,  0.2015, -1.0965,  0.0138,  0.5691,  1.0484,  0.6350,\n",
      "          0.5002,  0.5973,  0.5511,  0.8635,  0.1120, -0.8902, -0.0285, -1.2166,\n",
      "         -0.2452, -0.8211, -0.2736,  0.3870, -0.8391,  0.2405,  0.8925,  0.3384,\n",
      "         -0.6507, -1.2841,  0.0757, -0.1499,  0.5858, -1.8142, -0.6469,  0.2438,\n",
      "         -0.2332,  1.2824,  0.6189, -0.4385, -0.0579, -0.5556,  0.7731,  1.3920,\n",
      "         -0.3132, -0.0961,  0.0364,  0.9273, -1.3485, -0.6626, -0.8174,  1.3262,\n",
      "          1.0694,  0.3464, -0.0541, -0.0149,  0.0604,  0.3965],\n",
      "        [ 0.7268, -1.5707,  1.0420, -1.4435,  0.4843, -1.9943, -0.8145,  0.1536,\n",
      "          1.1309,  0.7732, -1.1291,  0.8265,  0.3604, -0.9273,  0.5306,  0.7530,\n",
      "         -1.9981,  0.5450,  0.5575,  0.5079, -0.6529, -0.3784,  0.4699,  0.9511,\n",
      "          0.1983, -1.2531, -0.2801,  0.6318,  0.5179, -1.8017, -1.1898,  0.4429,\n",
      "          1.2912,  1.5408,  0.4589, -2.0283,  0.0222,  0.0158, -0.7348,  0.1074,\n",
      "         -0.3394, -0.1500,  0.6536, -1.2790,  0.5939,  0.5543,  0.3342, -1.4941,\n",
      "         -0.8423,  0.4730,  0.9908, -0.1082, -1.5232, -0.0912],\n",
      "        [ 0.0473,  0.0079,  1.4536,  0.4028, -1.5186,  0.4034, -0.5183,  0.5149,\n",
      "         -0.5918, -1.9225,  0.1350,  1.0155,  0.8287, -0.4152, -0.8476,  1.3861,\n",
      "         -2.8189,  1.2257,  2.7049, -0.0056,  1.0725,  0.0169, -0.1286, -1.1917,\n",
      "          1.0752,  0.3131, -1.3169, -0.5970, -0.4101, -0.4897, -0.4099,  0.8759,\n",
      "          3.0988,  1.4805, -0.7985, -1.8506, -2.3917, -0.2517,  0.0780, -2.0378,\n",
      "          0.2816,  0.3994, -0.2706, -1.8198,  0.3228, -0.7213, -1.0252,  0.4784,\n",
      "         -0.5321,  0.0038,  0.2080,  0.8051,  0.7762,  0.1069]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "(tensor([[ 1.6480e+00, -1.1055e+00,  4.5854e-01, -1.4272e+00, -6.0719e-02,\n",
      "          1.1701e+00,  4.0011e-01, -1.0249e+00,  3.3128e-01,  6.4539e-01,\n",
      "          7.0595e-01,  5.1709e-01, -3.7997e-01, -2.1933e+00,  4.1392e-01,\n",
      "         -2.1666e+00,  2.0995e+00, -7.4481e-01, -6.3350e-01,  7.1107e-01,\n",
      "         -3.4441e+00, -3.8155e-01,  2.8200e+00,  5.7074e-01, -6.4065e-01,\n",
      "         -1.9718e+00, -7.3341e-01, -1.1878e-01,  3.8149e-01, -2.2751e+00,\n",
      "         -7.0370e-01,  1.6179e-01, -2.8600e-01,  2.5367e+00,  7.6838e-01,\n",
      "         -8.4774e-01,  1.0101e+00, -1.2695e+00,  1.6471e+00,  1.0247e+00,\n",
      "          3.7001e-01,  7.7221e-02,  1.7222e-01,  1.6026e+00, -6.1331e-01,\n",
      "         -6.8537e-01, -2.7543e+00,  1.1926e+00,  2.0162e+00,  9.2536e-02,\n",
      "         -1.9234e-01, -2.7393e-01,  1.0291e+00,  1.2529e-02],\n",
      "        [ 1.0757e+00, -1.7858e+00,  1.2435e+00, -6.7621e-01,  4.9702e-01,\n",
      "         -1.6982e+00, -4.8739e+00,  1.9355e-01,  1.9119e+00,  6.9777e-01,\n",
      "         -9.9444e-01,  1.2908e+00,  2.9457e-01, -1.5300e+00,  5.1073e-01,\n",
      "         -8.4935e-02, -1.9127e+00,  3.8772e-01,  2.7118e-01,  7.4684e-01,\n",
      "         -1.5495e+00, -6.7323e-01,  9.3034e-01,  1.0739e+00,  8.0387e-01,\n",
      "         -1.4964e+00, -1.3956e-01,  7.2760e-01,  1.9436e-01, -3.7402e+00,\n",
      "         -1.2328e+00,  3.0640e-01,  1.1386e+00,  2.2771e+00, -8.3870e-01,\n",
      "         -1.8104e+00,  3.5347e-01, -7.8683e-03, -1.0993e+00, -1.5696e-02,\n",
      "          9.1832e-02, -4.6557e-02,  2.7820e-01, -1.7184e+00,  8.3804e-01,\n",
      "          1.2596e+00,  2.5659e-01, -2.1041e+00, -8.3609e-01,  2.2475e-01,\n",
      "          1.7555e+00, -5.7377e-01, -1.0926e+00, -2.6468e-01],\n",
      "        [-9.5061e-02, -1.0202e+00,  3.1730e+00, -1.1492e-01, -1.3213e+00,\n",
      "          6.1878e-01,  4.2922e-01,  1.9303e+00, -7.8239e-01, -1.1344e+00,\n",
      "          8.5359e-01,  7.2108e-01,  9.3156e-01, -7.4386e-01, -2.3579e+00,\n",
      "          1.1397e+00, -1.3249e+00,  6.4655e-01,  7.1398e+00,  4.9882e-01,\n",
      "          4.0659e+00, -6.5452e-01,  9.6258e-01, -1.2507e+00,  9.0021e-01,\n",
      "          5.5512e-01, -1.5355e+00, -3.1379e-01, -3.3720e-01, -3.9982e+00,\n",
      "          1.1124e+00,  1.7880e-01,  1.7665e+00,  2.0072e+00, -3.8250e+00,\n",
      "         -7.4036e-01,  3.1356e-02, -5.6075e-01,  2.5976e-01, -2.3497e+00,\n",
      "         -2.1084e-01,  1.1526e-01, -6.9281e-03, -8.8367e-01,  7.5963e-01,\n",
      "         -1.5368e+00, -4.3834e-01, -5.7504e-02,  1.5905e-02,  2.1683e-02,\n",
      "          5.3956e-01,  6.7149e-01, -1.1533e-01, -1.4555e-01]],\n",
      "       dtype=torch.float64, grad_fn=<CatBackward>), tensor([ -94.0178,  -98.7214, -127.3097], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>), tensor([-7.5999, -1.7884, -3.6133], dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>))\n",
      "tensor([[0.5777, 0.5914, 0.5262, 0.4603, 0.5632, 0.4622, 0.5239, 0.4753, 0.4794,\n",
      "         0.4933, 0.5431, 0.5338, 0.5689, 0.5089, 0.3860, 0.4252, 0.5374, 0.5550,\n",
      "         0.4426, 0.4678, 0.5189, 0.5095, 0.5751, 0.3959, 0.5066, 0.3603, 0.4103,\n",
      "         0.5316, 0.4813, 0.4775, 0.4006, 0.4290, 0.4801, 0.5654, 0.5731, 0.5219,\n",
      "         0.5825, 0.3998, 0.5402, 0.4696, 0.5506, 0.5805, 0.5585, 0.4974, 0.5911,\n",
      "         0.4226, 0.4564, 0.5315, 0.5405, 0.5679, 0.6013, 0.6390, 0.5375, 0.5001],\n",
      "        [0.6704, 0.5117, 0.5981, 0.3401, 0.4635, 0.4322, 0.4859, 0.4181, 0.5721,\n",
      "         0.4600, 0.4042, 0.5630, 0.5139, 0.4506, 0.3088, 0.4298, 0.5045, 0.5957,\n",
      "         0.4811, 0.4735, 0.4319, 0.5160, 0.5368, 0.2940, 0.4935, 0.3998, 0.3444,\n",
      "         0.6085, 0.5359, 0.4698, 0.3052, 0.4918, 0.5094, 0.5564, 0.4884, 0.4718,\n",
      "         0.6417, 0.4368, 0.5462, 0.4427, 0.5721, 0.5496, 0.6125, 0.3842, 0.5560,\n",
      "         0.3916, 0.5204, 0.5617, 0.6070, 0.5668, 0.5592, 0.6358, 0.4649, 0.5351],\n",
      "        [0.6278, 0.5772, 0.4076, 0.5893, 0.5712, 0.5100, 0.5961, 0.3903, 0.5191,\n",
      "         0.4842, 0.4841, 0.4442, 0.6619, 0.4778, 0.4162, 0.4524, 0.6702, 0.5720,\n",
      "         0.4217, 0.5670, 0.5382, 0.5757, 0.5496, 0.4083, 0.5105, 0.3573, 0.2963,\n",
      "         0.5980, 0.6333, 0.3498, 0.4627, 0.4840, 0.6209, 0.4690, 0.5368, 0.5792,\n",
      "         0.5661, 0.6150, 0.5162, 0.5807, 0.5981, 0.6114, 0.5142, 0.4209, 0.6127,\n",
      "         0.4125, 0.5067, 0.3212, 0.6764, 0.5265, 0.6789, 0.5973, 0.5336, 0.3720]],\n",
      "       dtype=torch.float64, grad_fn=<SigmoidBackward>)\n",
      "tensor([[-0.8782, -0.5983, -0.9312,  0.9601,  0.8303, -0.0434, -0.7086, -1.9649,\n",
      "          0.6545,  2.1821,  0.3359, -0.0612, -2.1077,  0.5455,  1.4520,  1.1562,\n",
      "         -0.3276,  0.2372, -0.6320, -1.4326,  1.0899, -0.1996,  1.5388, -0.4198,\n",
      "         -1.1453, -0.0834,  0.7042,  1.3722, -0.8105, -0.0092,  0.8845, -0.5668,\n",
      "         -0.7489,  0.7818, -3.2516, -0.4885, -1.4114, -1.0144, -0.7876, -1.4204,\n",
      "         -1.5334, -0.9341,  1.1816, -0.7264,  0.2777, -0.4770, -0.8668, -0.6272,\n",
      "          1.3919, -1.0003,  0.2372,  1.7907, -0.7589, -0.2157],\n",
      "        [ 0.4525, -0.2758,  0.9144,  0.4924,  1.2592, -0.6745, -1.3392, -0.0975,\n",
      "          1.3243,  1.1394,  0.2348, -0.3249,  0.1189, -0.9660, -0.9289,  0.6181,\n",
      "         -1.9890, -0.5864, -0.1129, -0.8121, -0.7307, -1.2595,  0.1975,  1.9325,\n",
      "         -0.9397,  0.2723,  0.8914, -0.8601,  1.6187,  0.8985,  0.0486,  0.2270,\n",
      "         -0.7814,  0.7277, -0.3031, -1.8323,  0.5178,  0.7819, -1.1592,  2.0358,\n",
      "         -0.8664, -0.7521,  0.5417,  0.8023, -0.5144, -0.5898,  0.2165, -1.5411,\n",
      "          0.1587,  0.0628, -0.3407,  0.1382, -1.4535, -1.8377],\n",
      "        [ 1.1303,  1.1527, -0.4867, -0.3703,  0.0493, -0.1753, -1.1109,  1.1587,\n",
      "         -0.9975, -0.0681,  1.9185, -0.0149,  2.6485, -1.0321, -0.1055,  0.2210,\n",
      "         -0.2863,  1.4400,  0.1030, -0.3022, -0.1823,  0.0933, -1.4578, -0.3069,\n",
      "         -0.6764, -0.3355,  0.3871, -1.6308,  2.6209, -0.4760, -0.6512,  0.6212,\n",
      "         -1.1963,  0.3532, -0.8218,  1.9174, -0.6091,  0.4334, -0.4013, -0.2405,\n",
      "          0.9553, -0.4805,  1.1241,  0.6821, -0.4026,  0.3245,  0.1077,  0.0047,\n",
      "          0.2575,  0.1135, -0.3298, -1.2819,  1.3632, -0.9245]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "x1: torch.Size([3, 27])\n",
      "(tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<CatBackward>), tensor([nan, nan, nan], dtype=torch.float64, grad_fn=<SubBackward0>), tensor([nan, nan, nan], dtype=torch.float64, grad_fn=<AddBackward0>))\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan]], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Charlie\\AppData\\Local\\Temp/ipykernel_17016/1918419272.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  localX = to_var(torch.tensor(b[i]))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `x >= 0. && x <= 1.' failed. input value should be between 0~1, but got -nan(ind) at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\THNN/generic/BCECriterion.c:62",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17016/1918419272.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mrecon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocalX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocalX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#doing kl-divergence loss correctly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \"\"\"This bound (kl loss) provides a unified objective function for \n\u001b[0;32m     26\u001b[0m         op-timization of both the parameters θ and φ of the model and variational approximation, respectively.\"\"\"\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17016/1586302299.py\u001b[0m in \u001b[0;36mloss_fn\u001b[1;34m(recon_x, x, mu, logvar)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#print(recon_x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#print(x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mBCE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, size_average=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# see Appendix B from VAE paper:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2074\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2076\u001b[1;33m     return torch._C._nn.binary_cross_entropy(\n\u001b[0m\u001b[0;32m   2077\u001b[0m         input, target, weight, reduction_enum)\n\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Assertion `x >= 0. && x <= 1.' failed. input value should be between 0~1, but got -nan(ind) at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\THNN/generic/BCECriterion.c:62"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "anomaly_history = []\n",
    "loss_history = []\n",
    "avgSum = 0\n",
    "avgCount = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    b = get_batch(X,batch)\n",
    "    #print(demo.rnn(b).shape)\n",
    "    #b = demo.rnn(b)\n",
    "    #print(range(batch))\n",
    "    for i in range(batch):\n",
    "        #localX = torch.tensor(b[i].cuda())\n",
    "        optimizer.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        optimizer3.zero_grad()\n",
    "        #localX = to_var(b[i])\n",
    "        localX = to_var(torch.tensor(b[i]))\n",
    "        #print(localX.shape)\n",
    "        #print(localX.type)\n",
    "        recon, mu, logvar = demo(localX)\n",
    "        print(recon)\n",
    "        loss = loss_fn(recon, localX, mu, logvar) #doing kl-divergence loss correctly\n",
    "        \"\"\"This bound (kl loss) provides a unified objective function for \n",
    "        op-timization of both the parameters θ and φ of the model and variational approximation, respectively.\"\"\"\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer2.step()\n",
    "        optimizer3.step()\n",
    "        scheduler.step()\n",
    "        #scheduler2.step()\n",
    "        idx = idx + 1\n",
    "\n",
    "        avgSum = avgSum + torch.mean(loss/batch)\n",
    "        avgCount = avgCount + 1\n",
    "        anomaly_score = abs(torch.mean(localX-recon))\n",
    "\n",
    "        if idx%step_size == 0:\n",
    "            loss_history.append(avgSum/avgCount)\n",
    "            anomaly_history.append(anomaly_score)\n",
    "            avgSum = 0\n",
    "            avgCount = 0\n",
    "\n",
    "        if idx%100 == 0:\n",
    "            print(\"Epoch[{}/{}] Loss: {:.3f}\".format(epoch+1, epochs, loss.data.item()/batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a83b47fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2513f0e11c0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = plt.figure()\n",
    "plt.plot(loss_history,'g-',label='h 10,z 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96acd88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "t1 = tep_testing_stepped(dataTest,step_size)\n",
    "step_start = 0\n",
    "anomalies = []\n",
    "y_nomalies = []\n",
    "county = 0\n",
    "setNum = 0\n",
    "for step in t1:\n",
    "    step = to_var(torch.tensor(step,dtype=torch.float64))\n",
    "    recon,_,_ = demo(step)\n",
    "    anom = abs(torch.mean(step-recon))\n",
    "    anom2 = torch.mean(torch.tensor(faultNumbersTest[county:county+step_size],dtype=torch.float64))\n",
    "    anomalies.append(anom)\n",
    "    y_nomalies.append(anom2)\n",
    "    step_start = step_start + 1\n",
    "    county = county + step_size\n",
    "p3 = plt.figure()\n",
    "plt.plot(anomalies, 'g-')\n",
    "p4 = plt.figure()\n",
    "plt.plot(y_nomalies, 'b-')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd9b5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"C:/Users/Charlie/Desktop/picsForDemo2/tepGeneratedLast/\"\n",
    "\n",
    "strtOfTwo = int(len(dataTest)/20)\n",
    "sets = []\n",
    "runNum = 0 \n",
    "print(len(faultNumbersTest))\n",
    "\n",
    "print(\"////////////////////\")\n",
    "for i in range(20):\n",
    "    print(str(strtOfTwo*runNum) + \" : \" + str((strtOfTwo*runNum)+run_length))\n",
    "    #print(faultNumbersTest[strtOfTwo*runNum])\n",
    "    dt = dataTest[strtOfTwo*runNum:(strtOfTwo*runNum)+run_length]\n",
    "    t1 = tep_testing_stepped(dt,step_size)\n",
    "    sets.append(t1)\n",
    "    runNum = runNum + 1\n",
    "print(\"////////////////////\")\n",
    "\n",
    "setNum = 0\n",
    "for set in sets:\n",
    "    setName = \"anom\" + str(setNum+1) + \"Faulty_Flow_166\"\n",
    "    fileLoc = loc + setName + \".png\"\n",
    "    step_start = 0\n",
    "    anomalies = []\n",
    "    y_nomalies = []\n",
    "    vals = []\n",
    "    pred_vals = []\n",
    "    county = 0\n",
    "    #print(type(XX))\n",
    "    #print(len(XX))\n",
    "    #print(XX.shape)\n",
    "    for step in set:\n",
    "      step = to_var(torch.tensor(step,dtype=torch.float64))\n",
    "      if True:\n",
    "          #step = torch.tensor(XX[step_start:step_start+step_size])[0]\n",
    "          recon,_,_ = demo(step)\n",
    "          print(recon)\n",
    "          a = 1\n",
    "          #b = 1\n",
    "          for j in range(len(step)):\n",
    "            a = (step[0][j] - recon[0][j])\n",
    "            #a = a + (step[0][j] - recon[0][j])\n",
    "            #b = b * recon[0][j]\n",
    "          #anom = abs(torch.mean(step-recon))\n",
    "          #print(\"-------------: \" + str(torch.mean(step-recon)))\n",
    "          #print(str(a) + \",    \" + str(b))\n",
    "          anom = a\n",
    "          anom2 = torch.mean(torch.tensor(faultNumbersTest[(setNum*strtOfTwo)+(county*step_size):(setNum*strtOfTwo)+(county*step_size+step_size)],dtype=torch.float64))\n",
    "          #if county%100 == 0:\n",
    "            #print(\"step: \" + str(step))\n",
    "            #print(\"recon: \" + str(recon))\n",
    "            #print(anom)\n",
    "          #anomalies.append(torch.tensor(math.sqrt(abs(anom))))\n",
    "          #anomalies.append(torch.tensor(abs(anom)))\n",
    "          anomalies.append(torch.tensor(abs(anom)))\n",
    "          y_nomalies.append(anom2)\n",
    "          pred_vals.append(torch.mean(recon))\n",
    "          vals.append(torch.mean(step))\n",
    "          step_start = step_start + 1\n",
    "          county = county + 1\n",
    "    setNum = setNum + 1\n",
    "    start = 0\n",
    "    view = []\n",
    "    max = -99999\n",
    "    min = 99999\n",
    "    maxA = -99999\n",
    "    minA = 99999\n",
    "\n",
    "    print(len(anomalies))\n",
    "\n",
    "    for a in anomalies:\n",
    "        if start+1 < len(anomalies):\n",
    "            view.append(abs(anomalies[start+1].item() - a.item()))  \n",
    "            start = start + 1\n",
    "\n",
    "    for i in range(len(view)):\n",
    "        j = i + 1\n",
    "        if view[i] > max:\n",
    "            max = a.item()\n",
    "        if view[i] < min:\n",
    "            min = a.item()\n",
    "        if i < len(view)-1:\n",
    "            #print(\"i: \" + str(view[i]))\n",
    "            #print(\"j: \" + str(view[j]))\n",
    "            v = abs(view[i]-view[j])\n",
    "            #print(\"v: \" + str(v))\n",
    "            if v > maxA:\n",
    "                maxA = v\n",
    "            if v < minA:\n",
    "                minA = v\n",
    "\n",
    "    print()\n",
    "    print(max)\n",
    "    print(maxA)\n",
    "    print(minA)\n",
    "\n",
    "    #could get loc min and max given a step size rather than literally between individual points\n",
    "\n",
    "    #ADD LINEAR REGRESSION LINE\n",
    "    \n",
    "    p3 = plt.figure()\n",
    "    plt.plot(anomalies, 'g-')\n",
    "    plt.savefig(fileLoc)\n",
    "    p4 = plt.figure()\n",
    "    #plt.plot(y_nomalies, 'b-')\n",
    "    plt.plot(vals, 'b-')\n",
    "    p5 = plt.figure()\n",
    "    plt.plot(pred_vals, 'r-')\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88e781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3cf57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d681334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
