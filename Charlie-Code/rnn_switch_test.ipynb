{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b163925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "#import torch.cuda\n",
    "import random\n",
    "from itertools import chain as chain\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "#conda activate base\n",
    "cudaOn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27468186",
   "metadata": {},
   "outputs": [],
   "source": [
    "tepLoc = \"C:/Users/Charlie/Desktop/TEP_Data/\"\n",
    "\n",
    "#tepTrain = tepLoc + \"TEP_Faulty_Training.csv\"\n",
    "tepTrain = tepLoc + \"TEP_FaultFree_Training.csv\"\n",
    "\n",
    "#tepTest = tepLoc + \"TEP_FaultFree_Testing.csv\"\n",
    "tepTest = tepLoc + \"TEP_Faulty_Testing.csv\"\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#data = pd.read_csv('c172_file_1.csv')\n",
    "data = pd.read_csv(tepTrain)\n",
    "dataTest = pd.read_csv(tepTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9080601b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>xmeas_1</th>\n",
       "      <th>xmeas_2</th>\n",
       "      <th>xmeas_3</th>\n",
       "      <th>xmeas_4</th>\n",
       "      <th>xmeas_5</th>\n",
       "      <th>xmeas_6</th>\n",
       "      <th>xmeas_7</th>\n",
       "      <th>xmeas_8</th>\n",
       "      <th>xmeas_9</th>\n",
       "      <th>...</th>\n",
       "      <th>xmv_2</th>\n",
       "      <th>xmv_3</th>\n",
       "      <th>xmv_4</th>\n",
       "      <th>xmv_5</th>\n",
       "      <th>xmv_6</th>\n",
       "      <th>xmv_7</th>\n",
       "      <th>xmv_8</th>\n",
       "      <th>xmv_9</th>\n",
       "      <th>xmv_10</th>\n",
       "      <th>xmv_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.25038</td>\n",
       "      <td>3674.0</td>\n",
       "      <td>4529.0</td>\n",
       "      <td>9.2320</td>\n",
       "      <td>26.889</td>\n",
       "      <td>42.402</td>\n",
       "      <td>2704.3</td>\n",
       "      <td>74.863</td>\n",
       "      <td>120.41</td>\n",
       "      <td>...</td>\n",
       "      <td>53.744</td>\n",
       "      <td>24.657</td>\n",
       "      <td>62.544</td>\n",
       "      <td>22.137</td>\n",
       "      <td>39.935</td>\n",
       "      <td>42.323</td>\n",
       "      <td>47.757</td>\n",
       "      <td>47.510</td>\n",
       "      <td>41.258</td>\n",
       "      <td>18.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.25109</td>\n",
       "      <td>3659.4</td>\n",
       "      <td>4556.6</td>\n",
       "      <td>9.4264</td>\n",
       "      <td>26.721</td>\n",
       "      <td>42.576</td>\n",
       "      <td>2705.0</td>\n",
       "      <td>75.000</td>\n",
       "      <td>120.41</td>\n",
       "      <td>...</td>\n",
       "      <td>53.414</td>\n",
       "      <td>24.588</td>\n",
       "      <td>59.259</td>\n",
       "      <td>22.084</td>\n",
       "      <td>40.176</td>\n",
       "      <td>38.554</td>\n",
       "      <td>43.692</td>\n",
       "      <td>47.427</td>\n",
       "      <td>41.359</td>\n",
       "      <td>17.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.25038</td>\n",
       "      <td>3660.3</td>\n",
       "      <td>4477.8</td>\n",
       "      <td>9.4426</td>\n",
       "      <td>26.875</td>\n",
       "      <td>42.070</td>\n",
       "      <td>2706.2</td>\n",
       "      <td>74.771</td>\n",
       "      <td>120.42</td>\n",
       "      <td>...</td>\n",
       "      <td>54.357</td>\n",
       "      <td>24.666</td>\n",
       "      <td>61.275</td>\n",
       "      <td>22.380</td>\n",
       "      <td>40.244</td>\n",
       "      <td>38.990</td>\n",
       "      <td>46.699</td>\n",
       "      <td>47.468</td>\n",
       "      <td>41.199</td>\n",
       "      <td>20.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.24977</td>\n",
       "      <td>3661.3</td>\n",
       "      <td>4512.1</td>\n",
       "      <td>9.4776</td>\n",
       "      <td>26.758</td>\n",
       "      <td>42.063</td>\n",
       "      <td>2707.2</td>\n",
       "      <td>75.224</td>\n",
       "      <td>120.39</td>\n",
       "      <td>...</td>\n",
       "      <td>53.946</td>\n",
       "      <td>24.725</td>\n",
       "      <td>59.856</td>\n",
       "      <td>22.277</td>\n",
       "      <td>40.257</td>\n",
       "      <td>38.072</td>\n",
       "      <td>47.541</td>\n",
       "      <td>47.658</td>\n",
       "      <td>41.643</td>\n",
       "      <td>18.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.29405</td>\n",
       "      <td>3679.0</td>\n",
       "      <td>4497.0</td>\n",
       "      <td>9.3381</td>\n",
       "      <td>26.889</td>\n",
       "      <td>42.650</td>\n",
       "      <td>2705.1</td>\n",
       "      <td>75.388</td>\n",
       "      <td>120.39</td>\n",
       "      <td>...</td>\n",
       "      <td>53.658</td>\n",
       "      <td>28.797</td>\n",
       "      <td>60.717</td>\n",
       "      <td>21.947</td>\n",
       "      <td>39.144</td>\n",
       "      <td>41.955</td>\n",
       "      <td>47.645</td>\n",
       "      <td>47.346</td>\n",
       "      <td>41.507</td>\n",
       "      <td>18.461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249995</th>\n",
       "      <td>496</td>\n",
       "      <td>0.29325</td>\n",
       "      <td>3640.1</td>\n",
       "      <td>4473.0</td>\n",
       "      <td>9.1949</td>\n",
       "      <td>26.867</td>\n",
       "      <td>42.379</td>\n",
       "      <td>2700.2</td>\n",
       "      <td>75.533</td>\n",
       "      <td>120.41</td>\n",
       "      <td>...</td>\n",
       "      <td>53.429</td>\n",
       "      <td>29.249</td>\n",
       "      <td>60.773</td>\n",
       "      <td>21.532</td>\n",
       "      <td>40.451</td>\n",
       "      <td>34.064</td>\n",
       "      <td>48.953</td>\n",
       "      <td>48.291</td>\n",
       "      <td>40.812</td>\n",
       "      <td>18.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249996</th>\n",
       "      <td>497</td>\n",
       "      <td>0.29134</td>\n",
       "      <td>3625.7</td>\n",
       "      <td>4506.2</td>\n",
       "      <td>9.2109</td>\n",
       "      <td>26.889</td>\n",
       "      <td>42.291</td>\n",
       "      <td>2700.6</td>\n",
       "      <td>75.935</td>\n",
       "      <td>120.39</td>\n",
       "      <td>...</td>\n",
       "      <td>53.830</td>\n",
       "      <td>28.975</td>\n",
       "      <td>61.517</td>\n",
       "      <td>21.750</td>\n",
       "      <td>42.762</td>\n",
       "      <td>42.645</td>\n",
       "      <td>51.055</td>\n",
       "      <td>48.589</td>\n",
       "      <td>40.933</td>\n",
       "      <td>19.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249997</th>\n",
       "      <td>498</td>\n",
       "      <td>0.29438</td>\n",
       "      <td>3600.2</td>\n",
       "      <td>4478.3</td>\n",
       "      <td>9.1957</td>\n",
       "      <td>26.820</td>\n",
       "      <td>42.448</td>\n",
       "      <td>2700.3</td>\n",
       "      <td>74.706</td>\n",
       "      <td>120.41</td>\n",
       "      <td>...</td>\n",
       "      <td>54.163</td>\n",
       "      <td>28.676</td>\n",
       "      <td>61.656</td>\n",
       "      <td>21.487</td>\n",
       "      <td>42.109</td>\n",
       "      <td>39.770</td>\n",
       "      <td>46.770</td>\n",
       "      <td>48.648</td>\n",
       "      <td>41.465</td>\n",
       "      <td>19.344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249998</th>\n",
       "      <td>499</td>\n",
       "      <td>0.25269</td>\n",
       "      <td>3683.5</td>\n",
       "      <td>4486.4</td>\n",
       "      <td>9.2832</td>\n",
       "      <td>27.188</td>\n",
       "      <td>42.757</td>\n",
       "      <td>2697.4</td>\n",
       "      <td>75.101</td>\n",
       "      <td>120.39</td>\n",
       "      <td>...</td>\n",
       "      <td>53.453</td>\n",
       "      <td>24.889</td>\n",
       "      <td>61.564</td>\n",
       "      <td>21.392</td>\n",
       "      <td>39.334</td>\n",
       "      <td>42.274</td>\n",
       "      <td>43.623</td>\n",
       "      <td>48.797</td>\n",
       "      <td>39.835</td>\n",
       "      <td>18.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249999</th>\n",
       "      <td>500</td>\n",
       "      <td>0.25214</td>\n",
       "      <td>3648.2</td>\n",
       "      <td>4467.8</td>\n",
       "      <td>9.1344</td>\n",
       "      <td>26.886</td>\n",
       "      <td>42.534</td>\n",
       "      <td>2695.1</td>\n",
       "      <td>74.787</td>\n",
       "      <td>120.41</td>\n",
       "      <td>...</td>\n",
       "      <td>53.676</td>\n",
       "      <td>24.943</td>\n",
       "      <td>61.254</td>\n",
       "      <td>21.208</td>\n",
       "      <td>38.991</td>\n",
       "      <td>42.873</td>\n",
       "      <td>44.400</td>\n",
       "      <td>48.876</td>\n",
       "      <td>41.076</td>\n",
       "      <td>16.158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sample  xmeas_1  xmeas_2  xmeas_3  xmeas_4  xmeas_5  xmeas_6  xmeas_7  \\\n",
       "0            1  0.25038   3674.0   4529.0   9.2320   26.889   42.402   2704.3   \n",
       "1            2  0.25109   3659.4   4556.6   9.4264   26.721   42.576   2705.0   \n",
       "2            3  0.25038   3660.3   4477.8   9.4426   26.875   42.070   2706.2   \n",
       "3            4  0.24977   3661.3   4512.1   9.4776   26.758   42.063   2707.2   \n",
       "4            5  0.29405   3679.0   4497.0   9.3381   26.889   42.650   2705.1   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "249995     496  0.29325   3640.1   4473.0   9.1949   26.867   42.379   2700.2   \n",
       "249996     497  0.29134   3625.7   4506.2   9.2109   26.889   42.291   2700.6   \n",
       "249997     498  0.29438   3600.2   4478.3   9.1957   26.820   42.448   2700.3   \n",
       "249998     499  0.25269   3683.5   4486.4   9.2832   27.188   42.757   2697.4   \n",
       "249999     500  0.25214   3648.2   4467.8   9.1344   26.886   42.534   2695.1   \n",
       "\n",
       "        xmeas_8  xmeas_9  ...   xmv_2   xmv_3   xmv_4   xmv_5   xmv_6   xmv_7  \\\n",
       "0        74.863   120.41  ...  53.744  24.657  62.544  22.137  39.935  42.323   \n",
       "1        75.000   120.41  ...  53.414  24.588  59.259  22.084  40.176  38.554   \n",
       "2        74.771   120.42  ...  54.357  24.666  61.275  22.380  40.244  38.990   \n",
       "3        75.224   120.39  ...  53.946  24.725  59.856  22.277  40.257  38.072   \n",
       "4        75.388   120.39  ...  53.658  28.797  60.717  21.947  39.144  41.955   \n",
       "...         ...      ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "249995   75.533   120.41  ...  53.429  29.249  60.773  21.532  40.451  34.064   \n",
       "249996   75.935   120.39  ...  53.830  28.975  61.517  21.750  42.762  42.645   \n",
       "249997   74.706   120.41  ...  54.163  28.676  61.656  21.487  42.109  39.770   \n",
       "249998   75.101   120.39  ...  53.453  24.889  61.564  21.392  39.334  42.274   \n",
       "249999   74.787   120.41  ...  53.676  24.943  61.254  21.208  38.991  42.873   \n",
       "\n",
       "         xmv_8   xmv_9  xmv_10  xmv_11  \n",
       "0       47.757  47.510  41.258  18.447  \n",
       "1       43.692  47.427  41.359  17.194  \n",
       "2       46.699  47.468  41.199  20.530  \n",
       "3       47.541  47.658  41.643  18.089  \n",
       "4       47.645  47.346  41.507  18.461  \n",
       "...        ...     ...     ...     ...  \n",
       "249995  48.953  48.291  40.812  18.756  \n",
       "249996  51.055  48.589  40.933  19.360  \n",
       "249997  46.770  48.648  41.465  19.344  \n",
       "249998  43.623  48.797  39.835  18.512  \n",
       "249999  44.400  48.876  41.076  16.158  \n",
       "\n",
       "[250000 rows x 53 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop('Unnamed: 0',axis=1)\n",
    "data = data.drop('faultNumber',axis=1)\n",
    "data = data.drop('simulationRun',axis=1)\n",
    "#data = data.drop('sample',axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a897cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTest = dataTest[dataTest['simulationRun']==310]\n",
    "faultNumbersT = dataTest.get('faultNumber')\n",
    "#\n",
    "dataTest = dataTest.drop('Unnamed: 0',axis=1)\n",
    "dataTest = dataTest.drop('faultNumber',axis=1)\n",
    "dataTest = dataTest.drop('simulationRun',axis=1)\n",
    "#dataTest = dataTest.drop('sample',axis=1)\n",
    "#dataTest = data\n",
    "#dataTest = dataTest.iloc(0)[0:19500] #test A and B\n",
    "#dataTest = dataTest.iloc(0)[19500:38500] #test C and D\n",
    "#dataTest = dataTest.iloc(0)[39000:58000]\n",
    "\n",
    "faultNumbersTest = []\n",
    "for i in faultNumbersT:\n",
    "    faultNumbersTest.append(i)\n",
    "\n",
    "data = data.astype('float64')\n",
    "dataTest = dataTest.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad0fe51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1\n",
      "960: 2\n",
      "1920: 3\n",
      "2880: 4\n",
      "3840: 5\n",
      "4800: 6\n",
      "5760: 7\n",
      "6720: 8\n",
      "7680: 9\n",
      "8640: 10\n",
      "9600: 11\n",
      "10560: 12\n",
      "11520: 13\n",
      "12480: 14\n",
      "13440: 15\n",
      "14400: 16\n",
      "15360: 17\n",
      "16320: 18\n",
      "17280: 19\n",
      "18240: 20\n",
      "         sample  xmeas_1  xmeas_2  xmeas_3  xmeas_4  xmeas_5  xmeas_6  \\\n",
      "5932800     1.0  0.25191   3673.9   4530.7   9.4061   27.112   42.114   \n",
      "5932801     2.0  0.25011   3682.9   4535.5   9.3432   26.843   42.271   \n",
      "5932802     3.0  0.25058   3680.2   4498.5   9.4781   27.203   42.433   \n",
      "5932803     4.0  0.25071   3675.6   4503.6   9.4455   26.795   42.501   \n",
      "5932804     5.0  0.29856   3635.2   4492.9   9.4382   26.682   42.184   \n",
      "...         ...      ...      ...      ...      ...      ...      ...   \n",
      "5951995   956.0  0.27327   3700.6   4471.6   9.3316   26.827   42.340   \n",
      "5951996   957.0  0.22831   3649.8   4495.6   9.5169   26.830   41.940   \n",
      "5951997   958.0  0.22671   3660.0   4479.1   9.4543   26.392   42.255   \n",
      "5951998   959.0  0.27968   3657.6   4469.4   9.3489   26.713   42.744   \n",
      "5951999   960.0  0.27997   3641.4   4485.6   9.3813   26.539   42.131   \n",
      "\n",
      "         xmeas_7  xmeas_8  xmeas_9  ...   xmv_2   xmv_3   xmv_4   xmv_5  \\\n",
      "5932800   2704.1   74.968   120.39  ...  53.724  24.507  58.504  21.829   \n",
      "5932801   2705.7   74.385   120.41  ...  53.667  24.684  61.330  22.074   \n",
      "5932802   2706.4   74.853   120.41  ...  54.108  24.646  58.236  21.819   \n",
      "5932803   2707.1   74.950   120.43  ...  54.048  24.633  59.877  21.843   \n",
      "5932804   2705.3   74.584   120.38  ...  53.869  28.987  59.506  22.204   \n",
      "...          ...      ...      ...  ...     ...     ...     ...     ...   \n",
      "5951995   2696.5   74.100   120.39  ...  53.194  27.060  61.227  21.047   \n",
      "5951996   2699.3   75.067   120.40  ...  53.868  22.238  61.442  21.552   \n",
      "5951997   2699.5   75.162   120.43  ...  54.066  22.395  60.835  21.082   \n",
      "5951998   2702.7   75.602   120.38  ...  53.431  27.466  60.318  20.985   \n",
      "5951999   2703.1   74.189   120.40  ...  53.238  27.438  62.081  20.842   \n",
      "\n",
      "          xmv_6   xmv_7   xmv_8   xmv_9  xmv_10  xmv_11  \n",
      "5932800  39.603  46.148  45.683  47.579  41.246  18.732  \n",
      "5932801  40.088  35.377  47.637  47.506  41.359  16.555  \n",
      "5932802  39.847  38.281  46.350  47.575  40.375  16.144  \n",
      "5932803  40.164  36.584  47.918  47.318  41.977  18.449  \n",
      "5932804  38.968  38.446  47.183  47.609  40.533  19.471  \n",
      "...         ...     ...     ...     ...     ...     ...  \n",
      "5951995  39.816  42.675  45.616  41.621  40.766  20.372  \n",
      "5951996  41.562  40.058  39.737  41.770  41.045  18.918  \n",
      "5951997  41.527  39.177  47.184  41.680  40.743  17.757  \n",
      "5951998  37.894  35.452  48.564  41.457  40.431  19.973  \n",
      "5951999  37.852  41.124  43.769  41.404  40.985  18.528  \n",
      "\n",
      "[19200 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "run_length = 960\n",
    "batch_size = 512\n",
    "#num = 960\n",
    "for i in range(int(len(faultNumbersTest)/run_length)):\n",
    "    print(str(i*run_length) + \": \" + str(faultNumbersTest[i*run_length]))\n",
    "print(dataTest)\n",
    "numVariables = 53#52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e68f48c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split and reshape the data set by step_size , use min-max or stanrdardlize method to rescale the data\n",
    "def Splitting_dataset(data, step_size, scale=True, scaler_type=MinMaxScaler):\n",
    "        l = len(data) \n",
    "        data = scaler_type().fit_transform(data)\n",
    "        Xs = []\n",
    "        Ys = []\n",
    "        for i in range(0, (len(data) - step_size)):\n",
    "            Xs.append(data[i:i+step_size])\n",
    "            Ys.append(data[i:i+step_size])\n",
    "        train_x, test_x, train_y, test_y = [np.array(x) for x in train_test_split(Xs, Ys)]\n",
    "        assert train_x.shape[2] == test_x.shape[2] == (data.shape[1] if (type(data) == np.ndarray) else len(data))\n",
    "        return  (train_x.shape[2], train_x, train_y, test_x, test_y)\n",
    "    \n",
    "def get_batch(x, batch_size):\n",
    "    \"\"\"Made with taking test_x or XX as input\"\"\"\n",
    "    t = 0\n",
    "    while t >= 0:\n",
    "        x_mod = len(x) % batch_size\n",
    "        start = random.random() * (len(x)-x_mod)\n",
    "        start = int(start)\n",
    "        if start + batch_size < len(x):\n",
    "            t = t-1\n",
    "    batch = torch.tensor(x[start:(start+batch_size)]) #!! added tensor line\n",
    "    #print(batch.shape)\n",
    "    return batch\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    \n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu**2 -  logvar.exp())\n",
    "        return BCE + KLD\n",
    "    \n",
    "def tep_testing_stepped(dat,step_size):\n",
    "    res = []\n",
    "    ind = 0\n",
    "    scale = MinMaxScaler().fit(dat)\n",
    "    dat = pd.DataFrame(scale.transform(dat))\n",
    "    #print(int((len(data)/step_size)))\n",
    "    for i in range(int((len(dat)/step_size))):\n",
    "        if ind + step_size < len(dat):\n",
    "            step = []\n",
    "            for j in range(step_size):\n",
    "              #print(data.iloc(0)[ind])#[ind])\n",
    "              step.append(dat.iloc(0)[ind])\n",
    "              ind = ind + 1\n",
    "            res.append(step)\n",
    "    #print(res.shape)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b2ccdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=27, z_dim=31, n_flow_steps=1):\n",
    "        super(VAE, self).__init__()\n",
    "        #self.gruCheck = True\n",
    "        self.rnn = nn.GRU(image_size,53,batch_size)#,batch_first=True)#,bidirectional=True)#,bidirectional=True)\n",
    "        #self.encodeCell = nn.GRUCell(image_size*2,image_size)\n",
    "        \n",
    "        #lol you cant use GRU in sequential\n",
    "        #mayybe cuz of built in stuffs buttt\n",
    "        #its output is both an output and a hidden state, so it cant just pass a single output on\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            #nn.GRU(image_size,image_size,batch_first=True),\n",
    "            nn.Linear(image_size, h_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(h_dim, z_dim*2) #is it saying its getting a mu and a var for each z dim out?\n",
    "            \n",
    "            #how can I represent the encoder as a distribution acting as the prior?\n",
    "        )\n",
    "        print(z_dim*2)\n",
    "        self.decoder = nn.Sequential(\n",
    "            #nn.GRUCell(z_dim,z_dim),\n",
    "            nn.Linear(z_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, image_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        print(image_size)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_() \n",
    "        esp = to_var(torch.randn(*mu.size()))\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #q = self.recurrent(x)\n",
    "        #print(\"--\" + str(x.shape))\n",
    "        #if self.gruCheck == False:\n",
    "            #self.gruOutput = self.encodeCell(torch.cat([x,self.gruOutput],1))\n",
    "        #else:\n",
    "            #self.gruOutput = self.encodeCell(x)\n",
    "            #self.gruCheck = False\n",
    "        print(\"//////////////////\")\n",
    "        print(self.encoder(x).shape)\n",
    "        h = self.encoder(self.rnn(x)[0])\n",
    "        print(h.shape)\n",
    "        print(\"//////////////////\")\n",
    "        #print(\"--\" + str(h.shape))\n",
    "        mu, logvar = torch.chunk(h, 2, dim=1)\n",
    "        #print(mu.shape)\n",
    "        #print(logvar.shape)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        #print(z.shape)\n",
    "        #z = z.float()\n",
    "        z = model(z)\n",
    "        #print(z)\n",
    "        tensorZ = z[0]#torch.tensor(z[0])\n",
    "        #print(tensorZ.shape)\n",
    "        #print(z[0])\n",
    "        #print(\"--\" + str(self.decoder(tensorZ).shape))\n",
    "\n",
    "        return self.decoder(tensorZ), mu, logvar\n",
    "    \n",
    "\n",
    "class stacked_NVP(nn.Module):\n",
    "    def __init__(self, d, k, hidden, n):\n",
    "        super().__init__()\n",
    "        self.bijectors = nn.ModuleList([\n",
    "            R_NVP(d, k, hidden=hidden) for _ in range(n)\n",
    "        ])\n",
    "        self.flips = [True if i%2 else False for i in range(n)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        log_jacobs = []\n",
    "\n",
    "        for bijector, f in zip(self.bijectors, self.flips):\n",
    "            x, log_pz, lj = bijector(x, flip=f)\n",
    "            log_jacobs.append(lj)\n",
    "        \n",
    "        return x, log_pz, sum(log_jacobs)\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        for bijector, f in zip(reversed(self.bijectors), reversed(self.flips)):\n",
    "            z = bijector.inverse(z, flip=f)\n",
    "        return z\n",
    "    \n",
    "class R_NVP(nn.Module):\n",
    "    def __init__(self, d, k, hidden):\n",
    "        super().__init__()\n",
    "        self.d, self.k = d, k\n",
    "        self.sig_net = nn.Sequential(\n",
    "                    nn.Linear(k, hidden),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(hidden, d - k))\n",
    "\n",
    "        self.mu_net = nn.Sequential(\n",
    "                    nn.Linear(k, hidden),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(hidden, d - k))\n",
    "\n",
    "    def forward(self, x, flip=False):\n",
    "        x1, x2 = x[:, :self.k], x[:, self.k:] \n",
    "\n",
    "        if flip:\n",
    "            x2, x1 = x1, x2\n",
    "        \n",
    "        # forward\n",
    "        sig = self.sig_net(x1)\n",
    "        z1, z2 = x1, x2 * torch.exp(sig) + self.mu_net(x1)\n",
    "        \n",
    "        if flip:\n",
    "            z2, z1 = z1, z2\n",
    "        \n",
    "        z_hat = torch.cat([z1, z2], dim=-1)\n",
    "\n",
    "        log_pz = base_dist.log_prob(z_hat)\n",
    "        log_jacob = sig.sum(-1)\n",
    "        \n",
    "        return z_hat, log_pz, log_jacob\n",
    "    \n",
    "    def inverse(self, Z, flip=False):\n",
    "        z1, z2 = Z[:, :self.k], Z[:, self.k:] \n",
    "        \n",
    "        if flip:\n",
    "            z2, z1 = z1, z2\n",
    "        \n",
    "        x1 = z1\n",
    "        x2 = (z2 - self.mu_net(z1)) * torch.exp(-self.sig_net(z1))\n",
    "        \n",
    "        if flip:\n",
    "            x2, x1 = x1, x2\n",
    "        return torch.cat([x1, x2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3f31e4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0., 0.], dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.]], dtype=torch.float64, requires_grad=True)\n",
      "4\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "step_size = 3\n",
    "batch = 512\n",
    "index_step_length = numVariables\n",
    "epochs = 30\n",
    "\n",
    "num = 7\n",
    "\n",
    "d = 2\n",
    "k = 1\n",
    "\n",
    "base_mu, base_cov = torch.zeros(2), torch.eye(2)\n",
    "\n",
    "base_mu = torch.nn.parameter.Parameter(to_var(base_mu.double()))\n",
    "base_cov = torch.nn.parameter.Parameter(to_var(base_cov.double()))\n",
    "#base_mu = torch.nn.parameter.Parameter(base_mu,requires_grad=True)\n",
    "#base_cov = torch.nn.parameter.Parameter(base_cov,requires_grad=True)\n",
    "print(base_mu)\n",
    "print(base_cov)\n",
    "base_dist = MultivariateNormal(base_mu, base_cov)\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "labels, X, Y, XX, YY = Splitting_dataset(data, step_size)\n",
    "#XX.cuda()\n",
    "demo = VAE(index_step_length,h_dim=3,z_dim=2)\n",
    "model = stacked_NVP(d, k, hidden=3,n=num)\n",
    "demo.double()\n",
    "model.double()\n",
    "    \n",
    "#next set of tests should be with n=3, last set was with n=1\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "optimizer2 = torch.optim.Adam(demo.parameters(), lr=1e-3)\n",
    "optimizer3 = torch.optim.Adam([base_mu,base_cov], lr=1e-3)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.999)\n",
    "\n",
    "if torch.cuda.is_available() & cudaOn:\n",
    "    demo.cuda()\n",
    "    print(\"demo done\")\n",
    "    model.cuda()\n",
    "    print(\"model done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0214a761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Charlie\\AppData\\Local\\Temp/ipykernel_15104/1771369540.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  localX = to_var(torch.tensor(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 3, 53])\n",
      "torch.Size([512, 3, 53])\n",
      "torch.Size([512, 3, 53])\n",
      "//////////////////\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [3 x 106], m2: [53 x 3] at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\TH/generic/THTensorMath.cpp:136",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15104/1771369540.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mlocalX2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlocalX2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mrecon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocalX2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocalX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#doing kl-divergence loss correctly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15104/1777188078.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;31m#self.gruCheck = False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"//////////////////\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1368\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [3 x 106], m2: [53 x 3] at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\TH/generic/THTensorMath.cpp:136"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "anomaly_history = []\n",
    "loss_history = []\n",
    "avgSum = 0\n",
    "avgCount = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    b = get_batch(X,batch)\n",
    "    #print(b[0])\n",
    "    #b = demo.rnn(b)[0]\n",
    "    #print(range(batch))\n",
    "    hidden = torch.zeros([512,3,53])\n",
    "    for i in range(batch):\n",
    "        #localX = torch.tensor(b[i].cuda())\n",
    "        optimizer.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        optimizer3.zero_grad()\n",
    "        #localX = to_var(torch.tensor(b[i]))\n",
    "        \n",
    "        #print(localX.shape)\n",
    "        localX = to_var(torch.tensor(b))\n",
    "        print(localX.shape)\n",
    "        localX2, hidden = demo.rnn.forward(localX)\n",
    "        print(localX2.shape)\n",
    "        print(hidden.shape)\n",
    "        localX2 = torch.cat([localX2,hidden],axis=2)\n",
    "        recon, mu, logvar = demo(localX2[0])\n",
    "        \n",
    "        loss = loss_fn(recon, localX, mu, logvar) #doing kl-divergence loss correctly\n",
    "        \"\"\"\n",
    "        This bound (kl loss) provides a unified objective function for \n",
    "        op-timization of both the parameters θ and φ of the model and variational approximation, respectively.\"\n",
    "        \"\"\"\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer2.step()\n",
    "        optimizer3.step()\n",
    "        scheduler.step()\n",
    "        idx = idx + 1\n",
    "\n",
    "        avgSum = avgSum + torch.mean(loss/batch)\n",
    "        avgCount = avgCount + 1\n",
    "        anomaly_score = abs(torch.mean(localX-recon))\n",
    "\n",
    "        if idx%30 == 0:\n",
    "            loss_history.append(avgSum/avgCount)\n",
    "            anomaly_history.append(anomaly_score)\n",
    "            avgSum = 0\n",
    "            avgCount = 0\n",
    "\n",
    "        if idx%100 == 0:\n",
    "            print(\"Epoch[{}/{}] Loss: {:.3f}\".format(epoch+1, epochs, loss.data.item()/batch))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1274e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure()\n",
    "plt.plot(loss_history,'g-',label='h 10,z 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c36fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"C:/Users/Charlie/Desktop/picsForDemo2/tepGeneratedNeat/\"\n",
    "\n",
    "strtOfTwo = int(len(dataTest)/20)\n",
    "sets = []\n",
    "runNum = 0 \n",
    "print(len(faultNumbersTest))\n",
    "\n",
    "print(\"////////////////////\")\n",
    "for i in range(20):\n",
    "    #print(str(strtOfTwo*runNum) + \" : \" + str((strtOfTwo*runNum)+run_length))\n",
    "    #print(faultNumbersTest[strtOfTwo*runNum])\n",
    "    dt = dataTest[strtOfTwo*runNum:(strtOfTwo*runNum)+run_length]\n",
    "    t1 = tep_testing_stepped(dt,step_size)\n",
    "    sets.append(t1)\n",
    "    runNum = runNum + 1\n",
    "print(\"////////////////////\")\n",
    "print(sets[0][0][0].shape)\n",
    "\n",
    "setNum = 0\n",
    "for set in sets:\n",
    "    setName = \"anom\" + str(setNum+1) + \"_Faulty_2\"\n",
    "    fileLoc = loc + setName + \".png\"\n",
    "    step_start = 0\n",
    "    anomalies = []\n",
    "    y_nomalies = []\n",
    "    vals = []\n",
    "    pred_vals = []\n",
    "    county = 0\n",
    "    #print(type(XX))\n",
    "    #print(len(XX))\n",
    "    #print(XX.shape)\n",
    "    for step in set:\n",
    "      step = to_var(torch.tensor(step,dtype=torch.float64))\n",
    "      if True:\n",
    "          #step = torch.tensor(XX[step_start:step_start+step_size])[0]\n",
    "          recon,_,_ = demo(step)\n",
    "          a = 1\n",
    "          #b = 1\n",
    "          for j in range(len(step)):\n",
    "            a = a + (step[0][j] - recon[0][j])\n",
    "            #b = b * recon[0][j]\n",
    "          #anom = abs(torch.mean(step-recon))\n",
    "          #print(\"-------------: \" + str(torch.mean(step-recon)))\n",
    "          #print(str(a) + \",    \" + str(b))\n",
    "          anom = a\n",
    "          anom2 = torch.mean(torch.tensor(faultNumbersTest[(setNum*strtOfTwo)+(county*step_size):(setNum*strtOfTwo)+(county*step_size+step_size)],dtype=torch.float64))\n",
    "          #if county%100 == 0:\n",
    "            #print(\"step: \" + str(step))\n",
    "            #print(\"recon: \" + str(recon))\n",
    "            #print(anom)\n",
    "          anomalies.append(torch.tensor(math.sqrt(abs(anom))))\n",
    "          y_nomalies.append(anom2)\n",
    "          pred_vals.append(torch.mean(recon))\n",
    "          vals.append(torch.mean(step))\n",
    "          step_start = step_start + 1\n",
    "          county = county + 1\n",
    "    setNum = setNum + 1\n",
    "    start = 0\n",
    "    view = []\n",
    "    max = -99999\n",
    "    min = 99999\n",
    "    maxA = -99999\n",
    "    minA = 99999\n",
    "\n",
    "    print(len(anomalies))\n",
    "\n",
    "    for a in anomalies:\n",
    "        if start+1 < len(anomalies):\n",
    "            view.append(abs(anomalies[start+1].item() - a.item()))  \n",
    "            start = start + 1\n",
    "\n",
    "    for i in range(len(view)):\n",
    "        j = i + 1\n",
    "        if view[i] > max:\n",
    "            max = a.item()\n",
    "        if view[i] < min:\n",
    "            min = a.item()\n",
    "        if i < len(view)-1:\n",
    "            #print(\"i: \" + str(view[i]))\n",
    "            #print(\"j: \" + str(view[j]))\n",
    "            v = abs(view[i]-view[j])\n",
    "            #print(\"v: \" + str(v))\n",
    "            if v > maxA:\n",
    "                maxA = v\n",
    "            if v < minA:\n",
    "                minA = v\n",
    "\n",
    "    print()\n",
    "    print(max)\n",
    "    print(maxA)\n",
    "    print(minA)\n",
    "\n",
    "    #could get loc min and max given a step size rather than literally between individual points\n",
    "\n",
    "    p3 = plt.figure()\n",
    "    plt.plot(anomalies, 'g-')\n",
    "    plt.savefig(fileLoc)\n",
    "    p4 = plt.figure()\n",
    "    #plt.plot(y_nomalies, 'b-')\n",
    "    plt.plot(vals, 'b-')\n",
    "    p5 = plt.figure()\n",
    "    plt.plot(pred_vals, 'r-')\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963e1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
