{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17017c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from itertools import chain as chain\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6507ca8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AltAGL</th>\n",
       "      <th>AltB</th>\n",
       "      <th>AltGPS</th>\n",
       "      <th>AltMSL</th>\n",
       "      <th>BaroA</th>\n",
       "      <th>E1_CHT1</th>\n",
       "      <th>E1_CHT2</th>\n",
       "      <th>E1_CHT3</th>\n",
       "      <th>E1_CHT4</th>\n",
       "      <th>E1_EGT1</th>\n",
       "      <th>...</th>\n",
       "      <th>LatAc</th>\n",
       "      <th>NormAc</th>\n",
       "      <th>OAT</th>\n",
       "      <th>Pitch</th>\n",
       "      <th>Roll</th>\n",
       "      <th>TAS</th>\n",
       "      <th>VSpd</th>\n",
       "      <th>VSpdG</th>\n",
       "      <th>WndDr</th>\n",
       "      <th>WndSpd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>855.8</td>\n",
       "      <td>745.7</td>\n",
       "      <td>833.6</td>\n",
       "      <td>30.05</td>\n",
       "      <td>231.88</td>\n",
       "      <td>224.27</td>\n",
       "      <td>243.57</td>\n",
       "      <td>245.74</td>\n",
       "      <td>1047.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1.25</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.35</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>856.8</td>\n",
       "      <td>746.2</td>\n",
       "      <td>834.0</td>\n",
       "      <td>30.05</td>\n",
       "      <td>232.23</td>\n",
       "      <td>224.58</td>\n",
       "      <td>243.87</td>\n",
       "      <td>246.04</td>\n",
       "      <td>1046.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1.23</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>855.8</td>\n",
       "      <td>746.4</td>\n",
       "      <td>834.2</td>\n",
       "      <td>30.05</td>\n",
       "      <td>232.59</td>\n",
       "      <td>224.84</td>\n",
       "      <td>244.12</td>\n",
       "      <td>246.34</td>\n",
       "      <td>1046.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1.21</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.96</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>856.8</td>\n",
       "      <td>745.2</td>\n",
       "      <td>833.1</td>\n",
       "      <td>30.05</td>\n",
       "      <td>232.96</td>\n",
       "      <td>225.10</td>\n",
       "      <td>244.46</td>\n",
       "      <td>246.59</td>\n",
       "      <td>1046.32</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1.22</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.95</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>856.8</td>\n",
       "      <td>745.2</td>\n",
       "      <td>833.0</td>\n",
       "      <td>30.05</td>\n",
       "      <td>233.30</td>\n",
       "      <td>225.36</td>\n",
       "      <td>244.75</td>\n",
       "      <td>246.86</td>\n",
       "      <td>1043.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1.22</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.36</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5055</th>\n",
       "      <td>0.0</td>\n",
       "      <td>844.2</td>\n",
       "      <td>751.5</td>\n",
       "      <td>839.3</td>\n",
       "      <td>30.02</td>\n",
       "      <td>282.56</td>\n",
       "      <td>270.85</td>\n",
       "      <td>296.29</td>\n",
       "      <td>288.37</td>\n",
       "      <td>1041.83</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.17</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5056</th>\n",
       "      <td>0.0</td>\n",
       "      <td>844.2</td>\n",
       "      <td>751.6</td>\n",
       "      <td>839.4</td>\n",
       "      <td>30.02</td>\n",
       "      <td>282.49</td>\n",
       "      <td>270.90</td>\n",
       "      <td>296.36</td>\n",
       "      <td>288.45</td>\n",
       "      <td>1036.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5057</th>\n",
       "      <td>0.0</td>\n",
       "      <td>844.2</td>\n",
       "      <td>751.7</td>\n",
       "      <td>839.5</td>\n",
       "      <td>30.02</td>\n",
       "      <td>282.43</td>\n",
       "      <td>270.93</td>\n",
       "      <td>296.42</td>\n",
       "      <td>288.52</td>\n",
       "      <td>1030.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5058</th>\n",
       "      <td>1.0</td>\n",
       "      <td>845.2</td>\n",
       "      <td>752.4</td>\n",
       "      <td>840.2</td>\n",
       "      <td>30.02</td>\n",
       "      <td>282.55</td>\n",
       "      <td>271.14</td>\n",
       "      <td>296.58</td>\n",
       "      <td>288.66</td>\n",
       "      <td>1024.47</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5059</th>\n",
       "      <td>0.0</td>\n",
       "      <td>845.2</td>\n",
       "      <td>751.0</td>\n",
       "      <td>838.8</td>\n",
       "      <td>30.02</td>\n",
       "      <td>282.44</td>\n",
       "      <td>271.07</td>\n",
       "      <td>296.52</td>\n",
       "      <td>288.66</td>\n",
       "      <td>1022.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5060 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AltAGL   AltB  AltGPS  AltMSL  BaroA  E1_CHT1  E1_CHT2  E1_CHT3  \\\n",
       "0        0.0  855.8   745.7   833.6  30.05   231.88   224.27   243.57   \n",
       "1        0.0  856.8   746.2   834.0  30.05   232.23   224.58   243.87   \n",
       "2        0.0  855.8   746.4   834.2  30.05   232.59   224.84   244.12   \n",
       "3        0.0  856.8   745.2   833.1  30.05   232.96   225.10   244.46   \n",
       "4        0.0  856.8   745.2   833.0  30.05   233.30   225.36   244.75   \n",
       "...      ...    ...     ...     ...    ...      ...      ...      ...   \n",
       "5055     0.0  844.2   751.5   839.3  30.02   282.56   270.85   296.29   \n",
       "5056     0.0  844.2   751.6   839.4  30.02   282.49   270.90   296.36   \n",
       "5057     0.0  844.2   751.7   839.5  30.02   282.43   270.93   296.42   \n",
       "5058     1.0  845.2   752.4   840.2  30.02   282.55   271.14   296.58   \n",
       "5059     0.0  845.2   751.0   838.8  30.02   282.44   271.07   296.52   \n",
       "\n",
       "      E1_CHT4  E1_EGT1  ...  LatAc  NormAc   OAT  Pitch  Roll  TAS   VSpd  \\\n",
       "0      245.74  1047.12  ...   0.01    0.01  27.2   1.25 -0.21  0.0 -14.35   \n",
       "1      246.04  1046.06  ...   0.01   -0.00  27.2   1.23 -0.15  0.0   0.74   \n",
       "2      246.34  1046.17  ...   0.00   -0.01  27.2   1.21 -0.14  0.0   5.96   \n",
       "3      246.59  1046.32  ...  -0.00   -0.01  27.2   1.22 -0.14  0.0   9.95   \n",
       "4      246.86  1043.91  ...   0.01   -0.01  27.2   1.22 -0.10  0.0  13.36   \n",
       "...       ...      ...  ...    ...     ...   ...    ...   ...  ...    ...   \n",
       "5055   288.37  1041.83  ...   0.00    0.04  26.0   1.45  0.00  0.0  15.17   \n",
       "5056   288.45  1036.33  ...   0.00   -0.02  26.0   1.29  0.03  0.0  15.98   \n",
       "5057   288.52  1030.14  ...   0.00    0.01  26.0   1.76  0.02  0.0  17.44   \n",
       "5058   288.66  1024.47  ...  -0.00   -0.01  26.0   1.80  0.01  0.0  25.02   \n",
       "5059   288.66  1022.10  ...  -0.00   -0.01  26.0   1.80  0.01  0.0  17.27   \n",
       "\n",
       "      VSpdG  WndDr  WndSpd  \n",
       "0      -3.9    0.0     0.0  \n",
       "1      -3.9    0.0     0.0  \n",
       "2      -3.9    0.0     0.0  \n",
       "3      -3.9    0.0     0.0  \n",
       "4      -3.9    0.0     0.0  \n",
       "...     ...    ...     ...  \n",
       "5055   -3.9    0.0     0.0  \n",
       "5056    0.0    0.0     0.0  \n",
       "5057    0.0    0.0     0.0  \n",
       "5058    0.0    0.0     0.0  \n",
       "5059    0.0    0.0     0.0  \n",
       "\n",
       "[5060 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('c172_file_1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0ec85e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split and reshape the data set by step_size , use min-max or stanrdardlize method to rescale the data\n",
    "def Splitting_dataset(data, step_size, scale=True, scaler_type=MinMaxScaler):\n",
    "        l = len(data) \n",
    "        data = scaler_type().fit_transform(data)\n",
    "        Xs = []\n",
    "        Ys = []\n",
    "        for i in range(0, (len(data) - step_size)):\n",
    "            Xs.append(data[i:i+step_size])\n",
    "            Ys.append(data[i:i+step_size])\n",
    "        train_x, test_x, train_y, test_y = [np.array(x) for x in train_test_split(Xs, Ys)]\n",
    "        assert train_x.shape[2] == test_x.shape[2] == (data.shape[1] if (type(data) == np.ndarray) else len(data))\n",
    "        return  (train_x.shape[2], train_x, train_y, test_x, test_y)\n",
    "    \n",
    "def get_batch(x, batch_size):\n",
    "    \"\"\"Made with taking test_x or XX as input\"\"\"\n",
    "    t = 0\n",
    "    while t >= 0:\n",
    "        x_mod = len(x) % batch_size\n",
    "        start = random.random() * (len(x)-x_mod)\n",
    "        start = int(start)\n",
    "        if start + batch_size < len(x):\n",
    "            t = t-1\n",
    "    batch = x[start:(start+batch_size)]\n",
    "    #print(batch.shape)\n",
    "    return batch\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    \n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu**2 -  logvar.exp())\n",
    "        return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b3f90729",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=27, z_dim=31, n_flow_steps=1):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(image_size, h_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(h_dim, z_dim*2) #is it saying its getting a mu and a var for each z dim out?\n",
    "            \n",
    "            #how can I represent the encoder as a distribution acting as the prior?\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, image_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_() \n",
    "        esp = to_var(torch.randn(*mu.size()))\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(h, 2, dim=1)\n",
    "        #print(mu.shape)\n",
    "        #print(logvar.shape)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        #print(z.shape)\n",
    "        #z = z.float()\n",
    "        z = model(z)\n",
    "        #print(z)\n",
    "        tensorZ = z[0]#torch.tensor(z[0])\n",
    "        #print(tensorZ.shape)\n",
    "        #print(z[0])\n",
    "        return self.decoder(tensorZ), mu, logvar\n",
    "    \n",
    "\n",
    "class stacked_NVP(nn.Module):\n",
    "    def __init__(self, d, k, hidden, n):\n",
    "        super().__init__()\n",
    "        self.bijectors = nn.ModuleList([\n",
    "            R_NVP(d, k, hidden=hidden) for _ in range(n)\n",
    "        ])\n",
    "        self.flips = [True if i%2 else False for i in range(n)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        log_jacobs = []\n",
    "\n",
    "        for bijector, f in zip(self.bijectors, self.flips):\n",
    "            x, log_pz, lj = bijector(x, flip=f)\n",
    "            log_jacobs.append(lj)\n",
    "        \n",
    "        return x, log_pz, sum(log_jacobs)\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        for bijector, f in zip(reversed(self.bijectors), reversed(self.flips)):\n",
    "            z = bijector.inverse(z, flip=f)\n",
    "        return z\n",
    "    \n",
    "class R_NVP(nn.Module):\n",
    "    def __init__(self, d, k, hidden):\n",
    "        super().__init__()\n",
    "        self.d, self.k = d, k\n",
    "        self.sig_net = nn.Sequential(\n",
    "                    nn.Linear(k, hidden),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(hidden, d - k))\n",
    "\n",
    "        self.mu_net = nn.Sequential(\n",
    "                    nn.Linear(k, hidden),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(hidden, d - k))\n",
    "\n",
    "    def forward(self, x, flip=False):\n",
    "        x1, x2 = x[:, :self.k], x[:, self.k:] \n",
    "\n",
    "        if flip:\n",
    "            x2, x1 = x1, x2\n",
    "        \n",
    "        # forward\n",
    "        sig = self.sig_net(x1)\n",
    "        z1, z2 = x1, x2 * torch.exp(sig) + self.mu_net(x1)\n",
    "        \n",
    "        if flip:\n",
    "            z2, z1 = z1, z2\n",
    "        \n",
    "        z_hat = torch.cat([z1, z2], dim=-1)\n",
    "\n",
    "        log_pz = base_dist.log_prob(z_hat)\n",
    "        log_jacob = sig.sum(-1)\n",
    "        \n",
    "        return z_hat, log_pz, log_jacob\n",
    "    \n",
    "    def inverse(self, Z, flip=False):\n",
    "        z1, z2 = Z[:, :self.k], Z[:, self.k:] \n",
    "        \n",
    "        if flip:\n",
    "            z2, z1 = z1, z2\n",
    "        \n",
    "        x1 = z1\n",
    "        x2 = (z2 - self.mu_net(z1)) * torch.exp(-self.sig_net(z1))\n",
    "        \n",
    "        if flip:\n",
    "            x2, x1 = x1, x2\n",
    "        return torch.cat([x1, x2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "500c2ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 9\n",
    "batch = 512\n",
    "index_step_length = 31\n",
    "epochs = 1000\n",
    "\n",
    "d = 2\n",
    "k = 1\n",
    "\n",
    "base_mu, base_cov = torch.zeros(2), torch.eye(2)\n",
    "base_mu = base_mu.double()\n",
    "base_cov = base_cov.double()\n",
    "base_mu = Variable(base_mu)\n",
    "base_cov = Variable(base_cov)\n",
    "base_dist = MultivariateNormal(base_mu, base_cov)\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "labels, X, Y, XX, YY = Splitting_dataset(data, step_size)\n",
    "\n",
    "demo = VAE(index_step_length,h_dim=7,z_dim=2)\n",
    "model = stacked_NVP(d, k, hidden=512,n=3)\n",
    "demo.double()\n",
    "model.double()\n",
    "#next set of tests should be with n=3, last set was with n=1\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "optimizer2 = torch.optim.RMSprop(demo.parameters(), lr=1e-3)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "1a063aee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The right-most size of value must match event_shape: torch.Size([9, 9]) vs torch.Size([2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26260/1541229430.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mlocalX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mrecon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocalX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocalX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#doing kl-divergence loss correctly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \"\"\"This bound (kl loss) provides a unified objective function for \n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26260/1539735925.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#print(z.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m#z = z.float()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;31m#print(z)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mtensorZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#torch.tensor(z[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26260/1539735925.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbijector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflips\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_pz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbijector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[0mlog_jacobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26260/1539735925.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, flip)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mz_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mlog_pz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[0mlog_jacob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\distributions\\multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_batch_mahalanobis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\distributions\\distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mevent_dim_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mevent_dim_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m             raise ValueError('The right-most size of value must match event_shape: {} vs {}.'.\n\u001b[0m\u001b[0;32m    260\u001b[0m                              format(value.size(), self._event_shape))\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The right-most size of value must match event_shape: torch.Size([9, 9]) vs torch.Size([2])."
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "anomaly_history = []\n",
    "loss_history = []\n",
    "avgSum = 0\n",
    "avgCount = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    b = get_batch(XX,batch)\n",
    "    #print(range(batch))\n",
    "    for i in range(batch):\n",
    "        localX = torch.tensor(b[i])\n",
    "        recon, mu, logvar = demo(localX)\n",
    "        loss = loss_fn(recon, localX, mu, logvar) #doing kl-divergence loss correctly\n",
    "        \"\"\"This bound (kl loss) provides a unified objective function for \n",
    "        op-timization of both the parameters θ and φ of the model and variational approximation, respectively.\"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer2.step()\n",
    "        scheduler.step()\n",
    "        idx = idx + 1\n",
    "\n",
    "        avgSum = avgSum + torch.mean(loss/batch)\n",
    "        avgCount = avgCount + 1\n",
    "        anomaly_score = torch.mean(localX/recon)\n",
    "\n",
    "        if idx%30 == 0:\n",
    "            loss_history.append(avgSum/avgCount)\n",
    "            anomaly_history.append(anomaly_score)\n",
    "            avgSum = 0\n",
    "            avgCount = 0\n",
    "\n",
    "        if idx%100 == 0:\n",
    "            print(\"Epoch[{}/{}] Loss: {:.3f}\".format(epoch+1, epochs, loss.data.item()/batch))\n",
    "p1 = plt.figure()\n",
    "plt.plot(loss_history,'g-',label='h 10,z 2')\n",
    "p2 = plt.figure()\n",
    "plt.plot(anomaly_history,'b-',label='h 10,z 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ac721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99decd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
