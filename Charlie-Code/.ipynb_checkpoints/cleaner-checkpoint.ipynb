{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5c67c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42c2b0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AltAGL</th>\n",
       "      <th>AltB</th>\n",
       "      <th>AltGPS</th>\n",
       "      <th>AltMSL</th>\n",
       "      <th>BaroA</th>\n",
       "      <th>E1_CHT1</th>\n",
       "      <th>E1_CHT2</th>\n",
       "      <th>E1_CHT3</th>\n",
       "      <th>E1_CHT4</th>\n",
       "      <th>E1_EGT1</th>\n",
       "      <th>...</th>\n",
       "      <th>LatAc</th>\n",
       "      <th>NormAc</th>\n",
       "      <th>OAT</th>\n",
       "      <th>Pitch</th>\n",
       "      <th>Roll</th>\n",
       "      <th>TAS</th>\n",
       "      <th>VSpd</th>\n",
       "      <th>VSpdG</th>\n",
       "      <th>WndDr</th>\n",
       "      <th>WndSpd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>855.8</td>\n",
       "      <td>745.7</td>\n",
       "      <td>833.6</td>\n",
       "      <td>30.05</td>\n",
       "      <td>231.88</td>\n",
       "      <td>224.27</td>\n",
       "      <td>243.57</td>\n",
       "      <td>245.74</td>\n",
       "      <td>1047.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1.25</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.35</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>856.8</td>\n",
       "      <td>746.2</td>\n",
       "      <td>834.0</td>\n",
       "      <td>30.05</td>\n",
       "      <td>232.23</td>\n",
       "      <td>224.58</td>\n",
       "      <td>243.87</td>\n",
       "      <td>246.04</td>\n",
       "      <td>1046.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1.23</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>855.8</td>\n",
       "      <td>746.4</td>\n",
       "      <td>834.2</td>\n",
       "      <td>30.05</td>\n",
       "      <td>232.59</td>\n",
       "      <td>224.84</td>\n",
       "      <td>244.12</td>\n",
       "      <td>246.34</td>\n",
       "      <td>1046.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1.21</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.96</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>856.8</td>\n",
       "      <td>745.2</td>\n",
       "      <td>833.1</td>\n",
       "      <td>30.05</td>\n",
       "      <td>232.96</td>\n",
       "      <td>225.10</td>\n",
       "      <td>244.46</td>\n",
       "      <td>246.59</td>\n",
       "      <td>1046.32</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1.22</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.95</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>856.8</td>\n",
       "      <td>745.2</td>\n",
       "      <td>833.0</td>\n",
       "      <td>30.05</td>\n",
       "      <td>233.30</td>\n",
       "      <td>225.36</td>\n",
       "      <td>244.75</td>\n",
       "      <td>246.86</td>\n",
       "      <td>1043.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1.22</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.36</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5055</th>\n",
       "      <td>0.0</td>\n",
       "      <td>844.2</td>\n",
       "      <td>751.5</td>\n",
       "      <td>839.3</td>\n",
       "      <td>30.02</td>\n",
       "      <td>282.56</td>\n",
       "      <td>270.85</td>\n",
       "      <td>296.29</td>\n",
       "      <td>288.37</td>\n",
       "      <td>1041.83</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.17</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5056</th>\n",
       "      <td>0.0</td>\n",
       "      <td>844.2</td>\n",
       "      <td>751.6</td>\n",
       "      <td>839.4</td>\n",
       "      <td>30.02</td>\n",
       "      <td>282.49</td>\n",
       "      <td>270.90</td>\n",
       "      <td>296.36</td>\n",
       "      <td>288.45</td>\n",
       "      <td>1036.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5057</th>\n",
       "      <td>0.0</td>\n",
       "      <td>844.2</td>\n",
       "      <td>751.7</td>\n",
       "      <td>839.5</td>\n",
       "      <td>30.02</td>\n",
       "      <td>282.43</td>\n",
       "      <td>270.93</td>\n",
       "      <td>296.42</td>\n",
       "      <td>288.52</td>\n",
       "      <td>1030.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5058</th>\n",
       "      <td>1.0</td>\n",
       "      <td>845.2</td>\n",
       "      <td>752.4</td>\n",
       "      <td>840.2</td>\n",
       "      <td>30.02</td>\n",
       "      <td>282.55</td>\n",
       "      <td>271.14</td>\n",
       "      <td>296.58</td>\n",
       "      <td>288.66</td>\n",
       "      <td>1024.47</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5059</th>\n",
       "      <td>0.0</td>\n",
       "      <td>845.2</td>\n",
       "      <td>751.0</td>\n",
       "      <td>838.8</td>\n",
       "      <td>30.02</td>\n",
       "      <td>282.44</td>\n",
       "      <td>271.07</td>\n",
       "      <td>296.52</td>\n",
       "      <td>288.66</td>\n",
       "      <td>1022.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5060 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AltAGL   AltB  AltGPS  AltMSL  BaroA  E1_CHT1  E1_CHT2  E1_CHT3  \\\n",
       "0        0.0  855.8   745.7   833.6  30.05   231.88   224.27   243.57   \n",
       "1        0.0  856.8   746.2   834.0  30.05   232.23   224.58   243.87   \n",
       "2        0.0  855.8   746.4   834.2  30.05   232.59   224.84   244.12   \n",
       "3        0.0  856.8   745.2   833.1  30.05   232.96   225.10   244.46   \n",
       "4        0.0  856.8   745.2   833.0  30.05   233.30   225.36   244.75   \n",
       "...      ...    ...     ...     ...    ...      ...      ...      ...   \n",
       "5055     0.0  844.2   751.5   839.3  30.02   282.56   270.85   296.29   \n",
       "5056     0.0  844.2   751.6   839.4  30.02   282.49   270.90   296.36   \n",
       "5057     0.0  844.2   751.7   839.5  30.02   282.43   270.93   296.42   \n",
       "5058     1.0  845.2   752.4   840.2  30.02   282.55   271.14   296.58   \n",
       "5059     0.0  845.2   751.0   838.8  30.02   282.44   271.07   296.52   \n",
       "\n",
       "      E1_CHT4  E1_EGT1  ...  LatAc  NormAc   OAT  Pitch  Roll  TAS   VSpd  \\\n",
       "0      245.74  1047.12  ...   0.01    0.01  27.2   1.25 -0.21  0.0 -14.35   \n",
       "1      246.04  1046.06  ...   0.01   -0.00  27.2   1.23 -0.15  0.0   0.74   \n",
       "2      246.34  1046.17  ...   0.00   -0.01  27.2   1.21 -0.14  0.0   5.96   \n",
       "3      246.59  1046.32  ...  -0.00   -0.01  27.2   1.22 -0.14  0.0   9.95   \n",
       "4      246.86  1043.91  ...   0.01   -0.01  27.2   1.22 -0.10  0.0  13.36   \n",
       "...       ...      ...  ...    ...     ...   ...    ...   ...  ...    ...   \n",
       "5055   288.37  1041.83  ...   0.00    0.04  26.0   1.45  0.00  0.0  15.17   \n",
       "5056   288.45  1036.33  ...   0.00   -0.02  26.0   1.29  0.03  0.0  15.98   \n",
       "5057   288.52  1030.14  ...   0.00    0.01  26.0   1.76  0.02  0.0  17.44   \n",
       "5058   288.66  1024.47  ...  -0.00   -0.01  26.0   1.80  0.01  0.0  25.02   \n",
       "5059   288.66  1022.10  ...  -0.00   -0.01  26.0   1.80  0.01  0.0  17.27   \n",
       "\n",
       "      VSpdG  WndDr  WndSpd  \n",
       "0      -3.9    0.0     0.0  \n",
       "1      -3.9    0.0     0.0  \n",
       "2      -3.9    0.0     0.0  \n",
       "3      -3.9    0.0     0.0  \n",
       "4      -3.9    0.0     0.0  \n",
       "...     ...    ...     ...  \n",
       "5055   -3.9    0.0     0.0  \n",
       "5056    0.0    0.0     0.0  \n",
       "5057    0.0    0.0     0.0  \n",
       "5058    0.0    0.0     0.0  \n",
       "5059    0.0    0.0     0.0  \n",
       "\n",
       "[5060 rows x 31 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('c172_file_1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f62645cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split and reshape the data set by step_size , use min-max or stanrdardlize method to rescale the data\n",
    "def Splitting_dataset(data, step_size, scale=True, scaler_type=MinMaxScaler):\n",
    "        l = len(data) \n",
    "        data = scaler_type().fit_transform(data)\n",
    "        Xs = []\n",
    "        Ys = []\n",
    "        for i in range(0, (len(data) - step_size)):\n",
    "            Xs.append(data[i:i+step_size])\n",
    "            Ys.append(data[i:i+step_size])\n",
    "        train_x, test_x, train_y, test_y = [np.array(x) for x in train_test_split(Xs, Ys)]\n",
    "        assert train_x.shape[2] == test_x.shape[2] == (data.shape[1] if (type(data) == np.ndarray) else len(data))\n",
    "        return  (train_x.shape[2], train_x, train_y, test_x, test_y)\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    \n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu**2 -  logvar.exp())\n",
    "        return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "939d840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=27, z_dim=31, n_flow_steps=1):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(image_size, h_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(h_dim, z_dim*2) #is it saying its getting a mu and a var for each z dim out?\n",
    "            \n",
    "            #how can I represent the encoder as a distribution acting as the prior?\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, image_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        #self.flow = stacked_NVP(z_dim,)\n",
    "        \n",
    "        #should i add a flow model as a paremeter of the vae for easy access in parameterize?\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_() \n",
    "        esp = to_var(torch.randn(*mu.size()))\n",
    "        #print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        #print(\"esp: \" + str(esp))\n",
    "        #randn - Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).\n",
    "        z = mu + std * esp\n",
    "        #print(z)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        #print(\"h: \" + str(h))\n",
    "        mu, logvar = torch.chunk(h, 2, dim=1)\n",
    "        #reparameterize is doing the sampling with esp\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        #print(\"z: \" + str(z))\n",
    "\n",
    "        prior = torch.distributions.Normal(mu,(logvar.mul(0.5).exp_()))\n",
    "        ml = nn.ModuleList()\n",
    "        ml.append(AffineConstantFlow(31))\n",
    "        ml.append(AffineConstantFlow(31))\n",
    "        ml.append(AffineConstantFlow(31))\n",
    "        ml.append(AffineConstantFlow(31))\n",
    "        #sacf = \n",
    "\n",
    "        nf = NormalizingFlowModel(prior,ml)\n",
    "        zTransformed = nf(z) #this is where I apply the normalizing flow once its implemented, the points are transformed as theyre generated\n",
    "    \n",
    "        #print(\"Z: \" + str(z))\n",
    "        #print(\"ZT: \" + str(zTransformed))\n",
    "        print(z)\n",
    "        z = zTransformed[0]\n",
    "        print(z)\n",
    "        return self.decoder(z), mu, logvar\n",
    "    \n",
    "class NormalizingFlow(nn.Module):\n",
    "    \"\"\" A sequence of Normalizing Flows is a Normalizing Flow \"\"\"\n",
    "\n",
    "    def __init__(self, flows):\n",
    "        super().__init__()\n",
    "        self.flows = nn.ModuleList(flows)\n",
    "\n",
    "    def forward(self, x):\n",
    "        m, _ = x.shape\n",
    "        log_det = torch.zeros(m)\n",
    "        zs = [x]\n",
    "        for flow in self.flows:\n",
    "            #h = demo.encoder(x)\n",
    "            #mu, logvar = torch.chunk(h, 2, dim=1)\n",
    "            x, ld = flow.forward(x)#,demo.reparameterize(mu,logvar))\n",
    "            log_det += ld\n",
    "            zs.append(x)\n",
    "            #here we're just summing the log deterimant of each of the flows\n",
    "        return zs, log_det\n",
    "\n",
    "    def backward(self, z):\n",
    "        m, _ = z.shape\n",
    "        log_det = torch.zeros(m)\n",
    "        xs = [z]\n",
    "        for flow in self.flows[::-1]:\n",
    "            z, ld = flow.backward(z)\n",
    "            log_det += ld\n",
    "            xs.append(z)\n",
    "        return xs, log_det\n",
    "\n",
    "class NormalizingFlowModel(nn.Module):\n",
    "    \"\"\" A Normalizing Flow Model is a (prior, flow) pair \"\"\"\n",
    "    \n",
    "    def __init__(self, prior, flows):\n",
    "        super().__init__()\n",
    "        self.prior = prior\n",
    "        self.flow = NormalizingFlow(flows)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        zs, log_det = self.flow.forward(x)\n",
    "        prior_logprob = self.prior.log_prob(zs[-1]).view(x.size(0), -1).sum(1)\n",
    "        return zs, prior_logprob, log_det\n",
    "\n",
    "    def backward(self, z):\n",
    "        xs, log_det = self.flow.backward(z)\n",
    "        return xs, log_det\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        z = self.prior.sample((num_samples,))\n",
    "        xs, _ = self.flow.backward(z)#should i update this to be the approximate posterior??????????????????\n",
    "        return xs\n",
    "    \n",
    "class AffineConstantFlow(nn.Module):\n",
    "    \"\"\" \n",
    "    Scales + Shifts the flow by (learned) constants per dimension.\n",
    "    In NICE paper there is a Scaling layer which is a special case of this where t is None\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, scale=True, shift=True):\n",
    "        super().__init__()\n",
    "        self.s = nn.Parameter(torch.randn(1, dim, requires_grad=True)) if scale else None\n",
    "        self.t = nn.Parameter(torch.randn(1, dim, requires_grad=True)) if shift else None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        s = self.s if self.s is not None else x.new_zeros(x.size())\n",
    "        t = self.t if self.t is not None else x.new_zeros(x.size())\n",
    "        z = x * torch.exp(s) + t\n",
    "        log_det = torch.sum(s, dim=1)\n",
    "        return z, log_det\n",
    "    \n",
    "    def backward(self, z):\n",
    "        s = self.s if self.s is not None else z.new_zeros(z.size())\n",
    "        t = self.t if self.t is not None else z.new_zeros(z.size())\n",
    "        x = (z - t) * torch.exp(-s)\n",
    "        log_det = torch.sum(-s, dim=1)\n",
    "        return x, log_det\n",
    "    \n",
    "class stacked_NVP(nn.Module):\n",
    "    def __init__(self, d, k, hidden, n):\n",
    "        super().__init__()\n",
    "        self.bijectors = nn.ModuleList([\n",
    "            R_NVP(d, k, hidden=hidden) for _ in range(n)\n",
    "        ])\n",
    "        self.flips = [True if i%2 else False for i in range(n)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        log_jacobs = []\n",
    "        \n",
    "        for bijector, f in zip(self.bijectors, self.flips):\n",
    "            x, log_pz, lj = bijector(x, flip=f)\n",
    "            log_jacobs.append(lj)\n",
    "        \n",
    "        return x, log_pz, sum(log_jacobs)\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        for bijector, f in zip(reversed(self.bijectors), reversed(self.flips)):\n",
    "            z = bijector.inverse(z, flip=f)\n",
    "        return z\n",
    "    \n",
    "class R_NVP(nn.Module):\n",
    "    def __init__(self, d, k, hidden):\n",
    "        super().__init__()\n",
    "        self.d, self.k = d, k\n",
    "        self.sig_net = nn.Sequential(\n",
    "                    nn.Linear(k, hidden),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(hidden, d - k))\n",
    "\n",
    "        self.mu_net = nn.Sequential(\n",
    "                    nn.Linear(k, hidden),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(hidden, d - k))\n",
    "\n",
    "    def forward(self, x, flip=False):\n",
    "        x1, x2 = x[:, :self.k], x[:, self.k:] \n",
    "\n",
    "        if flip:\n",
    "            x2, x1 = x1, x2\n",
    "        \n",
    "        # forward\n",
    "        sig = self.sig_net(x1)\n",
    "        z1, z2 = x1, x2 * torch.exp(sig) + self.mu_net(x1)\n",
    "        \n",
    "        if flip:\n",
    "            z2, z1 = z1, z2\n",
    "        \n",
    "        z_hat = torch.cat([z1, z2], dim=-1)\n",
    "\n",
    "        log_pz = base_dist.log_prob(z_hat)\n",
    "        log_jacob = sig.sum(-1)\n",
    "        \n",
    "        return z_hat, log_pz, log_jacob\n",
    "    \n",
    "    def inverse(self, Z, flip=False):\n",
    "        z1, z2 = Z[:, :self.k], Z[:, self.k:] \n",
    "        \n",
    "        if flip:\n",
    "            z2, z1 = z1, z2\n",
    "        \n",
    "        x1 = z1\n",
    "        x2 = (z2 - self.mu_net(z1)) * torch.exp(-self.sig_net(z1))\n",
    "        \n",
    "        if flip:\n",
    "            x2, x1 = x1, x2\n",
    "        return torch.cat([x1, x2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ebcc1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size= 3\n",
    "batch = 128\n",
    "index_step_length = 31\n",
    "epochs = 50\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "labels, X, Y, XX, YY = Splitting_dataset(data, step_size)\n",
    "demo = VAE(index_step_length,h_dim=7,z_dim=31)\n",
    "demo.double()\n",
    "optimizer = torch.optim.RMSprop(demo.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3c2b60d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5991, -0.0212,  0.6303,  0.6349, -0.0767, -0.0538,  1.2043,  0.2768,\n",
      "          0.7824, -0.5514, -0.5827, -0.7598, -0.4023, -2.2200, -1.3051,  1.2046,\n",
      "          0.1177, -0.6034,  0.4572,  0.2785, -0.0067,  1.1227,  0.6311,  1.4870,\n",
      "         -0.9993,  0.5951, -1.3940, -0.2718,  0.4112, -0.8404,  0.2380],\n",
      "        [-0.6002,  0.0323, -1.2107, -2.1806, -0.5048,  0.7025,  0.8752,  0.0955,\n",
      "         -0.5106,  0.9217, -1.3593,  1.0043,  0.8327, -0.9230, -3.1001,  0.5776,\n",
      "         -0.4932,  0.0515, -0.0086,  0.7985, -0.4797,  0.3490,  0.6819,  1.5249,\n",
      "          1.8461,  0.1721, -1.6569, -0.7379,  0.1017, -0.5791,  0.0802],\n",
      "        [ 1.1383, -2.6333,  1.9630,  0.5328,  0.1359,  0.6788,  0.9605,  0.9210,\n",
      "          1.4338, -0.7539, -1.2324, -0.0102, -1.5740, -0.4085, -0.7668,  1.1021,\n",
      "          0.9752, -0.3364,  1.9784, -0.2998, -0.1155, -1.2463, -1.7838,  1.7423,\n",
      "         -0.3187, -1.6483, -0.6261,  0.9374, -1.0074, -0.3927,  0.2433]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "([tensor([[ 0.5991, -0.0212,  0.6303,  0.6349, -0.0767, -0.0538,  1.2043,  0.2768,\n",
      "          0.7824, -0.5514, -0.5827, -0.7598, -0.4023, -2.2200, -1.3051,  1.2046,\n",
      "          0.1177, -0.6034,  0.4572,  0.2785, -0.0067,  1.1227,  0.6311,  1.4870,\n",
      "         -0.9993,  0.5951, -1.3940, -0.2718,  0.4112, -0.8404,  0.2380],\n",
      "        [-0.6002,  0.0323, -1.2107, -2.1806, -0.5048,  0.7025,  0.8752,  0.0955,\n",
      "         -0.5106,  0.9217, -1.3593,  1.0043,  0.8327, -0.9230, -3.1001,  0.5776,\n",
      "         -0.4932,  0.0515, -0.0086,  0.7985, -0.4797,  0.3490,  0.6819,  1.5249,\n",
      "          1.8461,  0.1721, -1.6569, -0.7379,  0.1017, -0.5791,  0.0802],\n",
      "        [ 1.1383, -2.6333,  1.9630,  0.5328,  0.1359,  0.6788,  0.9605,  0.9210,\n",
      "          1.4338, -0.7539, -1.2324, -0.0102, -1.5740, -0.4085, -0.7668,  1.1021,\n",
      "          0.9752, -0.3364,  1.9784, -0.2998, -0.1155, -1.2463, -1.7838,  1.7423,\n",
      "         -0.3187, -1.6483, -0.6261,  0.9374, -1.0074, -0.3927,  0.2433]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>), tensor([[ 1.7296, -0.0630, -0.2948,  1.1159,  0.5782, -0.5030,  3.4505,  0.2587,\n",
      "          3.4385, -0.9626, -0.3939, -1.0669, -0.8045, -2.0434,  0.7496,  3.2564,\n",
      "          0.1192,  0.6441, -0.2989, -1.4743, -0.5529,  1.2562,  2.0435,  3.2299,\n",
      "         -1.3306,  3.4631, -0.4321, -1.1851,  0.4491,  0.0270, -0.0437],\n",
      "        [ 1.1888,  0.0960, -0.6306, -1.0533,  0.1699, -0.0879,  2.3265, -0.5299,\n",
      "          0.1066,  2.8591, -3.4100, -0.9860, -0.3548, -0.0194,  0.0363,  1.7379,\n",
      "         -2.5392,  0.7133, -0.4036, -1.1051, -1.8114,  0.8666,  2.1467,  3.3109,\n",
      "         -0.4114,  1.0935, -0.5073, -1.2720,  0.3121,  0.2052, -0.1500],\n",
      "        [ 1.9728, -7.8346, -0.0517,  1.0373,  0.7810, -0.1009,  2.6178,  3.0603,\n",
      "          5.1173, -1.4880, -2.9172, -1.0325, -1.2312,  0.7833,  0.9634,  3.0081,\n",
      "          3.8504,  0.6723,  0.0430, -1.8849, -0.8423,  0.0632, -2.8619,  3.7765,\n",
      "         -1.1107, -9.1026, -0.2126, -0.9597, -0.1788,  0.3323, -0.0401]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>), tensor([[ 1.6689,  1.5107, -3.0496,  0.3922, -0.2561,  2.1606,  8.5292,  0.2262,\n",
      "          4.0797, -1.7289,  0.3814, -1.4294,  0.1862, -3.4376, -0.3328,  1.5110,\n",
      "          0.9219,  4.8690, -1.0319, -1.4111,  0.1492,  0.1642,  0.6105,  0.4750,\n",
      "          0.3889,  0.7625,  1.2624, -1.1889,  1.0610, -0.6675,  0.2157],\n",
      "        [ 1.4196,  1.7078, -3.1350, -1.1164, -0.9301,  2.6457,  5.9657, -1.1372,\n",
      "          1.2075,  3.7007, -0.1786, -1.2819,  0.7106, -0.8384, -1.2806,  0.9687,\n",
      "         -0.8623,  5.3111, -1.0846, -1.1069, -1.8905, -0.0295,  0.6779,  0.5202,\n",
      "          0.4862,  0.3990,  1.2336, -1.2311,  0.6146, -0.2624,  0.0961],\n",
      "        [ 1.7810, -8.1202, -2.9877,  0.3375,  0.0788,  2.6305,  6.6301,  5.0698,\n",
      "          5.5270, -2.4753, -0.0871, -1.3667, -0.3112,  0.1924, -0.0486,  1.4223,\n",
      "          3.4261,  5.0493, -0.8596, -1.7494, -0.3199, -0.4291, -2.5929,  0.7802,\n",
      "          0.4121, -1.1651,  1.3465, -1.0795, -0.9851,  0.0266,  0.2198]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>), tensor([[ 1.7684e+00,  2.3487e+00, -3.0813e+00,  2.0815e+00, -2.3890e+00,\n",
      "          1.9558e+00,  8.0713e+00,  1.8888e-02,  6.2516e+00, -3.8251e+00,\n",
      "          2.1659e+00, -5.6729e-01,  3.3261e-01, -1.5739e+01,  1.9315e+00,\n",
      "          1.4163e-01,  1.1258e+00,  4.7495e+00, -5.2580e-01, -6.2792e+00,\n",
      "          4.6169e-01,  1.6344e+00,  6.0509e-01,  2.3314e-02,  5.1816e-01,\n",
      "         -5.1530e-01,  4.8847e+00,  1.0080e+00, -4.1163e-01,  2.5043e-01,\n",
      "          2.7472e-01],\n",
      "        [ 1.5283e+00,  2.6236e+00, -3.1330e+00, -3.1212e+00, -2.9290e+00,\n",
      "          2.1894e+00,  5.5618e+00, -6.2360e-01,  2.0038e+00,  2.8737e+00,\n",
      "         -4.6298e-01, -4.3956e-01,  7.7187e-01, -2.8306e+00,  1.6477e+00,\n",
      "          8.0307e-03, -2.6284e-01,  5.2497e+00, -6.1000e-01, -5.1504e+00,\n",
      "         -4.5404e+00,  1.4675e+00,  6.4757e-01,  3.0997e-02,  8.4175e-01,\n",
      "         -6.2589e-01,  4.8501e+00,  9.8861e-01, -5.5531e-01,  3.2959e-01,\n",
      "          2.2562e-01],\n",
      "        [ 1.8763e+00, -1.1081e+01, -3.0438e+00,  1.8930e+00, -2.1208e+00,\n",
      "          2.1821e+00,  6.2122e+00,  2.3015e+00,  8.3919e+00, -4.7460e+00,\n",
      "         -3.3500e-02, -5.1301e-01, -8.4126e-02,  2.2888e+00,  2.0166e+00,\n",
      "          1.1978e-01,  3.0748e+00,  4.9535e+00, -2.5079e-01, -7.5346e+00,\n",
      "         -6.8862e-01,  1.1233e+00, -1.4137e+00,  7.5123e-02,  5.9556e-01,\n",
      "         -1.1018e+00,  4.9857e+00,  1.0583e+00, -1.0702e+00,  3.8608e-01,\n",
      "          2.7638e-01]], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([[ 1.9626e+00,  2.0245e+00, -2.5288e+00,  2.1076e+00, -1.3813e+00,\n",
      "          7.7833e+00,  1.6349e+01,  1.2001e+00,  2.8465e+00, -7.8591e+00,\n",
      "          2.1466e+00,  3.9464e-01, -1.2745e+00, -7.0082e+01,  1.2122e+00,\n",
      "          1.8474e+00,  2.1369e+00,  4.4958e+01, -5.7151e-01, -1.0601e+01,\n",
      "         -2.6220e-01,  1.9366e+00, -5.5312e-01,  1.0853e-01, -1.0375e+00,\n",
      "         -5.2266e-01,  4.0597e+00, -4.8947e-01,  2.4040e+00,  5.3236e-01,\n",
      "          2.4675e-01],\n",
      "        [ 1.5691e+00,  2.2086e+00, -2.5645e+00, -3.3224e+00, -1.6942e+00,\n",
      "          8.4083e+00,  1.1366e+01, -3.1607e+00,  1.3831e+00,  5.0300e+00,\n",
      "         -1.0018e+00,  4.7120e-01, -1.2106e+00, -1.2713e+01,  8.5703e-01,\n",
      "          1.5269e+00,  5.5338e-01,  4.9792e+01, -7.3334e-01, -8.7727e+00,\n",
      "         -4.6813e+00,  1.8883e+00, -4.7659e-01,  1.2529e-01, -9.9444e-01,\n",
      "         -5.8826e-01,  4.0174e+00, -4.9140e-01,  2.2192e+00,  5.7023e-01,\n",
      "          1.0022e-01],\n",
      "        [ 2.1395e+00, -6.9706e+00, -2.5030e+00,  1.9107e+00, -1.2258e+00,\n",
      "          8.3887e+00,  1.2657e+01,  1.6693e+01,  3.5839e+00, -9.6310e+00,\n",
      "         -4.8740e-01,  4.2717e-01, -1.3351e+00,  1.0039e+01,  1.3187e+00,\n",
      "          1.7950e+00,  4.3594e+00,  4.6929e+01, -4.2927e-02, -1.2634e+01,\n",
      "         -1.2784e+00,  1.7887e+00, -4.1894e+00,  2.2157e-01, -1.0272e+00,\n",
      "         -8.7053e-01,  4.1831e+00, -4.8447e-01,  1.5569e+00,  5.9726e-01,\n",
      "          2.5171e-01]], dtype=torch.float64, grad_fn=<AddBackward0>)], tensor([-3009.3970, -1434.4894, -1509.5497], dtype=torch.float64,\n",
      "       grad_fn=<SumBackward1>), tensor([-12.3653, -12.3653, -12.3653], grad_fn=<AddBackward0>))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4412/1186588080.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mrecon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocalX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocalX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4412/112038799.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzTransformed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mNormalizingFlow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "anomaly_history = []\n",
    "loss_history = []\n",
    "avgSum = 0\n",
    "avgCount = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(XX)):\n",
    "        localX = torch.tensor(XX[i])\n",
    "        \n",
    "        \n",
    "        recon, mu, logvar = demo(localX)\n",
    "        loss = loss_fn(recon, localX, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        idx = idx + 1\n",
    "\n",
    "        \n",
    "        avgSum = avgSum + torch.mean(loss/batch)\n",
    "        avgCount = avgCount + 1\n",
    "        anomaly_score = torch.mean(localX/recon)\n",
    "\n",
    "        if idx%10 == 0:\n",
    "            loss_history.append(avgSum/avgCount)\n",
    "            anomaly_history.append(anomaly_score)\n",
    "            avgSum = 0\n",
    "            avgCount = 0\n",
    "\n",
    "        if idx%100 == 0:\n",
    "            print(\"Epoch[{}/{}] Loss: {:.3f}\".format(epoch+1, epochs, loss.data.item()/batch))\n",
    "\n",
    "plt.plot(loss_history,'g-',label='h 10,z 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167231d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd30e6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
