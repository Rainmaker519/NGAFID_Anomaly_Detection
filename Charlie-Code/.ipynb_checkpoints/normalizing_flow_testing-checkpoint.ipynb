{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10ec2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pyro.nn import AutoRegressiveNN\n",
    "from pyro import distributions\n",
    "import pyro\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.distributions.transforms import householder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal as mvn\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "class NormalizingFlow(nn.Module):\n",
    "    def __init__(self,dim,\n",
    "                      n_flows,\n",
    "                     base_dist=lambda dim:distributions.Normal(torch.zeros(dim), torch.ones(dim)),\n",
    "                     flow_type=lambda kwargs:distributions.transforms.RadialFlow(**kwargs),\n",
    "                     args={'flow_args':{'dim':2}}):\n",
    "        super(NormalizingFlow, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.n_flows = n_flows\n",
    "        self.base_dist = base_dist(dim)\n",
    "        self.uuid = np.random.randint(low=0,high=10000,size=1)[0]\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        If the flow needs an autoregressive net, build it for every flow\n",
    "        \"\"\"\n",
    "        if 'arn_hidden' in args:\n",
    "            self.arns = nn.ModuleList([AutoRegressiveNN(dim,\n",
    "                                                        args['arn_hidden'],\n",
    "                                                        param_dims=[self.dim]*args['n_params']) for _ in range(n_flows)])\n",
    "    \n",
    "        \"\"\"\n",
    "        Initialize all flows\n",
    "        \"\"\"\n",
    "        self.nfs = []\n",
    "        for f in range(n_flows):\n",
    "            if 'autoregressive_nn' in args['flow_args']:\n",
    "                args['flow_args']['autoregressive_nn'] = self.arns[f]\n",
    "            nf = flow_type(args['flow_args'])\n",
    "            self.nfs.append(nf)\n",
    "\n",
    "        \"\"\"\n",
    "        This step assumes that nfs={f_i}_{i=1}^N and that base_dist=N(0,I)\n",
    "        Then, register the (biejctive) transformation Z=nfs(eps), eps~base_dist\n",
    "        \"\"\"\n",
    "        self.nf_dist = distributions.TransformedDistribution(self.base_dist, self.nfs)\n",
    "        \n",
    "        self._register()\n",
    "        \n",
    "    def _register(self):\n",
    "        \"\"\"\n",
    "        Register all N flows with Pyro\n",
    "        \"\"\"\n",
    "        for f in range(self.n_flows):\n",
    "            nf_module = pyro.module(\"%d_nf_%d\" %(self.uuid,f), self.nfs[f])\n",
    "\n",
    "    def target(self,x,p_z):\n",
    "        \"\"\"\n",
    "        p(x,z), but x is not required if there is a true density function (p_z in this case)\n",
    "        \n",
    "        1. Sample Z ~ p_z\n",
    "        2. Score it's likelihood against p_z\n",
    "        \"\"\"\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            p = p_z()\n",
    "            z = pyro.sample(\"latent\",p)\n",
    "            pyro.sample(\"obs\", p, obs=x.reshape(-1, self.dim))\n",
    "        \n",
    "    def model(self,x,p_z):\n",
    "        \"\"\"\n",
    "        q(z|x), once again x is not required\n",
    "        \n",
    "        1. Sample Z ~ nfs(eps), eps ~ N(0,I)\n",
    "        \n",
    "        This is the NN being trained\n",
    "        \"\"\"\n",
    "        self._register()\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            pyro.sample(\"latent\", self.nf_dist)\n",
    "\n",
    "    def sample(self,n):\n",
    "        \"\"\"\n",
    "        Sample a batch of (n,dim)\n",
    "        \n",
    "        Bug: in IAF and IAFStable, the dimensions throw an error (todo)\n",
    "        \"\"\"\n",
    "        return self.nf_dist.sample(torch.Size([n]))\n",
    "    \n",
    "    def log_prob(self,z):\n",
    "        \"\"\"\n",
    "        Returns log q(z|x) for z (assuming no x is required)\n",
    "        \"\"\"\n",
    "        return self.nf_dist.log_prob(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e1ac33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Splitting_dataset(data, step_size, scale=True, scaler_type=MinMaxScaler):\n",
    "        l = len(data) \n",
    "        data = scaler_type().fit_transform(data)\n",
    "        Xs = []\n",
    "        Ys = []\n",
    "        for i in range(0, (len(data) - step_size)):\n",
    "            Xs.append(data[i:i+step_size])\n",
    "            Ys.append(data[i:i+step_size])\n",
    "        train_x, test_x, train_y, test_y = [np.array(x) for x in train_test_split(Xs, Ys)]\n",
    "        assert train_x.shape[2] == test_x.shape[2] == (data.shape[1] if (type(data) == np.ndarray) else len(data))\n",
    "        return  (train_x.shape[2], train_x, train_y, test_x, test_y)\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    \n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu**2 -  logvar.exp())\n",
    "        return BCE + KLD\n",
    "    \n",
    "def loss_fn_2(recon_x, x, mu, logvar):\n",
    "    tmp = x - recon_x\n",
    "    tmp = tmp * tmp\n",
    "    loss = torch.sum(tmp)/len(recon_x)\n",
    "    return loss\n",
    "#------------------------------------------------------------------------------------------------------------------------------------- \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=10, z_dim=1):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential( #this is q (n_flow)\n",
    "            nn.Linear(image_size, h_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(h_dim, z_dim*2)\n",
    "            #add pnf here for OmniAnomaly\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential( #this is p (n_flow)\n",
    "            nn.Linear(z_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, image_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        esp = to_var(torch.randn(*mu.size()))\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(h, 2, dim=1)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3fb9c07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nf_obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21492/1011589351.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnf_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkdeplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_levels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshade\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nf_obj' is not defined"
     ]
    }
   ],
   "source": [
    "samples = nf_obj.sample(1000).numpy()\n",
    "\n",
    "sns.kdeplot(data=samples[:,0],data2=samples[:,1],n_levels=60, shade=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b74b6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb30af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc202c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ade201db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizingFlow(nn.Module):\n",
    "    \"\"\" A sequence of Normalizing Flows is a Normalizing Flow \"\"\"\n",
    "\n",
    "    def __init__(self, flows):\n",
    "        super().__init__()\n",
    "        self.flows = nn.ModuleList(flows)\n",
    "\n",
    "    def forward(self, x):\n",
    "        m, _ = x.shape\n",
    "        log_det = torch.zeros(m)\n",
    "        zs = [x]\n",
    "        for flow in self.flows:\n",
    "            x, ld = flow.forward(x)\n",
    "            log_det += ld\n",
    "            zs.append(x)\n",
    "        return zs, log_det\n",
    "\n",
    "    def backward(self, z):\n",
    "        m, _ = z.shape\n",
    "        log_det = torch.zeros(m)\n",
    "        xs = [z]\n",
    "        for flow in self.flows[::-1]:\n",
    "            z, ld = flow.backward(z)\n",
    "            log_det += ld\n",
    "            xs.append(z)\n",
    "        return xs, log_det\n",
    "\n",
    "class NormalizingFlowModel(nn.Module):\n",
    "    \"\"\" A Normalizing Flow Model is a (prior, flow) pair \"\"\"\n",
    "    \n",
    "    def __init__(self, prior, flows):\n",
    "        super().__init__()\n",
    "        self.prior = prior\n",
    "        self.flow = NormalizingFlow(flows)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        zs, log_det = self.flow.forward(x)\n",
    "        prior_logprob = self.prior.log_prob(zs[-1]).view(x.size(0), -1).sum(1)\n",
    "        return zs, prior_logprob, log_det\n",
    "\n",
    "    def backward(self, z):\n",
    "        xs, log_det = self.flow.backward(z)\n",
    "        return xs, log_det\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        z = self.prior.sample((num_samples,))\n",
    "        xs, _ = self.flow.backward(z)\n",
    "        return xs\n",
    "    \n",
    "    \n",
    "#THESE ARE TWO DIFFERENT IMPLEMENTATIONS DONT MIX UP!!!\n",
    "class PlanarFlow(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"Instantiates one step of planar flow.\n",
    "        Args:\n",
    "            dim: input dimensionality.\n",
    "        \"\"\"\n",
    "        super(PlanarFlow, self).__init__()\n",
    "\n",
    "        self.linear_u = nn.Linear(dim, dim)\n",
    "        self.linear_w = nn.Linear(dim, dim)\n",
    "        self.linear_b = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x, v):\n",
    "        \"\"\"Forward pass.\n",
    "        Args:\n",
    "            x: input tensor (B x D).\n",
    "            v: output from last layer of encoder (B x D).\n",
    "        Returns:\n",
    "            transformed x and log-determinant of Jacobian.\n",
    "        \"\"\"\n",
    "        u, w, b = self.linear_u(v), self.linear_w(v), self.linear_b(v)\n",
    "\n",
    "        def m(x):\n",
    "            return F.softplus(x) - 1.\n",
    "        def h(x):\n",
    "            return torch.tanh(x)\n",
    "        def h_prime(x):\n",
    "            return 1. - h(x)**2\n",
    "\n",
    "        inner = (w * u).sum(dim=1, keepdim=True)\n",
    "        u = u + (m(inner) - inner) * w / (w * w).sum(dim=1, keepdim=True)\n",
    "        activation = (w * x).sum(dim=1, keepdim=True) + b\n",
    "        x = x + u * h(activation)\n",
    "        psi = h_prime(activation) * w\n",
    "        log_det = torch.log(torch.abs(1. + (u * psi).sum(dim=1, keepdim=True)))\n",
    "\n",
    "        return x, v, log_det\n",
    "    \n",
    "class Flow(nn.Module): \n",
    "    def __init__(self, dim, type, length):\n",
    "        \"\"\"Instantiates a chain of flows.\n",
    "        Args:\n",
    "            dim: input dimensionality.\n",
    "            type: type of flow.\n",
    "            length: length of flow.\n",
    "        \"\"\"\n",
    "        super(Flow, self).__init__()\n",
    "\n",
    "        if type == 'planar':\n",
    "            self.flow = nn.ModuleList([PlanarFlow(dim) for _ in range(length)])\n",
    "        elif type == 'radial':\n",
    "            self.flow = nn.ModuleList([RadialFlow(dim) for _ in range(length)])\n",
    "        elif type == 'householder':\n",
    "            self.flow = nn.ModuleList([HouseholderFlow(dim) for _ in range(length)])\n",
    "        else:\n",
    "            self.flow = nn.ModuleList([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c3c2264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/2] Loss: 0.176\n",
      "Epoch[1/2] Loss: 0.158\n",
      "Epoch[1/2] Loss: 0.156\n",
      "Epoch[1/2] Loss: 0.153\n",
      "Epoch[1/2] Loss: 0.172\n",
      "Epoch[1/2] Loss: 0.173\n",
      "Epoch[1/2] Loss: 0.166\n",
      "Epoch[1/2] Loss: 0.173\n",
      "Epoch[1/2] Loss: 0.156\n",
      "Epoch[1/2] Loss: 0.149\n",
      "Epoch[1/2] Loss: 0.155\n",
      "Epoch[1/2] Loss: 0.153\n",
      "Epoch[2/2] Loss: 0.142\n",
      "Epoch[2/2] Loss: 0.132\n",
      "Epoch[2/2] Loss: 0.160\n",
      "Epoch[2/2] Loss: 0.151\n",
      "Epoch[2/2] Loss: 0.186\n",
      "Epoch[2/2] Loss: 0.158\n",
      "Epoch[2/2] Loss: 0.140\n",
      "Epoch[2/2] Loss: 0.138\n",
      "Epoch[2/2] Loss: 0.142\n",
      "Epoch[2/2] Loss: 0.145\n",
      "Epoch[2/2] Loss: 0.181\n",
      "Epoch[2/2] Loss: 0.180\n",
      "Epoch[2/2] Loss: 0.129\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('c172_file_1.csv')\n",
    "    step_size= 1\n",
    "    batch = 128\n",
    "    index_step_length = 31\n",
    "    epochs = 2\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------\n",
    "    labels, X, Y, XX, YY = Splitting_dataset(data, step_size)\n",
    "    demo = VAE(index_step_length)\n",
    "    demo.double()\n",
    "    optimizer = torch.optim.Adam(demo.parameters(), lr=1e-3)\n",
    "\n",
    "    idx = 0\n",
    "    \n",
    "    anomaly_history = []\n",
    "    loss_history = []\n",
    "    avgSum = 0\n",
    "    avgCount = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(XX)):\n",
    "            localX = torch.tensor(XX[i])\n",
    "            recon, mu, logvar = demo(localX)\n",
    "            loss = loss_fn(recon, localX, mu, logvar)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            idx = idx + 1\n",
    "            \n",
    "            avgSum = avgSum + torch.mean(loss/batch)\n",
    "            avgCount = avgCount + 1\n",
    "            \n",
    "            anomaly_score = torch.mean(localX/recon)\n",
    "            \n",
    "            if idx%40 == 0:\n",
    "                loss_history.append(avgSum/avgCount)\n",
    "                anomaly_history.append(anomaly_score)\n",
    "                avgSum = 0\n",
    "                avgCount = 0\n",
    "            \n",
    "            if idx%100 == 0:\n",
    "                print(\"Epoch[{}/{}] Loss: {:.3f}\".format(epoch+1, epochs, loss.data.item()/batch))\n",
    "                \n",
    "            #plt.plot(loss_history,'g-',label='h 10,z 2')\n",
    "            plt.plot(anomaly_history,'g-',label='h 10,z 2')\n",
    "            plt.plot(loss_history,'p-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097316a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
