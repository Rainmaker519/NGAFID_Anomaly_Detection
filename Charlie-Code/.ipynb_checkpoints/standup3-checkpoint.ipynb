{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8250272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "#import torch.cuda\n",
    "import random\n",
    "from itertools import chain as chain\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import math\n",
    "\n",
    "#conda activate base\n",
    "cudaOn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14af85d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715057ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tepLoc = \"C:/Users/Charlie/Desktop/TEP_Data/\"\n",
    "\n",
    "#tepTrain = tepLoc + \"TEP_Faulty_Training.csv\"\n",
    "tepTrain = tepLoc + \"TEP_FaultFree_Training.csv\"\n",
    "\n",
    "#tepTest = tepLoc + \"TEP_FaultFree_Testing.csv\"\n",
    "tepTest = tepLoc + \"TEP_Faulty_Testing.csv\"\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#data = pd.read_csv('c172_file_1.csv')\n",
    "data = pd.read_csv(tepTrain)\n",
    "dataTest = pd.read_csv(tepTest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eae22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookie = dataTest[dataTest['simulationRun']==112]\n",
    "#print(lookie[lookie['faultNumber']==5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03075b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('Unnamed: 0',axis=1)\n",
    "data = data.drop('faultNumber',axis=1)\n",
    "#data = data.drop('simulationRun',axis=1)\n",
    "#data = data.drop('sample',axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c817fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTest = dataTest[dataTest['simulationRun']==60]\n",
    "faultNumbersT = dataTest.get('faultNumber')\n",
    "#\n",
    "dataTest = dataTest.drop('Unnamed: 0',axis=1)\n",
    "dataTest = dataTest.drop('faultNumber',axis=1)\n",
    "#dataTest = dataTest.drop('simulationRun',axis=1)\n",
    "#dataTest = dataTest.drop('sample',axis=1)\n",
    "#dataTest = data\n",
    "#dataTest = dataTest.iloc(0)[0:19500] #test A and B\n",
    "#dataTest = dataTest.iloc(0)[19500:38500] #test C and D\n",
    "#dataTest = dataTest.iloc(0)[39000:58000]\n",
    "\n",
    "faultNumbersTest = []\n",
    "for i in faultNumbersT:\n",
    "    faultNumbersTest.append(i)\n",
    "\n",
    "data = data.astype('float64')\n",
    "dataTest = dataTest.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b49699",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_length = 960\n",
    "#num = 960\n",
    "for i in range(int(len(faultNumbersTest)/run_length)):\n",
    "    print(str(i*run_length) + \": \" + str(faultNumbersTest[i*run_length]))\n",
    "#print(faultNumbersTest[10000:10100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708cace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataTest)\n",
    "numVariables = 54#52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23747611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45cd691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split and reshape the data set by step_size , use min-max or stanrdardlize method to rescale the data\n",
    "def Splitting_dataset(data, step_size, scale=True, scaler_type=MinMaxScaler):\n",
    "        l = len(data) \n",
    "        data = scaler_type().fit_transform(data)\n",
    "        Xs = []\n",
    "        Ys = []\n",
    "        for i in range(0, (len(data) - step_size)):\n",
    "            Xs.append(data[i:i+step_size])\n",
    "            Ys.append(data[i:i+step_size])\n",
    "        train_x, test_x, train_y, test_y = [np.array(x) for x in train_test_split(Xs, Ys)]\n",
    "        assert train_x.shape[2] == test_x.shape[2] == (data.shape[1] if (type(data) == np.ndarray) else len(data))\n",
    "        return  (train_x.shape[2], train_x, train_y, test_x, test_y)\n",
    "    \n",
    "def get_batch(x, batch_size):\n",
    "    \"\"\"Made with taking test_x or XX as input\"\"\"\n",
    "    #make stochastic\n",
    "    t = 0\n",
    "    while t >= 0:\n",
    "        x_mod = len(x) % batch_size\n",
    "        start = random.random() * (len(x)-x_mod)\n",
    "        start = int(start)\n",
    "        if start + batch_size < len(x):\n",
    "            t = t-1\n",
    "    batch = torch.tensor(x[start:(start+batch_size)]) #!! added tensor line\n",
    "    #print(batch.shape)\n",
    "    return batch\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "        #print(recon_x.shape)\n",
    "        #print(x.shape)\n",
    "        BCE = F.binary_cross_entropy(recon_x, x)#, size_average=False)\n",
    "    \n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu**2 -  logvar.exp())\n",
    "        return BCE + KLD\n",
    "    \n",
    "def tep_testing_stepped(dat,step_size):\n",
    "    res = []\n",
    "    ind = 0\n",
    "    scale = MinMaxScaler().fit(dat)\n",
    "    dat = pd.DataFrame(scale.transform(dat))\n",
    "    #print(int((len(data)/step_size)))\n",
    "    for i in range(int((len(dat)/step_size))):\n",
    "        if ind + step_size < len(dat):\n",
    "            step = []\n",
    "            for j in range(step_size):\n",
    "              #print(data.iloc(0)[ind])#[ind])\n",
    "              step.append(dat.iloc(0)[ind])\n",
    "              ind = ind + 1\n",
    "            res.append(step)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450792c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efa905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=27, z_dim=31, n_flow_steps=1):\n",
    "        super(VAE, self).__init__()\n",
    "        #self.rnn = nn.GRU(image_size,image_size,batch_first=True)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(image_size, h_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(h_dim, z_dim*2) #is it saying its getting a mu and a var for each z dim out?\n",
    "            \n",
    "            #how can I represent the encoder as a distribution acting as the prior?\n",
    "        )\n",
    "        print(z_dim*2)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, image_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        print(image_size)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_() \n",
    "        esp = to_var(torch.randn(*mu.size()))\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #q = self.recurrent(x)\n",
    "        #print(\"--\" + str(x.shape))\n",
    "        h = self.encoder(x)\n",
    "        #print(\"--\" + str(h.shape))\n",
    "        mu, logvar = torch.chunk(h, 2, dim=1)\n",
    "        #print(mu.shape)\n",
    "        #print(logvar.shape)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        #print(str(z.shape) + str(\" XXX\"))\n",
    "        #print(z)\n",
    "        #z = z.float()\n",
    "        z = model(z)\n",
    "        #print(z)\n",
    "        tensorZ = z[0]#torch.tensor(z[0])\n",
    "        #print(str(tensorZ.shape) + str(\" YYY\"))\n",
    "        #print(z[0])\n",
    "        #print(\"--\" + str(self.decoder(tensorZ).shape))\n",
    "\n",
    "        return self.decoder(tensorZ), mu, logvar\n",
    "        #return self.decoder(z), mu, logvar\n",
    "    \n",
    "\n",
    "class stacked_NVP(nn.Module):\n",
    "    def __init__(self, d, k, hidden, n):\n",
    "        super().__init__()\n",
    "        self.bijectors = nn.ModuleList([\n",
    "            R_NVP(d, k, hidden=hidden) for _ in range(n)\n",
    "        ])\n",
    "        self.flips = [True if i%2 else False for i in range(n)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        log_jacobs = []\n",
    "\n",
    "        for bijector, f in zip(self.bijectors, self.flips):\n",
    "            x, log_pz, lj = bijector(x, flip=f)\n",
    "            log_jacobs.append(lj)\n",
    "        \n",
    "        return x, log_pz, sum(log_jacobs)\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        for bijector, f in zip(reversed(self.bijectors), reversed(self.flips)):\n",
    "            z = bijector.inverse(z, flip=f)\n",
    "        return z\n",
    "    \n",
    "class R_NVP(nn.Module):\n",
    "    def __init__(self, d, k, hidden):\n",
    "        super().__init__()\n",
    "        self.d, self.k = d, k\n",
    "        self.sig_net = nn.Sequential(\n",
    "                    nn.Linear(k, hidden),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(hidden, d - k))\n",
    "\n",
    "        self.mu_net = nn.Sequential(\n",
    "                    nn.Linear(k, hidden),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(hidden, d - k))\n",
    "\n",
    "    def forward(self, x, flip=False):\n",
    "        x1, x2 = x[:, :self.k], x[:, self.k:] \n",
    "        #print(\"t\")\n",
    "        if flip:\n",
    "            x2, x1 = x1, x2\n",
    "        \n",
    "        # forward\n",
    "        sig = self.sig_net(x1)\n",
    "        print(\"x1: \" + str(x1.shape))\n",
    "        #print(\"x2: \" + str(x2.shape))\n",
    "        z1, z2 = x1, x2 * torch.exp(sig) + self.mu_net(x1)\n",
    "        \n",
    "        if flip:\n",
    "            z2, z1 = z1, z2\n",
    "        \n",
    "        z_hat = torch.cat([z1, z2], dim=-1)\n",
    "        #print(z_hat.shape)\n",
    "        #print(\"ooooooo\")\n",
    "        #print(z_hat)\n",
    "        log_pz = base_dist.log_prob(z_hat)\n",
    "        log_jacob = sig.sum(-1)\n",
    "        \n",
    "        return z_hat, log_pz, log_jacob\n",
    "    \n",
    "    def inverse(self, Z, flip=False):\n",
    "        z1, z2 = Z[:, :self.k], Z[:, self.k:] \n",
    "        \n",
    "        if flip:\n",
    "            z2, z1 = z1, z2\n",
    "        \n",
    "        x1 = z1\n",
    "        x2 = (z2 - self.mu_net(z1)) * torch.exp(-self.sig_net(z1))\n",
    "        \n",
    "        if flip:\n",
    "            x2, x1 = x1, x2\n",
    "        return torch.cat([x1, x2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8932e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 3\n",
    "batch = 512\n",
    "index_step_length = numVariables\n",
    "epochs = 20\n",
    "\n",
    "num = 10\n",
    "\n",
    "d = 54\n",
    "k = 27\n",
    "\n",
    "base_mu, base_cov = torch.zeros(54), torch.eye(54)\n",
    "\n",
    "base_mu = torch.nn.parameter.Parameter(to_var(base_mu.double()))\n",
    "base_cov = torch.nn.parameter.Parameter(to_var(base_cov.double()))\n",
    "#base_mu = torch.nn.parameter.Parameter(base_mu,requires_grad=True)\n",
    "#base_cov = torch.nn.parameter.Parameter(base_cov,requires_grad=True)\n",
    "print(base_mu)\n",
    "print(base_cov)\n",
    "base_dist = MultivariateNormal(base_mu, base_cov)\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "labels, X, Y, XX, YY = Splitting_dataset(data, step_size)\n",
    "#XX.cuda()\n",
    "demo = VAE(index_step_length,h_dim=13,z_dim=54)\n",
    "model = stacked_NVP(d, k, hidden=512,n=num)#hidden -> 512\n",
    "demo.double()\n",
    "model.double()\n",
    "    \n",
    "#next set of tests should be with n=3, last set was with n=1\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "optimizer2 = torch.optim.RMSprop(demo.parameters(), lr=1e-3)\n",
    "optimizer3 = torch.optim.RMSprop([base_mu,base_cov], lr=1e-3)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.999)\n",
    "#scheduler2 = torch.optim.lr_scheduler.ExponentialLR(optimizer2, 0.999)\n",
    "\n",
    "if torch.cuda.is_available() & cudaOn:\n",
    "    demo.cuda()\n",
    "    print(\"demo done\")\n",
    "    model.cuda()\n",
    "    print(\"model done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda6b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191093b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "anomaly_history = []\n",
    "loss_history = []\n",
    "avgSum = 0\n",
    "avgCount = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    b = get_batch(X,batch)\n",
    "    #print(demo.rnn(b).shape)\n",
    "    #b = demo.rnn(b)\n",
    "    #print(range(batch))\n",
    "    for i in range(batch):\n",
    "        #localX = torch.tensor(b[i].cuda())\n",
    "        optimizer.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        optimizer3.zero_grad()\n",
    "        localX = to_var(b[i])\n",
    "        #localX = to_var(torch.tensor(b))\n",
    "        #print(localX.shape)\n",
    "        #print(localX.type)\n",
    "        recon, mu, logvar = demo(localX)\n",
    "        print(recon)\n",
    "        loss = loss_fn(recon, localX, mu, logvar) #doing kl-divergence loss correctly\n",
    "        \"\"\"This bound (kl loss) provides a unified objective function for \n",
    "        op-timization of both the parameters θ and φ of the model and variational approximation, respectively.\"\"\"\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer2.step()\n",
    "        optimizer3.step()\n",
    "        scheduler.step()\n",
    "        #scheduler2.step()\n",
    "        idx = idx + 1\n",
    "\n",
    "        avgSum = avgSum + torch.mean(loss/batch)\n",
    "        avgCount = avgCount + 1\n",
    "        anomaly_score = abs(torch.mean(localX-recon))\n",
    "\n",
    "        if idx%step_size == 0:\n",
    "            loss_history.append(avgSum/avgCount)\n",
    "            anomaly_history.append(anomaly_score)\n",
    "            avgSum = 0\n",
    "            avgCount = 0\n",
    "\n",
    "        if idx%100 == 0:\n",
    "            print(\"Epoch[{}/{}] Loss: {:.3f}\".format(epoch+1, epochs, loss.data.item()/batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b47fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure()\n",
    "plt.plot(loss_history,'g-',label='h 10,z 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96acd88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "t1 = tep_testing_stepped(dataTest,step_size)\n",
    "step_start = 0\n",
    "anomalies = []\n",
    "y_nomalies = []\n",
    "county = 0\n",
    "setNum = 0\n",
    "for step in t1:\n",
    "    step = to_var(torch.tensor(step,dtype=torch.float64))\n",
    "    recon,_,_ = demo(step)\n",
    "    anom = abs(torch.mean(step-recon))\n",
    "    anom2 = torch.mean(torch.tensor(faultNumbersTest[county:county+step_size],dtype=torch.float64))\n",
    "    anomalies.append(anom)\n",
    "    y_nomalies.append(anom2)\n",
    "    step_start = step_start + 1\n",
    "    county = county + step_size\n",
    "p3 = plt.figure()\n",
    "plt.plot(anomalies, 'g-')\n",
    "p4 = plt.figure()\n",
    "plt.plot(y_nomalies, 'b-')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd9b5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"C:/Users/Charlie/Desktop/picsForDemo2/tepGeneratedLast/\"\n",
    "\n",
    "strtOfTwo = int(len(dataTest)/20)\n",
    "sets = []\n",
    "runNum = 0 \n",
    "print(len(faultNumbersTest))\n",
    "\n",
    "print(\"////////////////////\")\n",
    "for i in range(20):\n",
    "    print(str(strtOfTwo*runNum) + \" : \" + str((strtOfTwo*runNum)+run_length))\n",
    "    #print(faultNumbersTest[strtOfTwo*runNum])\n",
    "    dt = dataTest[strtOfTwo*runNum:(strtOfTwo*runNum)+run_length]\n",
    "    t1 = tep_testing_stepped(dt,step_size)\n",
    "    sets.append(t1)\n",
    "    runNum = runNum + 1\n",
    "print(\"////////////////////\")\n",
    "\n",
    "setNum = 0\n",
    "for set in sets:\n",
    "    setName = \"anom\" + str(setNum+1) + \"Faulty_Flow_166\"\n",
    "    fileLoc = loc + setName + \".png\"\n",
    "    step_start = 0\n",
    "    anomalies = []\n",
    "    y_nomalies = []\n",
    "    vals = []\n",
    "    pred_vals = []\n",
    "    county = 0\n",
    "    #print(type(XX))\n",
    "    #print(len(XX))\n",
    "    #print(XX.shape)\n",
    "    for step in set:\n",
    "      step = to_var(torch.tensor(step,dtype=torch.float64))\n",
    "      if True:\n",
    "          #step = torch.tensor(XX[step_start:step_start+step_size])[0]\n",
    "          recon,_,_ = demo(step)\n",
    "          print(recon)\n",
    "          a = 1\n",
    "          #b = 1\n",
    "          for j in range(len(step)):\n",
    "            a = (step[0][j] - recon[0][j])\n",
    "            #a = a + (step[0][j] - recon[0][j])\n",
    "            #b = b * recon[0][j]\n",
    "          #anom = abs(torch.mean(step-recon))\n",
    "          #print(\"-------------: \" + str(torch.mean(step-recon)))\n",
    "          #print(str(a) + \",    \" + str(b))\n",
    "          anom = a\n",
    "          anom2 = torch.mean(torch.tensor(faultNumbersTest[(setNum*strtOfTwo)+(county*step_size):(setNum*strtOfTwo)+(county*step_size+step_size)],dtype=torch.float64))\n",
    "          #if county%100 == 0:\n",
    "            #print(\"step: \" + str(step))\n",
    "            #print(\"recon: \" + str(recon))\n",
    "            #print(anom)\n",
    "          #anomalies.append(torch.tensor(math.sqrt(abs(anom))))\n",
    "          #anomalies.append(torch.tensor(abs(anom)))\n",
    "          anomalies.append(torch.tensor(abs(anom)))\n",
    "          y_nomalies.append(anom2)\n",
    "          pred_vals.append(torch.mean(recon))\n",
    "          vals.append(torch.mean(step))\n",
    "          step_start = step_start + 1\n",
    "          county = county + 1\n",
    "    setNum = setNum + 1\n",
    "    start = 0\n",
    "    view = []\n",
    "    max = -99999\n",
    "    min = 99999\n",
    "    maxA = -99999\n",
    "    minA = 99999\n",
    "\n",
    "    print(len(anomalies))\n",
    "\n",
    "    for a in anomalies:\n",
    "        if start+1 < len(anomalies):\n",
    "            view.append(abs(anomalies[start+1].item() - a.item()))  \n",
    "            start = start + 1\n",
    "\n",
    "    for i in range(len(view)):\n",
    "        j = i + 1\n",
    "        if view[i] > max:\n",
    "            max = a.item()\n",
    "        if view[i] < min:\n",
    "            min = a.item()\n",
    "        if i < len(view)-1:\n",
    "            #print(\"i: \" + str(view[i]))\n",
    "            #print(\"j: \" + str(view[j]))\n",
    "            v = abs(view[i]-view[j])\n",
    "            #print(\"v: \" + str(v))\n",
    "            if v > maxA:\n",
    "                maxA = v\n",
    "            if v < minA:\n",
    "                minA = v\n",
    "\n",
    "    print()\n",
    "    print(max)\n",
    "    print(maxA)\n",
    "    print(minA)\n",
    "\n",
    "    #could get loc min and max given a step size rather than literally between individual points\n",
    "\n",
    "    #ADD LINEAR REGRESSION LINE\n",
    "    \n",
    "    p3 = plt.figure()\n",
    "    plt.plot(anomalies, 'g-')\n",
    "    plt.savefig(fileLoc)\n",
    "    p4 = plt.figure()\n",
    "    #plt.plot(y_nomalies, 'b-')\n",
    "    plt.plot(vals, 'b-')\n",
    "    p5 = plt.figure()\n",
    "    plt.plot(pred_vals, 'r-')\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88e781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3cf57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d681334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
