{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"colab":{"name":"cleaner.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"17017c5b","executionInfo":{"status":"ok","timestamp":1636737512324,"user_tz":300,"elapsed":26926,"user":{"displayName":"Charlie Selden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfbbHmqrIRC_YWHFUfqElKU9f6D4F88A1vYwTZ=s64","userId":"01585634715821529613"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.model_selection import train_test_split\n","from torch.autograd import Variable\n","#import torch.cuda\n","import random\n","from itertools import chain as chain\n","from torch.distributions.multivariate_normal import MultivariateNormal\n","\n","#conda activate base\n","cudaOn = True"],"id":"17017c5b","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"6507ca8e","executionInfo":{"status":"ok","timestamp":1636737636972,"user_tz":300,"elapsed":87584,"user":{"displayName":"Charlie Selden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfbbHmqrIRC_YWHFUfqElKU9f6D4F88A1vYwTZ=s64","userId":"01585634715821529613"}}},"source":["#tepLoc = \"C:/Users/Charlie/Desktop/TEP_Data/\"\n","#tepTrain = tepLoc + \"TEP_Faulty_Training.csv\"\n","#tepTest = tepLoc + \"TEP_Faulty_Testing.csv\"\n","tepTrain = '/content/TEP_Faulty_Training.csv'\n","tepTest = '/content/TEP_Faulty_Testing.csv'\n","\n","scaler = StandardScaler()\n","\n","#data = pd.read_csv('c172_file_1.csv')\n","data = pd.read_csv(tepTrain)\n","dataTest = pd.read_csv(tepTest)\n","#I should only be scaling the values that are double values, scaling fault number and other stuff just kinda fucks output\n","#scaler.fit(dataTest)\n","#data = pd.DataFrame(scaler.transform(data))\n","#dataTest = pd.DataFrame(scaler.transform(dataTest))\n"],"id":"6507ca8e","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"bf7e59bc","executionInfo":{"status":"ok","timestamp":1636737685813,"user_tz":300,"elapsed":6354,"user":{"displayName":"Charlie Selden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfbbHmqrIRC_YWHFUfqElKU9f6D4F88A1vYwTZ=s64","userId":"01585634715821529613"}}},"source":["data = data.drop('Unnamed: 0',axis=1)\n","data = data.drop('faultNumber',axis=1)\n","data = data.drop('simulationRun',axis=1)\n","data = data.drop('sample',axis=1)\n","faultNumbersTest = dataTest.get('faultNumber')\n","dataTest = dataTest.drop('Unnamed: 0',axis=1)\n","dataTest = dataTest.drop('faultNumber',axis=1)\n","dataTest = dataTest.drop('simulationRun',axis=1)\n","dataTest = dataTest.drop('sample',axis=1)\n","dataTest = dataTest.iloc(0)[0:18849]\n","\n"],"id":"bf7e59bc","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WuvIIiEuG9DR","executionInfo":{"status":"ok","timestamp":1636736995474,"user_tz":300,"elapsed":182,"user":{"displayName":"Charlie Selden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfbbHmqrIRC_YWHFUfqElKU9f6D4F88A1vYwTZ=s64","userId":"01585634715821529613"}},"outputId":"4bba6790-0f7e-42e9-f45c-1b584f8b0a95"},"source":["print(data)"],"id":"WuvIIiEuG9DR","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["         xmeas_1  xmeas_2  xmeas_3  xmeas_4  ...   xmv_8   xmv_9  xmv_10  xmv_11\n","0        0.25038   3674.0   4529.0   9.2320  ...  47.757  47.510  41.258  18.447\n","1        0.25109   3659.4   4556.6   9.4264  ...  43.692  47.427  41.359  17.194\n","2        0.25038   3660.3   4477.8   9.4426  ...  46.699  47.468  41.199  20.530\n","3        0.24977   3661.3   4512.1   9.4776  ...  47.541  47.658  41.643  18.089\n","4        0.29405   3679.0   4497.0   9.3381  ...  47.645  47.346  41.507  18.461\n","...          ...      ...      ...      ...  ...     ...     ...     ...     ...\n","3060044  0.66483   3684.3   4529.0   9.0643  ...  44.727  45.511  41.115  15.773\n","3060045  0.66528   3704.7   4571.7   9.0696  ...  44.257  46.020  41.014  20.038\n","3060046  0.66975   3670.9   4509.9   8.9267  ...  42.749  46.182  40.802  18.633\n","3060047  0.66847   3650.8   4507.2   8.8937  ...  45.676  46.684  41.316  19.655\n","3060048  0.74968   3674.4   4523.2   8.9405  ...     NaN     NaN     NaN     NaN\n","\n","[3060049 rows x 52 columns]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5pZNB1ldG_39","executionInfo":{"status":"ok","timestamp":1636737741127,"user_tz":300,"elapsed":263,"user":{"displayName":"Charlie Selden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfbbHmqrIRC_YWHFUfqElKU9f6D4F88A1vYwTZ=s64","userId":"01585634715821529613"}},"outputId":"dcb5acec-d772-44f0-df0b-b8dd8e0ab3eb"},"source":["print(dataTest)\n","numVariables = 52"],"id":"5pZNB1ldG_39","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["       xmeas_1  xmeas_2  xmeas_3  xmeas_4  ...   xmv_8   xmv_9  xmv_10  xmv_11\n","0      0.25171   3672.4   4466.3   9.5122  ...  47.955  47.300  42.100  15.345\n","1      0.25234   3642.2   4568.7   9.4145  ...  45.038  47.502  40.553  16.063\n","2      0.24840   3643.1   4507.5   9.2901  ...  44.553  47.479  41.341  20.452\n","3      0.25153   3628.3   4519.3   9.3347  ...  48.048  47.440  40.780  17.123\n","4      0.21763   3655.8   4571.0   9.3087  ...  44.678  47.530  41.089  18.681\n","...        ...      ...      ...      ...  ...     ...     ...     ...     ...\n","18844  0.23512   3659.0   4491.4   9.4481  ...  45.047  48.360  40.931  20.159\n","18845  0.23559   3666.5   4443.3   9.4082  ...  43.727  49.054  41.958  18.813\n","18846  0.27699   3676.6   4493.5   9.4221  ...  48.630  49.507  41.300  18.780\n","18847  0.27973   3701.7   4464.5   9.3144  ...  45.989  49.697  41.432  15.319\n","18848  0.29423   3707.4   4553.7   9.3818  ...  45.877  49.894  41.474  16.663\n","\n","[18849 rows x 52 columns]\n"]}]},{"cell_type":"code","metadata":{"id":"ada8c248","executionInfo":{"status":"ok","timestamp":1636738008928,"user_tz":300,"elapsed":2484,"user":{"displayName":"Charlie Selden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfbbHmqrIRC_YWHFUfqElKU9f6D4F88A1vYwTZ=s64","userId":"01585634715821529613"}}},"source":["def tep_testing_stepped(dat,step_size):\n","    res = []\n","    ind = 0\n","    scale = MinMaxScaler().fit(dat)\n","    dat = scale.transform(dat)\n","    #print(int((len(data)/step_size)))\n","    for i in range(int((len(dat)/step_size))):\n","        if ind + step_size < len(dat):\n","            step = []\n","            for j in range(step_size):\n","              #print(data.iloc(0)[ind])#[ind])\n","              step.append(data.iloc(0)[ind])\n","              ind = ind + 1\n","            res.append(step)\n","    return res\n","\n","t = tep_testing_stepped(dataTest,5)\n","#t = to_var(t)\n","#print(len(t[0][0]))"],"id":"ada8c248","execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ec85e94","executionInfo":{"status":"ok","timestamp":1636738016013,"user_tz":300,"elapsed":93,"user":{"displayName":"Charlie Selden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfbbHmqrIRC_YWHFUfqElKU9f6D4F88A1vYwTZ=s64","userId":"01585634715821529613"}}},"source":["#Split and reshape the data set by step_size , use min-max or stanrdardlize method to rescale the data\n","def Splitting_dataset(data, step_size, scale=True, scaler_type=MinMaxScaler):\n","        l = len(data) \n","        data = scaler_type().fit_transform(data)\n","        Xs = []\n","        Ys = []\n","        for i in range(0, (len(data) - step_size)):\n","            Xs.append(data[i:i+step_size])\n","            Ys.append(data[i:i+step_size])\n","        train_x, test_x, train_y, test_y = [np.array(x) for x in train_test_split(Xs, Ys)]\n","        assert train_x.shape[2] == test_x.shape[2] == (data.shape[1] if (type(data) == np.ndarray) else len(data))\n","        return  (train_x.shape[2], train_x, train_y, test_x, test_y)\n","    \n","def get_batch(x, batch_size):\n","    \"\"\"Made with taking test_x or XX as input\"\"\"\n","    t = 0\n","    while t >= 0:\n","        x_mod = len(x) % batch_size\n","        start = random.random() * (len(x)-x_mod)\n","        start = int(start)\n","        if start + batch_size < len(x):\n","            t = t-1\n","    batch = x[start:(start+batch_size)]\n","    #print(batch.shape)\n","    return batch\n","\n","def to_var(x):\n","    if torch.cuda.is_available():\n","        x = x.cuda()\n","    return Variable(x)\n","\n","def loss_fn(recon_x, x, mu, logvar):\n","        BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n","    \n","        # see Appendix B from VAE paper:\n","        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n","        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n","        KLD = -0.5 * torch.sum(1 + logvar - mu**2 -  logvar.exp())\n","        return BCE + KLD\n","    \n"],"id":"0ec85e94","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6b2b35f","executionInfo":{"status":"ok","timestamp":1636738018407,"user_tz":300,"elapsed":231,"user":{"displayName":"Charlie Selden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfbbHmqrIRC_YWHFUfqElKU9f6D4F88A1vYwTZ=s64","userId":"01585634715821529613"}}},"source":["class VAE(nn.Module):\n","    def __init__(self, image_size=784, h_dim=27, z_dim=31, n_flow_steps=1):\n","        super(VAE, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Linear(image_size, h_dim),\n","            nn.LeakyReLU(0.1),\n","            nn.Linear(h_dim, z_dim*2) #is it saying its getting a mu and a var for each z dim out?\n","            \n","            #how can I represent the encoder as a distribution acting as the prior?\n","        )\n","        \n","        self.decoder = nn.Sequential(\n","            nn.Linear(z_dim, h_dim),\n","            nn.ReLU(),\n","            nn.Linear(h_dim, image_size),\n","            nn.Sigmoid()\n","        )\n","    \n","    def reparameterize(self, mu, logvar):\n","        std = logvar.mul(0.5).exp_() \n","        esp = to_var(torch.randn(*mu.size()))\n","        z = mu + std * esp\n","        return z\n","    \n","    def forward(self, x):\n","        h = self.encoder(x)\n","        mu, logvar = torch.chunk(h, 2, dim=1)\n","        #print(mu.shape)\n","        #print(logvar.shape)\n","        z = self.reparameterize(mu, logvar)\n","        #print(z.shape)\n","        #z = z.float()\n","        z = model(z)\n","        #print(z)\n","        tensorZ = z[0]#torch.tensor(z[0])\n","        #print(tensorZ.shape)\n","        #print(z[0])\n","        return self.decoder(tensorZ), mu, logvar\n","    \n","\n","class stacked_NVP(nn.Module):\n","    def __init__(self, d, k, hidden, n):\n","        super().__init__()\n","        self.bijectors = nn.ModuleList([\n","            R_NVP(d, k, hidden=hidden) for _ in range(n)\n","        ])\n","        self.flips = [True if i%2 else False for i in range(n)]\n","        \n","    def forward(self, x):\n","        log_jacobs = []\n","\n","        for bijector, f in zip(self.bijectors, self.flips):\n","            x, log_pz, lj = bijector(x, flip=f)\n","            log_jacobs.append(lj)\n","        \n","        return x, log_pz, sum(log_jacobs)\n","    \n","    def inverse(self, z):\n","        for bijector, f in zip(reversed(self.bijectors), reversed(self.flips)):\n","            z = bijector.inverse(z, flip=f)\n","        return z\n","    \n","class R_NVP(nn.Module):\n","    def __init__(self, d, k, hidden):\n","        super().__init__()\n","        self.d, self.k = d, k\n","        self.sig_net = nn.Sequential(\n","                    nn.Linear(k, hidden),\n","                    nn.LeakyReLU(),\n","                    nn.Linear(hidden, d - k))\n","\n","        self.mu_net = nn.Sequential(\n","                    nn.Linear(k, hidden),\n","                    nn.LeakyReLU(),\n","                    nn.Linear(hidden, d - k))\n","\n","    def forward(self, x, flip=False):\n","        x1, x2 = x[:, :self.k], x[:, self.k:] \n","\n","        if flip:\n","            x2, x1 = x1, x2\n","        \n","        # forward\n","        sig = self.sig_net(x1)\n","        z1, z2 = x1, x2 * torch.exp(sig) + self.mu_net(x1)\n","        \n","        if flip:\n","            z2, z1 = z1, z2\n","        \n","        z_hat = torch.cat([z1, z2], dim=-1)\n","\n","        log_pz = base_dist.log_prob(z_hat)\n","        log_jacob = sig.sum(-1)\n","        \n","        return z_hat, log_pz, log_jacob\n","    \n","    def inverse(self, Z, flip=False):\n","        z1, z2 = Z[:, :self.k], Z[:, self.k:] \n","        \n","        if flip:\n","            z2, z1 = z1, z2\n","        \n","        x1 = z1\n","        x2 = (z2 - self.mu_net(z1)) * torch.exp(-self.sig_net(z1))\n","        \n","        if flip:\n","            x2, x1 = x1, x2\n","        return torch.cat([x1, x2], -1)"],"id":"a6b2b35f","execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"8a6c8460"},"source":["step_size = 5\n","batch = 512\n","index_step_length = numVariables\n","epochs = 100\n","\n","d = 2\n","k = 1\n","\n","base_mu, base_cov = torch.zeros(2), torch.eye(2)\n","\n","base_mu = to_var(base_mu.double())\n","base_cov = to_var(base_cov.double())\n","\n","base_dist = MultivariateNormal(base_mu, base_cov)\n","#---------------------------------------------------------------------------------------------------------------------------------\n","labels, X, Y, XX, YY = Splitting_dataset(data, step_size)\n","#XX.cuda()\n","demo = VAE(index_step_length,h_dim=7,z_dim=2)\n","model = stacked_NVP(d, k, hidden=512,n=5)\n","demo.double()\n","model.double()\n","    \n","#next set of tests should be with n=3, last set was with n=1\n","optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n","optimizer2 = torch.optim.RMSprop(demo.parameters(), lr=1e-3)\n","\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.999)\n","\n","if torch.cuda.is_available() & cudaOn:\n","    demo.cuda()\n","    print(\"demo done\")\n","    model.cuda()\n","    print(\"model done\")"],"id":"8a6c8460","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1d71db30"},"source":["idx = 0\n","\n","anomaly_history = []\n","loss_history = []\n","avgSum = 0\n","avgCount = 0\n","\n","for epoch in range(epochs):\n","    b = get_batch(X,batch)\n","    #print(range(batch))\n","    for i in range(batch):\n","        #localX = torch.tensor(b[i].cuda())\n","        localX = to_var(torch.tensor(b[i]))\n","        recon, mu, logvar = demo(localX)\n","        loss = loss_fn(recon, localX, mu, logvar) #doing kl-divergence loss correctly\n","        \"\"\"This bound (kl loss) provides a unified objective function for \n","        op-timization of both the parameters θ and φ of the model and variational approximation, respectively.\"\"\"\n","        optimizer.zero_grad()\n","        optimizer2.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        optimizer2.step()\n","        scheduler.step()\n","        idx = idx + 1\n","\n","        avgSum = avgSum + torch.mean(loss/batch)\n","        avgCount = avgCount + 1\n","        anomaly_score = torch.mean(localX/recon)\n","\n","        if idx%30 == 0:\n","            loss_history.append(avgSum/avgCount)\n","            anomaly_history.append(anomaly_score)\n","            avgSum = 0\n","            avgCount = 0\n","\n","        if idx%100 == 0:\n","            print(\"Epoch[{}/{}] Loss: {:.3f}\".format(epoch+1, epochs, loss.data.item()/batch))\n","            \n","p1 = plt.figure()\n","plt.plot(loss_history,'g-',label='h 10,z 2')\n","\n","\n","step_start = 0\n","anomalies = []\n","county = 0\n","#print(type(XX))\n","#print(len(XX))\n","#print(XX.shape)\n","for step in t:\n","  step = to_var(torch.tensor(step))\n","  if True:\n","      #step = torch.tensor(XX[step_start:step_start+step_size])[0]\n","      recon,_,_ = demo(step)\n","      anom = torch.mean(step/recon)\n","      if county%100 == 0:\n","        print(\"step: \" + str(step))\n","        print(\"recon: \" + str(recon))\n","        print(anom)\n","      anomalies.append(anom)\n","      step_start = step_start + 1\n","      county = county + 1\n","        \n","p3 = plt.figure()\n","plt.plot(anomalies, 'g-')"],"id":"1d71db30","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e516dea7"},"source":[""],"id":"e516dea7","execution_count":null,"outputs":[]}]}