{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b06f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97a3360d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AltAGL</th>\n",
       "      <th>AltB</th>\n",
       "      <th>AltGPS</th>\n",
       "      <th>AltMSL</th>\n",
       "      <th>BaroA</th>\n",
       "      <th>E1_CHT1</th>\n",
       "      <th>E1_CHT2</th>\n",
       "      <th>E1_CHT3</th>\n",
       "      <th>E1_CHT4</th>\n",
       "      <th>E1_EGT1</th>\n",
       "      <th>...</th>\n",
       "      <th>LatAc</th>\n",
       "      <th>NormAc</th>\n",
       "      <th>OAT</th>\n",
       "      <th>Pitch</th>\n",
       "      <th>Roll</th>\n",
       "      <th>TAS</th>\n",
       "      <th>VSpd</th>\n",
       "      <th>VSpdG</th>\n",
       "      <th>WndDr</th>\n",
       "      <th>WndSpd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>855.8</td>\n",
       "      <td>745.7</td>\n",
       "      <td>833.6</td>\n",
       "      <td>30.05</td>\n",
       "      <td>231.88</td>\n",
       "      <td>224.27</td>\n",
       "      <td>243.57</td>\n",
       "      <td>245.74</td>\n",
       "      <td>1047.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1.25</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.35</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>856.8</td>\n",
       "      <td>746.2</td>\n",
       "      <td>834.0</td>\n",
       "      <td>30.05</td>\n",
       "      <td>232.23</td>\n",
       "      <td>224.58</td>\n",
       "      <td>243.87</td>\n",
       "      <td>246.04</td>\n",
       "      <td>1046.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1.23</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>855.8</td>\n",
       "      <td>746.4</td>\n",
       "      <td>834.2</td>\n",
       "      <td>30.05</td>\n",
       "      <td>232.59</td>\n",
       "      <td>224.84</td>\n",
       "      <td>244.12</td>\n",
       "      <td>246.34</td>\n",
       "      <td>1046.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1.21</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.96</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>856.8</td>\n",
       "      <td>745.2</td>\n",
       "      <td>833.1</td>\n",
       "      <td>30.05</td>\n",
       "      <td>232.96</td>\n",
       "      <td>225.10</td>\n",
       "      <td>244.46</td>\n",
       "      <td>246.59</td>\n",
       "      <td>1046.32</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1.22</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.95</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>856.8</td>\n",
       "      <td>745.2</td>\n",
       "      <td>833.0</td>\n",
       "      <td>30.05</td>\n",
       "      <td>233.30</td>\n",
       "      <td>225.36</td>\n",
       "      <td>244.75</td>\n",
       "      <td>246.86</td>\n",
       "      <td>1043.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1.22</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.36</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5055</th>\n",
       "      <td>0.0</td>\n",
       "      <td>844.2</td>\n",
       "      <td>751.5</td>\n",
       "      <td>839.3</td>\n",
       "      <td>30.02</td>\n",
       "      <td>282.56</td>\n",
       "      <td>270.85</td>\n",
       "      <td>296.29</td>\n",
       "      <td>288.37</td>\n",
       "      <td>1041.83</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.17</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5056</th>\n",
       "      <td>0.0</td>\n",
       "      <td>844.2</td>\n",
       "      <td>751.6</td>\n",
       "      <td>839.4</td>\n",
       "      <td>30.02</td>\n",
       "      <td>282.49</td>\n",
       "      <td>270.90</td>\n",
       "      <td>296.36</td>\n",
       "      <td>288.45</td>\n",
       "      <td>1036.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5057</th>\n",
       "      <td>0.0</td>\n",
       "      <td>844.2</td>\n",
       "      <td>751.7</td>\n",
       "      <td>839.5</td>\n",
       "      <td>30.02</td>\n",
       "      <td>282.43</td>\n",
       "      <td>270.93</td>\n",
       "      <td>296.42</td>\n",
       "      <td>288.52</td>\n",
       "      <td>1030.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5058</th>\n",
       "      <td>1.0</td>\n",
       "      <td>845.2</td>\n",
       "      <td>752.4</td>\n",
       "      <td>840.2</td>\n",
       "      <td>30.02</td>\n",
       "      <td>282.55</td>\n",
       "      <td>271.14</td>\n",
       "      <td>296.58</td>\n",
       "      <td>288.66</td>\n",
       "      <td>1024.47</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5059</th>\n",
       "      <td>0.0</td>\n",
       "      <td>845.2</td>\n",
       "      <td>751.0</td>\n",
       "      <td>838.8</td>\n",
       "      <td>30.02</td>\n",
       "      <td>282.44</td>\n",
       "      <td>271.07</td>\n",
       "      <td>296.52</td>\n",
       "      <td>288.66</td>\n",
       "      <td>1022.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5060 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AltAGL   AltB  AltGPS  AltMSL  BaroA  E1_CHT1  E1_CHT2  E1_CHT3  \\\n",
       "0        0.0  855.8   745.7   833.6  30.05   231.88   224.27   243.57   \n",
       "1        0.0  856.8   746.2   834.0  30.05   232.23   224.58   243.87   \n",
       "2        0.0  855.8   746.4   834.2  30.05   232.59   224.84   244.12   \n",
       "3        0.0  856.8   745.2   833.1  30.05   232.96   225.10   244.46   \n",
       "4        0.0  856.8   745.2   833.0  30.05   233.30   225.36   244.75   \n",
       "...      ...    ...     ...     ...    ...      ...      ...      ...   \n",
       "5055     0.0  844.2   751.5   839.3  30.02   282.56   270.85   296.29   \n",
       "5056     0.0  844.2   751.6   839.4  30.02   282.49   270.90   296.36   \n",
       "5057     0.0  844.2   751.7   839.5  30.02   282.43   270.93   296.42   \n",
       "5058     1.0  845.2   752.4   840.2  30.02   282.55   271.14   296.58   \n",
       "5059     0.0  845.2   751.0   838.8  30.02   282.44   271.07   296.52   \n",
       "\n",
       "      E1_CHT4  E1_EGT1  ...  LatAc  NormAc   OAT  Pitch  Roll  TAS   VSpd  \\\n",
       "0      245.74  1047.12  ...   0.01    0.01  27.2   1.25 -0.21  0.0 -14.35   \n",
       "1      246.04  1046.06  ...   0.01   -0.00  27.2   1.23 -0.15  0.0   0.74   \n",
       "2      246.34  1046.17  ...   0.00   -0.01  27.2   1.21 -0.14  0.0   5.96   \n",
       "3      246.59  1046.32  ...  -0.00   -0.01  27.2   1.22 -0.14  0.0   9.95   \n",
       "4      246.86  1043.91  ...   0.01   -0.01  27.2   1.22 -0.10  0.0  13.36   \n",
       "...       ...      ...  ...    ...     ...   ...    ...   ...  ...    ...   \n",
       "5055   288.37  1041.83  ...   0.00    0.04  26.0   1.45  0.00  0.0  15.17   \n",
       "5056   288.45  1036.33  ...   0.00   -0.02  26.0   1.29  0.03  0.0  15.98   \n",
       "5057   288.52  1030.14  ...   0.00    0.01  26.0   1.76  0.02  0.0  17.44   \n",
       "5058   288.66  1024.47  ...  -0.00   -0.01  26.0   1.80  0.01  0.0  25.02   \n",
       "5059   288.66  1022.10  ...  -0.00   -0.01  26.0   1.80  0.01  0.0  17.27   \n",
       "\n",
       "      VSpdG  WndDr  WndSpd  \n",
       "0      -3.9    0.0     0.0  \n",
       "1      -3.9    0.0     0.0  \n",
       "2      -3.9    0.0     0.0  \n",
       "3      -3.9    0.0     0.0  \n",
       "4      -3.9    0.0     0.0  \n",
       "...     ...    ...     ...  \n",
       "5055   -3.9    0.0     0.0  \n",
       "5056    0.0    0.0     0.0  \n",
       "5057    0.0    0.0     0.0  \n",
       "5058    0.0    0.0     0.0  \n",
       "5059    0.0    0.0     0.0  \n",
       "\n",
       "[5060 rows x 31 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('c172_file_1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1bee624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split and reshape the data set by step_size , use min-max or stanrdardlize method to rescale the data\n",
    "def Splitting_dataset(data, step_size, scale=True, scaler_type=MinMaxScaler):\n",
    "        l = len(data) \n",
    "        data = scaler_type().fit_transform(data)\n",
    "        Xs = []\n",
    "        Ys = []\n",
    "        for i in range(0, (len(data) - step_size)):\n",
    "            Xs.append(data[i:i+step_size])\n",
    "            Ys.append(data[i:i+step_size])\n",
    "        train_x, test_x, train_y, test_y = [np.array(x) for x in train_test_split(Xs, Ys)]\n",
    "        assert train_x.shape[2] == test_x.shape[2] == (data.shape[1] if (type(data) == np.ndarray) else len(data))\n",
    "        return  (train_x.shape[2], train_x, train_y, test_x, test_y)\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    \n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu**2 -  logvar.exp())\n",
    "        return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0c04f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=27, z_dim=31, n_flow_steps=1):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(image_size, h_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(h_dim, z_dim*2) #is it saying its getting a mu and a var for each z dim out?\n",
    "            \n",
    "            #how can I represent the encoder as a distribution acting as the prior?\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, image_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        #self.flow = stacked_NVP(z_dim,)\n",
    "        \n",
    "        #should i add a flow model as a paremeter of the vae for easy access in parameterize?\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_() \n",
    "        esp = to_var(torch.randn(*mu.size()))\n",
    "        #print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        #print(\"esp: \" + str(esp))\n",
    "        #randn - Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).\n",
    "        z = mu + std * esp\n",
    "        #print(z)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        #print(\"h: \" + str(h))\n",
    "        mu, logvar = torch.chunk(h, 2, dim=1)\n",
    "        #reparameterize is doing the sampling with esp\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        #print(\"z: \" + str(z))\n",
    "\n",
    "        prior = torch.distributions.Normal(mu,(logvar.mul(0.5).exp_()))\n",
    "        ml = nn.ModuleList()\n",
    "        ml.append(AffineConstantFlow(31))\n",
    "        ml.append(AffineConstantFlow(31))\n",
    "        ml.append(AffineConstantFlow(31))\n",
    "        ml.append(AffineConstantFlow(31))\n",
    "        #sacf = \n",
    "\n",
    "        nf = NormalizingFlowModel(prior,ml)\n",
    "        zTransformed = nf(z) #this is where I apply the normalizing flow once its implemented, the points are transformed as theyre generated\n",
    "    \n",
    "        #print(\"Z: \" + str(z))\n",
    "        #print(\"ZT: \" + str(zTransformed))\n",
    "        print(z)\n",
    "        z = zTransformed[0]\n",
    "        print(z)\n",
    "        return self.decoder(z), mu, logvar\n",
    "    \n",
    "class NormalizingFlow(nn.Module):\n",
    "    \"\"\" A sequence of Normalizing Flows is a Normalizing Flow \"\"\"\n",
    "\n",
    "    def __init__(self, flows):\n",
    "        super().__init__()\n",
    "        self.flows = nn.ModuleList(flows)\n",
    "\n",
    "    def forward(self, x):\n",
    "        m, _ = x.shape\n",
    "        log_det = torch.zeros(m)\n",
    "        zs = [x]\n",
    "        for flow in self.flows:\n",
    "            #h = demo.encoder(x)\n",
    "            #mu, logvar = torch.chunk(h, 2, dim=1)\n",
    "            x, ld = flow.forward(x)#,demo.reparameterize(mu,logvar))\n",
    "            log_det += ld\n",
    "            zs.append(x)\n",
    "            #here we're just summing the log deterimant of each of the flows\n",
    "        return zs, log_det\n",
    "\n",
    "    def backward(self, z):\n",
    "        m, _ = z.shape\n",
    "        log_det = torch.zeros(m)\n",
    "        xs = [z]\n",
    "        for flow in self.flows[::-1]:\n",
    "            z, ld = flow.backward(z)\n",
    "            log_det += ld\n",
    "            xs.append(z)\n",
    "        return xs, log_det\n",
    "\n",
    "class NormalizingFlowModel(nn.Module):\n",
    "    \"\"\" A Normalizing Flow Model is a (prior, flow) pair \"\"\"\n",
    "    \n",
    "    def __init__(self, prior, flows):\n",
    "        super().__init__()\n",
    "        self.prior = prior\n",
    "        self.flow = NormalizingFlow(flows)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        zs, log_det = self.flow.forward(x)\n",
    "        prior_logprob = self.prior.log_prob(zs[-1]).view(x.size(0), -1).sum(1)\n",
    "        return zs, prior_logprob, log_det\n",
    "\n",
    "    def backward(self, z):\n",
    "        xs, log_det = self.flow.backward(z)\n",
    "        return xs, log_det\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        z = self.prior.sample((num_samples,))\n",
    "        xs, _ = self.flow.backward(z)#should i update this to be the approximate posterior??????????????????\n",
    "        return xs\n",
    "    \n",
    "class AffineConstantFlow(nn.Module):\n",
    "    \"\"\" \n",
    "    Scales + Shifts the flow by (learned) constants per dimension.\n",
    "    In NICE paper there is a Scaling layer which is a special case of this where t is None\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, scale=True, shift=True):\n",
    "        super().__init__()\n",
    "        self.s = nn.Parameter(torch.randn(1, dim, requires_grad=True)) if scale else None\n",
    "        self.t = nn.Parameter(torch.randn(1, dim, requires_grad=True)) if shift else None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        s = self.s if self.s is not None else x.new_zeros(x.size())\n",
    "        t = self.t if self.t is not None else x.new_zeros(x.size())\n",
    "        z = x * torch.exp(s) + t\n",
    "        log_det = torch.sum(s, dim=1)\n",
    "        return z, log_det\n",
    "    \n",
    "    def backward(self, z):\n",
    "        s = self.s if self.s is not None else z.new_zeros(z.size())\n",
    "        t = self.t if self.t is not None else z.new_zeros(z.size())\n",
    "        x = (z - t) * torch.exp(-s)\n",
    "        log_det = torch.sum(-s, dim=1)\n",
    "        return x, log_det\n",
    "    \n",
    "class stacked_NVP(nn.Module):\n",
    "    def __init__(self, d, k, hidden, n):\n",
    "        super().__init__()\n",
    "        self.bijectors = nn.ModuleList([\n",
    "            R_NVP(d, k, hidden=hidden) for _ in range(n)\n",
    "        ])\n",
    "        self.flips = [True if i%2 else False for i in range(n)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        log_jacobs = []\n",
    "        \n",
    "        for bijector, f in zip(self.bijectors, self.flips):\n",
    "            x, log_pz, lj = bijector(x, flip=f)\n",
    "            log_jacobs.append(lj)\n",
    "        \n",
    "        return x, log_pz, sum(log_jacobs)\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        for bijector, f in zip(reversed(self.bijectors), reversed(self.flips)):\n",
    "            z = bijector.inverse(z, flip=f)\n",
    "        return z\n",
    "    \n",
    "class R_NVP(nn.Module):\n",
    "    def __init__(self, d, k, hidden):\n",
    "        super().__init__()\n",
    "        self.d, self.k = d, k\n",
    "        self.sig_net = nn.Sequential(\n",
    "                    nn.Linear(k, hidden),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(hidden, d - k))\n",
    "\n",
    "        self.mu_net = nn.Sequential(\n",
    "                    nn.Linear(k, hidden),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(hidden, d - k))\n",
    "\n",
    "    def forward(self, x, flip=False):\n",
    "        x1, x2 = x[:, :self.k], x[:, self.k:] \n",
    "\n",
    "        if flip:\n",
    "            x2, x1 = x1, x2\n",
    "        \n",
    "        # forward\n",
    "        sig = self.sig_net(x1)\n",
    "        z1, z2 = x1, x2 * torch.exp(sig) + self.mu_net(x1)\n",
    "        \n",
    "        if flip:\n",
    "            z2, z1 = z1, z2\n",
    "        \n",
    "        z_hat = torch.cat([z1, z2], dim=-1)\n",
    "\n",
    "        log_pz = base_dist.log_prob(z_hat)\n",
    "        log_jacob = sig.sum(-1)\n",
    "        \n",
    "        return z_hat, log_pz, log_jacob\n",
    "    \n",
    "    def inverse(self, Z, flip=False):\n",
    "        z1, z2 = Z[:, :self.k], Z[:, self.k:] \n",
    "        \n",
    "        if flip:\n",
    "            z2, z1 = z1, z2\n",
    "        \n",
    "        x1 = z1\n",
    "        x2 = (z2 - self.mu_net(z1)) * torch.exp(-self.sig_net(z1))\n",
    "        \n",
    "        if flip:\n",
    "            x2, x1 = x1, x2\n",
    "        return torch.cat([x1, x2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cb56c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size= 3\n",
    "batch = 128\n",
    "index_step_length = 31\n",
    "epochs = 50\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "labels, X, Y, XX, YY = Splitting_dataset(data, step_size)\n",
    "demo = VAE(index_step_length,h_dim=7,z_dim=31)\n",
    "demo.double()\n",
    "optimizer = torch.optim.RMSprop(demo.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2e809db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9327,  0.1387,  0.7461, -0.0986,  1.4332,  0.9343, -0.2574,  1.3938,\n",
      "         -0.2541,  1.7783, -0.8193,  1.0418, -1.3503,  0.3057, -2.0342,  0.7451,\n",
      "          0.3038, -1.5743, -0.8273,  1.0899, -0.4213,  0.5909,  1.3237, -1.2872,\n",
      "          0.8823, -0.5909, -0.9916, -0.2545,  0.6067, -0.2910,  2.1028],\n",
      "        [ 0.0496, -0.0623,  1.2893, -0.6281, -0.9708, -2.1139,  0.1517, -0.7570,\n",
      "          0.3810, -0.1513,  0.1013,  0.3297, -1.4870, -1.9221,  0.4075, -2.3025,\n",
      "         -1.8509, -1.0612, -1.3508,  0.5216,  1.4911,  0.5932,  0.2857, -0.0171,\n",
      "          1.1302,  0.1357, -1.2503,  0.2010, -0.2165, -0.7769,  0.5447],\n",
      "        [ 0.2894, -0.7946, -2.3291, -1.8148,  0.7920, -1.8095, -0.6785, -1.0825,\n",
      "         -0.3049, -1.0473, -0.5414, -1.5631, -1.7942, -0.7918, -1.2143,  0.3409,\n",
      "         -1.1434,  1.1907, -0.5282,  0.0371, -0.4544, -0.0236, -0.7175,  0.7032,\n",
      "         -0.7953, -0.0548, -0.5624,  0.7704, -0.1357,  0.7720, -1.6490]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "[tensor([[-0.9327,  0.1387,  0.7461, -0.0986,  1.4332,  0.9343, -0.2574,  1.3938,\n",
      "         -0.2541,  1.7783, -0.8193,  1.0418, -1.3503,  0.3057, -2.0342,  0.7451,\n",
      "          0.3038, -1.5743, -0.8273,  1.0899, -0.4213,  0.5909,  1.3237, -1.2872,\n",
      "          0.8823, -0.5909, -0.9916, -0.2545,  0.6067, -0.2910,  2.1028],\n",
      "        [ 0.0496, -0.0623,  1.2893, -0.6281, -0.9708, -2.1139,  0.1517, -0.7570,\n",
      "          0.3810, -0.1513,  0.1013,  0.3297, -1.4870, -1.9221,  0.4075, -2.3025,\n",
      "         -1.8509, -1.0612, -1.3508,  0.5216,  1.4911,  0.5932,  0.2857, -0.0171,\n",
      "          1.1302,  0.1357, -1.2503,  0.2010, -0.2165, -0.7769,  0.5447],\n",
      "        [ 0.2894, -0.7946, -2.3291, -1.8148,  0.7920, -1.8095, -0.6785, -1.0825,\n",
      "         -0.3049, -1.0473, -0.5414, -1.5631, -1.7942, -0.7918, -1.2143,  0.3409,\n",
      "         -1.1434,  1.1907, -0.5282,  0.0371, -0.4544, -0.0236, -0.7175,  0.7032,\n",
      "         -0.7953, -0.0548, -0.5624,  0.7704, -0.1357,  0.7720, -1.6490]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>), tensor([[ 8.7230e-01, -2.8268e-01, -6.6800e-01, -2.9105e-02,  1.8750e-01,\n",
      "          2.7431e+00, -2.5101e-01, -3.1362e-01, -5.2747e-01,  3.2897e+00,\n",
      "         -4.4364e+00, -6.0802e-01, -8.5443e-01,  1.0916e+00, -1.2010e+00,\n",
      "          6.8156e+00,  9.2824e-01,  7.2101e-02,  1.2045e-01,  2.2670e+00,\n",
      "          1.0528e+00,  1.7171e+00,  6.0155e-01, -1.5957e+00,  1.0953e+01,\n",
      "         -3.0004e-01, -1.1435e+00,  1.7324e-01,  7.4541e-01,  3.1590e-01,\n",
      "          2.6403e+00],\n",
      "        [ 1.5392e+00, -6.0409e-01, -2.5964e-01, -1.7066e-01, -1.6898e-01,\n",
      "         -2.9555e-01,  4.5738e-01, -7.2447e-01, -3.8243e-02,  1.0669e-01,\n",
      "         -3.9636e+00, -1.1830e+00, -9.8114e-01, -2.0048e-01,  1.2565e+00,\n",
      "         -1.9799e+01,  4.4937e-01,  5.0469e-01,  2.3935e-02,  1.8107e+00,\n",
      "          1.7678e+00,  1.7211e+00, -5.9875e-01, -2.5805e-01,  1.3811e+01,\n",
      "          5.0183e-01, -1.2472e+00,  3.4432e-01,  1.8969e-01, -6.8908e-01,\n",
      "          8.1877e-01],\n",
      "        [ 1.7019e+00, -1.7749e+00, -2.9798e+00, -4.8787e-01,  9.2421e-02,\n",
      "          7.8793e-03, -9.8016e-01, -7.8666e-01, -5.6664e-01, -1.3714e+00,\n",
      "         -4.2937e+00, -2.7115e+00, -1.2658e+00,  4.5508e-01, -3.7583e-01,\n",
      "          3.2864e+00,  6.0660e-01,  2.4033e+00,  1.7561e-01,  1.4217e+00,\n",
      "          1.0404e+00,  6.4613e-01, -1.7589e+00,  5.0059e-01, -8.3838e+00,\n",
      "          2.9156e-01, -9.7139e-01,  5.5820e-01,  2.4423e-01,  2.5145e+00,\n",
      "         -1.7459e+00]], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([[  1.1130,   0.2755,   1.2232,  -0.3481,  -0.8123,   1.8807,   0.3813,\n",
      "          -1.6940,  -1.4642,   0.7977,   1.3094,  -0.8536,   2.3975,   1.2818,\n",
      "          -1.3154,   5.9379,   1.3111,   1.1492,  -0.3142,   3.1979,  -0.8209,\n",
      "           5.9786,  -0.0289,  -3.2027,  12.1750,  -1.7552,  -1.7186,  -1.4736,\n",
      "           0.7417,   2.4542,   1.9603],\n",
      "        [  2.3934,   0.1040,   1.2998,  -0.3936,  -1.0781,  -0.2453,   0.6592,\n",
      "          -4.4869,  -1.3925,   0.6696,   1.3270,  -0.9566,   2.2803,  -0.9957,\n",
      "           3.1985, -15.8119,   0.8722,   1.6086,  -0.4460,   2.3447,   0.2233,\n",
      "           5.9917,  -0.6772,  -0.3412,  14.9326,  -1.3073,  -1.8312,  -1.3830,\n",
      "          -0.1498,  -0.2400,   0.5234],\n",
      "        [  2.7059,  -0.5206,   0.7892,  -0.4956,  -0.8832,  -0.0330,   0.0952,\n",
      "          -4.9096,  -1.4699,   0.6101,   1.3147,  -1.2306,   2.0171,   0.1599,\n",
      "           0.2003,   3.0538,   1.0163,   3.6249,  -0.2388,   1.6174,  -0.8390,\n",
      "           2.4654,  -1.3039,   1.2817,  -6.4812,  -1.4248,  -1.5317,  -1.2697,\n",
      "          -0.0623,   8.3482,  -1.4998]], dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), tensor([[  2.9224,  -0.4331,   1.9803,   0.9154,   1.3770,   3.3629,   2.0371,\n",
      "          -1.0224,  -1.0166,  -0.2079,   0.2358,  -0.2292,  -0.2947,  -0.1979,\n",
      "          -4.0409,   6.8676,   0.9964,   5.0196,  -0.6462,   5.8198,  -2.1346,\n",
      "           5.6671,  -1.2860,  -4.5916,  10.1898,  -0.7619,  -1.1406,   0.0532,\n",
      "           1.7267,   1.0659,   2.1120],\n",
      "        [  5.3132,  -0.5082,   2.1305,   0.8741,   1.3462,  -0.1959,   3.4596,\n",
      "          -1.7145,  -0.9171,  -0.2970,   0.2501,  -0.2843,  -0.3698,  -2.6785,\n",
      "          11.0325, -17.2301,   0.2156,   6.9924,  -1.0032,   3.8167,  -0.9056,\n",
      "           5.6828,  -1.8776,  -0.9811,  12.6117,  -0.4991,  -1.2156,   0.0948,\n",
      "          -1.0102,  -0.2966,   1.0031],\n",
      "        [  5.8967,  -0.7815,   1.1296,   0.7816,   1.3687,   0.1595,   0.5729,\n",
      "          -1.8193,  -1.0246,  -0.3384,   0.2401,  -0.4308,  -0.5386,  -1.4199,\n",
      "           1.0205,   3.6722,   0.4720,  15.6507,  -0.4421,   2.1092,  -2.1559,\n",
      "           1.4288,  -2.4494,   1.0665,  -6.1953,  -0.5680,  -1.0161,   0.1469,\n",
      "          -0.7416,   4.0467,  -0.5581]], dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), tensor([[ 8.4821e+00, -1.3983e-01,  1.8787e+00,  8.0477e-01, -1.4817e+00,\n",
      "          3.1270e+00,  7.0286e-01, -7.0752e+00, -2.9732e-01, -1.5722e+00,\n",
      "         -1.3478e+00,  1.4590e+00, -4.7748e-01, -2.0215e-01, -1.9347e+00,\n",
      "          7.4264e+00,  3.6326e+00,  1.2784e+00, -1.0357e+00,  4.8490e+00,\n",
      "          8.6063e-01,  1.9440e+01, -9.1135e-01, -4.4891e+00,  1.0496e+01,\n",
      "         -3.3297e-03, -4.6555e-01, -1.9906e-01,  8.5693e-01, -2.4547e-01,\n",
      "          4.9250e+00],\n",
      "        [ 1.5023e+01, -3.1082e-01,  2.0936e+00,  7.4667e-01, -1.4848e+00,\n",
      "          4.6891e-01,  1.5145e+00, -1.1861e+01, -2.1176e-01, -1.8466e+00,\n",
      "         -1.3341e+00,  1.4263e+00, -5.4129e-01, -7.5678e-01,  4.8963e+00,\n",
      "         -1.6543e+01,  5.1930e-01,  2.2715e+00, -1.2752e+00,  2.8652e+00,\n",
      "          1.1772e+00,  1.9493e+01, -1.7607e+00, -7.2946e-01,  1.2963e+01,\n",
      "          8.3948e-02, -5.9286e-01, -1.5260e-01,  2.6939e-01, -9.1836e-01,\n",
      "          3.1596e+00],\n",
      "        [ 1.6620e+01, -9.3370e-01,  6.6236e-01,  6.1647e-01, -1.4825e+00,\n",
      "          7.3435e-01, -1.3260e-01, -1.2585e+01, -3.0417e-01, -1.9740e+00,\n",
      "         -1.3437e+00,  1.3394e+00, -6.8466e-01, -4.7537e-01,  3.5904e-01,\n",
      "          4.2479e+00,  1.5415e+00,  6.6301e+00, -8.9880e-01,  1.1742e+00,\n",
      "          8.5515e-01,  5.3032e+00, -2.5816e+00,  1.4028e+00, -6.1908e+00,\n",
      "          6.1062e-02, -2.5430e-01, -9.4528e-02,  3.2706e-01,  1.2266e+00,\n",
      "          6.7414e-01]], dtype=torch.float64, grad_fn=<AddBackward0>)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4412/1186588080.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mrecon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocalX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocalX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4412/1378097677.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzTransformed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mNormalizingFlow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "anomaly_history = []\n",
    "loss_history = []\n",
    "avgSum = 0\n",
    "avgCount = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(XX)):\n",
    "        localX = torch.tensor(XX[i])\n",
    "        \n",
    "        \n",
    "        recon, mu, logvar = demo(localX)\n",
    "        loss = loss_fn(recon, localX, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        idx = idx + 1\n",
    "\n",
    "        \n",
    "        avgSum = avgSum + torch.mean(loss/batch)\n",
    "        avgCount = avgCount + 1\n",
    "        anomaly_score = torch.mean(localX/recon)\n",
    "\n",
    "        if idx%10 == 0:\n",
    "            loss_history.append(avgSum/avgCount)\n",
    "            anomaly_history.append(anomaly_score)\n",
    "            avgSum = 0\n",
    "            avgCount = 0\n",
    "\n",
    "        if idx%100 == 0:\n",
    "            print(\"Epoch[{}/{}] Loss: {:.3f}\".format(epoch+1, epochs, loss.data.item()/batch))\n",
    "\n",
    "plt.plot(loss_history,'g-',label='h 10,z 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80a70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a95f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
